apiVersion: batch/v1
kind: Job
metadata:
  name: {{ template "ckaf-zookeeper.postUpgradeJobName" . }}
  labels:
{{ include "ckaf-zookeeper.commonlabels" . | indent 4 }}
{{- include "csf-common-lib.v1.customLabels" (tuple .Values.global.labels) | indent 4 }}
    app: {{ .Chart.Name }}
    release: {{ .Release.Name | quote }}
    heritage: {{ .Release.Service | quote }}
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: {{ .Values.global.seccompAllowedProfileNames }} 
    seccomp.security.alpha.kubernetes.io/defaultProfileName: {{ .Values.global.seccompDefaultProfileName }}
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "-5"
{{- include "csf-common-lib.v1.customAnnotations" (tuple .Values.global.annotations) | indent 4 }}
spec:
  template:
    metadata:
      labels:
        altiplano-role: {{ .Values.accessRoleLabel }}
        app: {{ .Chart.Name }}
{{ include "ckaf-zookeeper.commonlabels" . | indent 8 }}
{{- include "csf-common-lib.v1.customLabels" (tuple .Values.global.labels) | indent 8 }}
      annotations:
        sidecar.istio.io/inject: "false"
{{- include "csf-common-lib.v1.customAnnotations" (tuple .Values.global.annotations) | indent 8 }}        
    spec:
      {{- if .Values.zookeeperNodeSelector.enable }}
      nodeSelector:
{{ toYaml .Values.zookeeperNodeSelector.nodeLabel | indent 8 }}
      {{- end }}
      {{- include "ckaf-zookeeper.tolerations" . | indent 6 }}
      {{- if .Values.global.rbac.enabled }}
      serviceAccountName: {{ template "ckaf-zookeeper.serviceAccountName" . }}
      {{- end }}
      automountServiceAccountToken: true
      {{- if eq .Values.security.enabled true }}
      securityContext:
        {{- if ( ne ( toString ( .Values.security.runAsUser )) "auto" ) }}
        runAsUser: {{ .Values.security.runAsUser }}
        {{- end }}
        runAsNonRoot: true
        {{- if ( ne ( toString (.Values.security.fsGroup)) "auto" ) }}
        fsGroup: {{ .Values.security.fsGroup }}
        {{- end }}
        {{- if eq (include "ckaf-zookeeper.renderSeccompProfile" .) "true" }}
        seccompProfile:
          type: {{ .Values.security.seccompProfile.type }}
        {{- end }}
      {{- end }}
      {{- if  coalesce .Values.imagePullSecrets .Values.global.imagePullSecrets }}
      imagePullSecrets:
        - name: {{ coalesce .Values.imagePullSecrets .Values.global.imagePullSecrets }}
      {{- end }}
      restartPolicy: Never
      containers:
      - name: {{ template "ckaf-zookeeper.postUpgradeJobContainerName" . }}
        {{- if eq .Values.global.flatRegistry true }}
        image: "{{ .Values.global.registry }}/{{ .Values.kubectlImageName }}:{{ .Values.kubectlTag }}"
        {{- else }}
        image: "{{ coalesce .Values.global.registry3 .Values.global.registry }}/{{ .Values.kubectlImage }}:{{ .Values.kubectlTag }}"
        {{- end }}
        imagePullPolicy: IfNotPresent
        {{- if eq .Values.security.enabled true }}
        securityContext:
          runAsNonRoot: true
          readOnlyRootFilesystem: {{ .Values.security.readOnlyRootFilesystem }}
          {{- if ( ne ( toString ( .Values.security.runAsUser )) "auto" ) }}
          runAsUser: {{ .Values.security.runAsUser }}
          {{- end }}
          {{- if .Values.security.runAsGroup }}
          runAsGroup: {{ .Values.security.runAsGroup }}
          {{- end }} 
          allowPrivilegeEscalation: false
          {{- if eq (include "ckaf-zookeeper.renderSeccompProfile" .) "true" }}
          seccompProfile:
            type: {{ .Values.security.seccompProfile.type }}
          {{- end }}
          privileged: false
          capabilities:
            drop: ["ALL"]
        {{- end }}
        env:
        - name: TZ
          value: {{ template "ckaf-zookeeper.timeZoneEnv" . }}             
        resources:
{{- include "ckaf-zookeeper.getResources" (tuple . .Values.jobResources "1") | indent 10 }}
        command:
        - bash
        - -c
        - |
          echo "zookeeper upgrade changes have been sucessfully applied" 
          echo "zookeeper post-upgrade job in progress..."
          # Since previous ckaf zookeeper charts doesn't have pre-rollback.yaml
          {{if and .Values.global.prepareRollback .Values.enableUpgrade}}
          kubectl delete statefulset --cascade=false --namespace {{ .Release.Namespace }} -l app={{ .Chart.Name }},release={{ .Release.Name }}
          {{- end }} # .Values.global.prepareRollback
          echo "Zookeeper post-upgrade job completed..."
          echo "To monitor the status of the zookeeper pods "
          echo "kubectl get pods --namespace {{ .Release.Namespace }} -l app={{ .Chart.Name }},release={{ .Release.Name }}"


---
# scale feature support via upgrade hooks.
{{ if .Values.global.enable_scale_via_upgrade }}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ template "ckaf-zookeeper.postUpgradeScaleJobName" . }}
  labels:
{{ include "ckaf-zookeeper.commonlabels" . | indent 4 }}
{{- include "csf-common-lib.v1.customLabels" (tuple .Values.global.labels) | indent 4 }}
    app: {{ .Chart.Name }}
    release: {{ .Release.Name | quote }}
    heritage: {{ .Release.Service | quote }}
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: {{ .Values.global.seccompAllowedProfileNames }}
    seccomp.security.alpha.kubernetes.io/defaultProfileName: {{ .Values.global.seccompDefaultProfileName }}
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "-5"
{{- include "csf-common-lib.v1.customAnnotations" (tuple .Values.global.annotations) | indent 4 }}
spec:
  template:
    metadata:
      labels:
        altiplano-role: {{ .Values.accessRoleLabel }}
        app: {{ .Chart.Name }}
{{ include "ckaf-zookeeper.commonlabels" . | indent 8 }}
{{- include "csf-common-lib.v1.customLabels" (tuple .Values.global.labels) | indent 8 }}
      annotations:
        sidecar.istio.io/inject: "false"
{{- include "csf-common-lib.v1.customAnnotations" (tuple .Values.global.annotations) | indent 8 }}
    spec:
      {{- if .Values.zookeeperNodeSelector.enable }}
      nodeSelector:
{{ toYaml .Values.zookeeperNodeSelector.nodeLabel | indent 8 }}
      {{- end }}
{{- include "ckaf-zookeeper.tolerations" . | indent 6 }}
      {{- if .Values.global.rbac.enabled }}
      serviceAccountName: {{ template "ckaf-zookeeper.serviceAccountName" . }}
      {{ end }}
      automountServiceAccountToken: true
      {{- if eq .Values.security.enabled true }}
      securityContext:
        {{- if ( ne ( toString ( .Values.security.runAsUser )) "auto" ) }}
        runAsUser: {{ .Values.security.runAsUser }}
        {{- end }}
        runAsNonRoot: true
        {{- if ( ne ( toString (.Values.security.fsGroup)) "auto" ) }}
        fsGroup: {{ .Values.security.fsGroup }}
        {{- end }}
        {{- if eq (include "ckaf-zookeeper.renderSeccompProfile" .) "true" }}
        seccompProfile:
          type: {{ .Values.security.seccompProfile.type }}
        {{- end }}
      {{- end }}
      {{- if  coalesce .Values.imagePullSecrets .Values.global.imagePullSecrets }}
      imagePullSecrets:
        - name: {{ coalesce .Values.imagePullSecrets .Values.global.imagePullSecrets }}
      {{- end }}
      restartPolicy: Never
      containers:
      - name: {{ template "ckaf-zookeeper.postUpgradeScaleJobContainerName" . }}
        {{- if eq .Values.global.flatRegistry true }}
        image: "{{ .Values.global.registry }}/{{ .Values.kubectlImageName }}:{{ .Values.kubectlTag }}"
        {{- else }}
        image: "{{ coalesce .Values.global.registry3 .Values.global.registry }}/{{ .Values.kubectlImage }}:{{ .Values.kubectlTag }}"
        {{- end }}
        imagePullPolicy: IfNotPresent
        {{- if eq .Values.security.enabled true }}
        securityContext:
          runAsNonRoot: true
          readOnlyRootFilesystem: {{ .Values.security.readOnlyRootFilesystem }}
          {{- if ( ne ( toString ( .Values.security.runAsUser )) "auto" ) }}
          runAsUser: {{ .Values.security.runAsUser }}
          {{- end }}
          {{- if .Values.security.runAsGroup }}
          runAsGroup: {{ .Values.security.runAsGroup }}
          {{- end }} 
          allowPrivilegeEscalation: false
          {{- if eq (include "ckaf-zookeeper.renderSeccompProfile" .) "true" }}
          seccompProfile:
            type: {{ .Values.security.seccompProfile.type }}
          {{- end }}
          privileged: false
          capabilities:
            drop: ["ALL"]
        {{- end }}
        env:
        - name: TZ
          value: {{ template "ckaf-zookeeper.timeZoneEnv" . }}    
        - name: OLD_REPLICA_COUNT
          valueFrom:
            configMapKeyRef:
              name: {{ template "ckaf-zookeeper.name" . }}-replicas
              key: replicas                    
        resources:
{{- include "ckaf-zookeeper.getResources" (tuple . .Values.jobResources "1") | indent 10 }}
        command:
        - bash
        - -c
        - |
          echo "zookeeper upgrade changes have been sucessfully applied" 
          echo "zookeeper post-upgrade job in progress..."
          newReplicaCount={{ .Values.servers }}
          if [ "$newReplicaCount" -eq "$OLD_REPLICA_COUNT" ]
          then
            echo "Zookeeper Old and new replica counts are same. Nothing to do!"
          # scale in case
          elif [ "$newReplicaCount" -lt "$OLD_REPLICA_COUNT" ]
          then
            if  [[ {{ .Values.servers | quote }} -eq 0 ]]
            then
              echo "Replicas set to 0, PVC would not be deleted."
              printf -v cmdata "data:\n   replicas:  \"$newReplicaCount\""
              kubectl patch configmap {{ template "ckaf-zookeeper.name" . }}-replicas -n {{ .Release.Namespace }}  --type merge -p "$cmdata"
              exit 0
            else
              echo "Scaling in the Zookeeper cluster to ${newReplicaCount}, deleting the un-used PVC's"
              # clean up the pvcs of the old zookeeper pod's.
              # get the list of pvcs owned by the release.
              listOfPvcs=`kubectl get pvc -l app={{ .Chart.Name }},release={{ .Release.Name }} -n={{ .Release.Namespace }} | awk '{print $1}'`
              for i in ${listOfPvcs};do
                pvcOrdinality=${i##*-}
                # delete pvc having ordinality greater than the new cluster size
                if ((${pvcOrdinality} >= ${newReplicaCount}));then
                  echo "Delete Zookeeper pvc: ${i}"
                  kubectl delete pvc ${i} --namespace {{ .Release.Namespace }}
                fi
              done
            fi
          # scale out case
          else
            echo "Scaling out Zookeeper cluster to ${newReplicaCount}"
            echo "Performing reconfig add for the new zookeeper server"
            # constructing the reconfig add command 
            CONSTRUCT_FQDN="reconfig -add  server."
            for (( i=${OLD_REPLICA_COUNT}; i<${newReplicaCount}; i++ ));do
              CONSTRUCT_FQDN+=$((${i}+1))={{ template "ckaf-zookeeper.statefulset" . }}"-${i}."{{ template "ckaf-zookeeper.name" . }}"-headless."{{ .Release.Namespace }}".svc."{{ .Values.clusterDomain }}":"{{ .Values.serverPort }}":"{{ .Values.leaderElectionPort }}";0.0.0.0:"{{ .Values.zookeeperClientPort }}","
            done;
            # trim "," at the end of the string
            CONSTRUCT_FQDN=${CONSTRUCT_FQDN::-1}
            echo "${CONSTRUCT_FQDN}" | kubectl exec -i {{ template "ckaf-zookeeper.statefulset" . }}-0 -c {{ template "ckaf-zookeeper.containerName" . }} -n {{ .Release.Namespace }} -- bash -c  "env -u KAFKA_OPTS zookeeper-shell "{{ template "ckaf-zookeeper.statefulset" . }}"-0."{{ template "ckaf-zookeeper.name" . }}"-headless."{{ .Release.Namespace }}".svc."{{ .Values.clusterDomain }}":"{{ .Values.zookeeperClientPort }}; sleep 10
            
            # Verify if the reconfig add was sucessful. 
            for (( i=${OLD_REPLICA_COUNT}; i<${newReplicaCount}; i++ ));do
              CHECK_AFTER_RECONFIG_ADD=$(echo "config" | kubectl exec -i {{ template "ckaf-zookeeper.statefulset" . }}-0 -c {{ template "ckaf-zookeeper.containerName" . }} -n {{ .Release.Namespace }} -- bash -c  "env -u KAFKA_OPTS zookeeper-shell  "{{ template "ckaf-zookeeper.statefulset" . }}"-0."{{ template "ckaf-zookeeper.name" . }}"-headless."{{ .Release.Namespace }}".svc."{{ .Values.clusterDomain }}":"{{ .Values.zookeeperClientPort }} | grep -c {{ template "ckaf-zookeeper.statefulset" . }}-${i})
              if ! (( $CHECK_AFTER_RECONFIG_ADD ));then
                echo "Failed to add Zookeeper server $((${i}+1))..Retrying"
                echo "reconfig -add server.$((${i}+1))="{{ template "ckaf-zookeeper.statefulset" . }}"-${i}."{{ template "ckaf-zookeeper.name" . }}"-headless."{{ .Release.Namespace }}".svc."{{ .Values.clusterDomain }}":"{{ .Values.serverPort }}":"{{ .Values.leaderElectionPort }}";0.0.0.0:"{{ .Values.zookeeperClientPort }} | kubectl exec -i {{ template "ckaf-zookeeper.statefulset" . }}-0 -c {{ template "ckaf-zookeeper.containerName" . }} -n {{ .Release.Namespace }} -- bash -c  "env -u KAFKA_OPTS zookeeper-shell "{{ template "ckaf-zookeeper.statefulset" . }}"-0."{{ template "ckaf-zookeeper.name" . }}"-headless."{{ .Release.Namespace }}".svc."{{ .Values.clusterDomain }}":"{{ .Values.zookeeperClientPort }}
              fi;
            done;

            # updating the Zookeeper dynamic configmap data with the new replicavalues.
            CONFIG_MAP=$(echo "config" | kubectl exec -i {{ template "ckaf-zookeeper.statefulset" . }}-0 -c {{ template "ckaf-zookeeper.containerName" . }} -n {{ .Release.Namespace }} -- bash -c  "env -u KAFKA_OPTS zookeeper-shell  "{{ template "ckaf-zookeeper.statefulset" . }}"-0."{{ template "ckaf-zookeeper.name" . }}"-headless."{{ .Release.Namespace }}".svc."{{ .Values.clusterDomain }}":"{{ .Values.zookeeperClientPort }} | grep server)
            # add indents to server list
            for i in $CONFIG_MAP;do printf -v serverList "$serverList      $i\n";done
            printf -v zkCmData "data:\n   dynamicfileorg.cfg.dynamic: |-\n$serverList"
            kubectl patch configmap {{ template "ckaf-zookeeper.name" . }}-dynamic-config -n {{ .Release.Namespace }}  --type merge -p "$zkCmData"
          fi
          
          # update the config map with the latest Zookeeper cluster size.
          printf -v cmdata "data:\n   replicas:  \"$newReplicaCount\""
          kubectl patch configmap {{ template "ckaf-zookeeper.name" . }}-replicas -n {{ .Release.Namespace }}  --type merge -p "$cmdata"
          echo "Zookeeper post-upgrade job completed..."
          echo "To monitor the status of the zookeeper pods "
          echo "kubectl get pods --namespace {{ .Release.Namespace }} -l app={{ .Chart.Name }},release={{ .Release.Name }}"
{{- end }}
---

# Default values for altiplano-infra.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

tags:
  infra: true
  edge-infra: false
  ingress-only: false
  wfe: false
  altiplano-diag: false
  altiplano-long-retention-opentsdb: false

#NOTE
# EITHER keep the line uncommented
# OR keep it commented
# NEVER set it to enabled: false
global:
  altiplano-indexsearch:
    #enabled: true
  altiplano-standalone-fluentd:
    #enabled: true
  altiplano-alarms-indexsearch:
    #enabled: true
  altiplano-mariadb:
    #enabled: true
  altiplano-sso:
    #enabled: true
  altiplano-kafka:
    #enabled: true
  altiplano-opentsdb:
    #enabled: true
  altiplano-grafana:
    #enabled: true
  altiplano-webdav:
    #enabled: true
  altiplano-pts:
    #enabled: true
  altiplano-redis:
    #enabled: true
  altiplano-ingress:
    #enabled: true
  altiplano-kafka-mirrormaker:
    #enabled: true
  altiplano-oauth2-proxy:
    #enabled: true
  altiplano-oauth2-proxy-nbi:
    #enabled: true
  #clustered_opentsdb: true
  altiplano-cbur:
    #enabled: true
  altiplano-backup-restore-helper:
    #enabled: true
  altiplano-nspos-rabbitmq:
    #enabled: true
  altiplano-long-retention-opentsdb:
    #enabled: true  
  altiplano-diag:
    #enabled: true
  altiplano-ksql-server:
    #enabled: true
  vault_enabled: false

  altiplanoJob:
    securityContext:
      runAsUser: 65534
      runAsGroup: 65534

  ingressconfig: &ingressconfig
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/proxy-pass-suffix: $uri
      #kubernetes.io/tls-acme: "true"
    #hosts:
    #- "*"
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
  nginxIncIngress: &nginxIncIngressConfig
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.org/mergeable-ingress-type: minion
  ## NOTE the below values will be overridden by settings in each microservice
  #change these urls for externally launched services
  #LOG_INDEX_PATTERN environment variable is used when exporting the logs, the valid value is a string with restrictions mentioned below
  #Lowercase only. Cannot include \, /, *, ?, ", <, >, |, blank space, : , comma , #.  Cannot start with -, _, + . Cannot be . or ..  Cannot be longer than 255 characters
  #LOG_INDEX_PATTERN: logstash
  INGRESS_HTTPSPORT_ENABLED: false
  #INGRESS_CONTROLLER_HTTPSPORT: &ingressHttpsPort 32443
  #INGRESS_STATUS_PORT: &ingressStatusPort 18080
  RESTRICTED_TO_NAMESPACE: &restrictedNamespaceEnabled false
  ALTIPLANO_NAMESPACE: &altiplanoNamespace ""
  SKIP_IS_MIGRATION: true
  IS_MIGRATION_INDICES: intents,alarms-active,health-alarms-TODAY
  IS_MIGRATION_TIMEOUT_SECONDS: 900
  registry: &registryUrl artifactory.net.nokia.com
  ALTIPLANO_KEYCLOAK_UI_REALM: "master"
  ALTIPLANO_KEYCLOAK_NBI_REALM: "system"
  registry1: *registryUrl
  registry2: *registryUrl
  registry3: *registryUrl
  registry4: *registryUrl
  persistence: &persistence true # NOTE Use altiplano-volumeclaims chart to create pvc  s
  storageClass: &storageClass ""  # NOTE: specify the storageClass & update the individual storageClass configuration to use the value from the global variable using "Yaml Anchors". By default the value will be "" (Empty String)
  K8S_PUBLIC_IP:
  K8S_PUBLIC_HOSTNAME:
  EXTERNAL_SERVICE_IP:
  use_tls: &useTls true # Enable SSL for kafka
  cburEnable: &cburEnable true
  cburApiVersion: &cburApiVersion "cbur.csf.nokia.com/v1"
  GLOBAL_BACKEND: &globalBackend local  # defines the backup storage options, i.e. local, SFTP
  BACKUP_RETAIN: &backupRetain "5"
  clogEnable: false
  jobhookenable: false
  namenodeHAEnabled: false
  timeZoneEnv: &timeZoneEnv ""
  ALTIPLANO_SITE_NAME: nokia-altiplano-site-1   #NOTE: must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-site',  or 'site-123')
  ALTIPLANO_GEO_ACTIVE_SITE: true
  INGRESS_LIMIT_RPS: &ingressLimitRps "1000"
  INGRESS_LIMIT_BURST_MULTIPLIER: &ingressLimitBurstMultiplier "5"
  #if you are deploying open source ingress controller of Nginxinc , need to change this value to nginxinc
  INGRESS_CONTROLLER_TYPE: kubernetes #valid values are kubernetes / nginxinc
  GRAFANA_ADMIN_USERNAME: "grafanaadmin"
  GRAFANA_ADMIN_PASSWORD: "nokiafnms123"
  kubectl:
    repo: fnms-kubectl
    tag: nokia-1.2.0
    pullPolicy: IfNotPresent
  externalCertificates:
    # The secret is expected to contain the below listed arbitrary files
    fileNames:
      # Generally these are the CA related files
      ca_cert:             "ca-cert.pem"
      ca_key:              "ca-key.pem"
      # Generally these are the client related files
      client_cert:         "client-cert.pem"
      client_key:          "client-key.pem"
      client_req:          "client-req.pem"
      client_jks:          "client.jks"
      # Generally these are the server related files
      server_cert:         "server-cert.pem"
      server_key:          "server-key.pem"
      server_req:          "server-req.pem"
      server_jks:          "server.jks"
      # Generally these are the trustchain related files
      trustchain_cert:     "trustchain-cert.pem"
      trustchain_jks:      "trustchain.jks"
    passwords:
      # These contain the type and password for the CA related files
      ca_key_type:         "PEM"
      ca_key_pass:         "capass"
      # These contain the type and password for the client related files
      client_key_type:     "PEM"
      client_key_pass:     "clientpass"
      client_jks_type:     "JKS" #"PKCS12"
      client_jks_pass:     "altiplanokeystore" #"mypass"
      # These contain the type and password for the server related files
      server_key_type:     "PEM"
      server_key_pass:     "serverpass"
      server_jks_type:     "JKS" #"PKCS12"
      server_jks_pass:     "altiplanokeystore" #"mypass"
      # These contain the type and password for the trustchain related files
      trustchain_crt_type: "PEM"
      trustchain_jks_type: "JKS" #"PKCS12"
      trustchain_jks_pass: "altiplanokeystore" #"mypass"
      # This contains the password for ams_ssl_keystore
      ams_ssl_keystore_pass: "changeit"
  ingress_tcp_config:
    #mariadb service port
    #3306: "{{ .Release.Namespace }}/altiplano-mariadb:3306"
    #webdav http service port
    30080: "{{ .Release.Namespace }}/altiplano-webdav:8080"
    #webdav https service port
    30081: "{{ .Release.Namespace }}/altiplano-webdav:8443"
  ingress_diag_tcp_config:
    #diag http service port
    30082: "{{ .Release.Namespace }}/altiplano-diag:8080"
    #diag https service port
    30083: "{{ .Release.Namespace }}/altiplano-diag:8443"
  ingress_diag_udp_config:
    #diag service port for echo
    30084: "{{ .Release.Namespace }}/altiplano-diag:6380"
  ENABLE_KERBEROS_AUTH: false
  krb:
    krbConfigFilePath: "krb/krb5.conf"
    #Below are configuration to create secret altiplano-mm-sasl-secret
    krbSecretName: &saslSecret "mirror-sasl-secret"
    krbPrincipalKey: &krbPrincipalKey krbPrincipalKey
    krbPrincipalValue: "" #Please correct to your server where MM is running on. e.g kafka/10-131-232-11.bh-dc-os-dhn-40.eecloud.nsn-net.net@NOKIA.COM
    krbKeytabKey: &krbKeytabKey krbKeytabKey
    krbKeytabPath: "krb/mirrormaker.keytab" #To customise, please put your keytab under altiplano-infra/krb/ folder and correct path file.
  # For IPv4 and IPv6 dual stack support at global scope.
  # ipFamilyPolicy:  SingleStack | PreferDualStack | RequireDualStack
  ipFamilyPolicy: PreferDualStack
  # ipFamilies: ["IPv4"] | ["IPv6"] | ["IPv4","IPv6"] | ["IPv6","IPv4"]
  ipFamilies: [ ]
  # - IPv4
  # - IPv6
  existingServiceAccountName: "pre-upgrade-sa-admin"
  allowClusterLevelPrivileges: true
  #Vault Token Connection
  vault:
    enabled: false
    tls:
      enabled: false
      secretName: altiplano-secrets-trustchain-certs
      caCert: trustchain-cert.pem
    config:
      url: "https://altiplano-cskm:8200/v1" # Please provide url of Vault server. E.g: "https://altiplano-cskm-active:8200/v1"
      token: "" #Default token will be read from altiplano-cskm-secret secret if token is not provided. E.g: "hvs.U4tLdHP7GYYE8w2oXpRchE28"
      kvPath: "altiplano"
    endpoints:
      sso_endpoint: "altiplano-sso"
      webdav_endpoint: "altiplano-webdav"
      indexsearch_endpoint: "altiplano-indexsearch"
      rabbitmq_endpoint: "altiplano-nspos-rabbitmq"
      pts_endpoint: "altiplano-secrets"
      grafana_endpoint: "altiplano-grafana"
      redis_endpoint: "altiplano-redis"
      pts_node_exporter_endpoint: "altiplano-pts-node-exporter"
      
#pls provide more endpoint

# Below are configuration for various subcharts
#
altiplano-alarms-indexsearch:
  enabled: false
  bssc-indexsearch:
    nameOverride: altiplano-alarms-indexsearch
    #Containers' SecurityContext can be defined below
    containerSecurityContext:
      seccompProfile:
        type: RuntimeDefault
    accessRoleLabel: internal-access
    custom:
      manager:
        pod:
          annotations:
            kubectl.kubernetes.io/default-container: is-manager
            kubectl.kubernetes.io/default-logs-container: is-manager
      data:
        pod:
          annotations:
            kubectl.kubernetes.io/default-container: is-data
            kubectl.kubernetes.io/default-logs-container: is-data
      client:
        pod:
          annotations:
            kubectl.kubernetes.io/default-container: is-client
            kubectl.kubernetes.io/default-logs-container: is-client
    initContainer:
      repo: fnms-ofd-init
      tag: nokia-2.1.0
      resources:
        limits:
          cpu: "300m"
          memory: "200Mi"
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    certificates:
      secrets:
        altiplano_keystore_secrets: altiplano-keystore-secrets
      fileNames:
        altiplano_keystore_password: keystore-password
        indexsearch_server_key_pem: indexsearch-server-key.pem
        indexsearch_server_key_pass: indexsearch-server-key.pass
        indexsearch_server_cert_pem: indexsearch-server-cert.pem
        indexsearch_server_keystore_jks: server.jks
        indexsearch_server_trustchain_cert_pem: indexsearch-server-trustchain-cert.pem
        altiplano_sso_trustchain_cert_pem: sso-client-trustchain-cert.pem
        indexsearch_server_truststore_jks: trustchain.jks
        client_key_pem: client-key.pem
        client_key_pass: client-key.pass
        client_cert_pem: client-cert.pem
        client_keystore_jks: client.jks
        client_trustchain_cert_pem: indexsearch-server-trustchain-cert.pem
        client_truststore_jks: trustchain-client.jks
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301
    manager:
      livenessProbe:
        initialDelaySeconds: "30"
        periodSeconds: "20"
        timeoutSeconds: "10"
        failureThreshold: "28"
      replicas: 1
      resources:
        limits:
          cpu: "1"
          memory: "2Gi"
        requests:
          cpu: "250m"
          memory: "1Gi"
      java_opts: "-Xms1g -Xmx1g -Dlog4j2.formatMsgNoLookups=true"
      nodeSelector: {}
      image:
        repo: fnms-indexsearch
        tag: nokia-2.1.1

    client:
      livenessProbe:
        initialDelaySeconds: "30"
        periodSeconds: "20"
        timeoutSeconds: "10"
        failureThreshold: "28"
      readinessProbe:
        initialDelaySeconds: "30"
      replicas: 1
      #hostAliases:
      #  - ip: "2001:db8:0:3:f816:3eff:fe5c:a179"
      #    hostnames:
      #      - "altiplano-ipv6"
      resources:
        limits:
          cpu: "1"
          memory: "4500Mi"
        requests:
          cpu: "250m"
          memory: "2Gi"
      java_opts: "-Xms2g -Xmx2g -Dlog4j2.formatMsgNoLookups=true"
      nodeSelector: {}

    data:
      livenessProbe:
        initialDelaySeconds: "30"
        periodSeconds: "20"
        timeoutSeconds: "10"
        failureThreshold: "28"
      replicas: 1
      resources:
        limits:
          cpu: "1"
          memory: "5500Mi"
        requests:
          cpu: "250m"
          memory: "3Gi"
      java_opts: "-Xms3g -Xmx3g -Dlog4j2.formatMsgNoLookups=true"
      nodeSelector: {}

    jobs:
      secAdminUpgradeJob:
        resources:
          requests:
            cpu: 200m
            memory: 500Mi
          limits:
            cpu: 500m
            memory: 1Gi

    #     env:
    #       - name: "PATH_REPO"
    #         value: "/indexsearch-backup"
    persistence:
      enabled: *persistence
      storageClassName: *storageClass
      accessMode: ReadWriteOnce
      # Size of persistent storage of data pod to store the indexsearch  data.
      size: 15Gi
      # Size of persistent storage for master pod to persist cluster state
      managerStorage: 1Gi
      backup:
        enabled: *persistence
        # Size of non-cbur backup persistent storage
        backupStorage: 25Gi
      # set auto_delete to true when the PV also has to be deleted on deletion of the release.
      # this will delete all the previous data stored in the persistent volume.
      # When local storage is used only PVC will get deleted not PV.
      auto_delete: false

    #network_host is set to _site_ by default. To know more about network_host and configure this parameter, please refer https://www.elastic.co/guide/en/elasticsearch/reference/7.0/modules-network.html#network-interface-values
    #For IPv6 environment, this can be set to "_global:ipv6_". If the network interface is known, you can set it to "_[networkInterface]:ipv6_". For ex: "_eth0:ipv6_"
    network_host: "_site_"

    # Backup and restore supports only with Cinder and GlusterFS
    backup_restore:
      ## For production servers this number should likely be much larger.
      size: 20Gi
      storageClassName: *storageClass
      restoreSystemIndices: false
      # When backupGlobalState flag is set to true, existing cluster settings, templates etc will be backed up.
      backupGlobalState: true
      # When restoreGlobalState flag is set to true, existing cluster settings, templates etc whose names match those in the snapshot could be ovewritten.
      restoreGlobalState: true
    cbur:
      enabled: *cburEnable
      apiVersion: *cburApiVersion
      #an integer. This value only applies to statefulset. The value can be 0,1 or 2.
      #Recommended value of brOption is 0.
      brOption: 0
      #the maximum copy you want to saved.
      maxCopy: *backupRetain
      #Modes supported now: "local","NETBKUP","AVAMAR","CEPHS3","AWSS3", case insensitive
      backendMode: *globalBackend
      #Set below parameters to true for auto enabling cron job
      autoEnableCron: true
      #Set below parameter to true in case you want cronjob to be automatically deleted/updated based on autoEnableCron or cronJob parameters
      autoUpdateCron: false
      ## allows user to schedule backups
      cronJob: "0 1 * * *"

      cbura:
        imageRepo: cbur/cbur-agent
        imageTag: 1.2.0-alpine-580
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: "1"
            memory: "2Gi"
          requests:
            cpu: "500m"
            memory: "1Gi"
        #tmp_size is the mounted volume size of /tmp directory for cbur-sidecar.
        #The value should be around double the size of backup_restore.size
        tmp_size: 40Gi
        # Set below parameter to retore the indices with desired suffix.
        # CBUR restore parameters:
        # 1. CBUR_RESTORE_SUFFIX --> Suffix to be added to the indices that are restored.
        # 2. CBUR_RESTORE_SUFFIX_WITHTIME --> If flag is true, time(epoch format) will be added to the suffix of the restored indices. This is applicable only if CBUR_RESTORE_SUFFIX is present.
        # 3. CBUR_RESTORE_INDICES_OVERWRITE --> If this flag is true, the indices which are common in snapshot and the system will be overwritten . This is applicable only if CBUR_RESTORE_SUFFIX is empty. If flag is set to false and there are common indices in snapshot and system , the restore operation will not be performed and the script will exit.
        # 4. CBUR_RESTORE_DELETE_INDICES_NOTPARTOF_RESTORE_SNAPSHOT --> If this flag is true, the indices which are in the system and not part of snapshot will be deleted .
        # 5. RESTORE_SYSTEM_INDICES --> If this parameter is configured in configmap, it will take precedence to the value set to restoreSystemIndices in value.yaml and all system indices will be restored if value is set to true.
      cbur_restore_parameters: |-
        ---
        CBUR_RESTORE_SUFFIX:
        CBUR_RESTORE_SUFFIX_WITHTIME: false
        CBUR_RESTORE_INDICES_OVERWRITE: true
        CBUR_RESTORE_DELETE_INDICES_NOTPARTOF_RESTORE_SNAPSHOT: true
      # Set this flag to 'true' if you want CBUR to backup indexsearch configmaps
      backup_configmap: false

    # One can deploy mulitple elasticsearch in same namespace with different indexsearch service names.
    service:
      name: "altiplano-alarms-indexsearch"
      type: "ClusterIP"
      #client_nodeport: 30092

      #Set prometheus_metrics to true to scrape metrics from elasticsearch
      prometheus_metrics:
        enabled: true
        #If security is enabled, you will have to create a custom scrape job in cpro chart. Refer belk user-guide for the same.
        #Prometheus annotation for scraping metrics from indexsearch https endpoints. If this annotation is modified, make sure to add the same name in custom scrape job created in cpro chart.
        pro_annotation_https_scrape: "prometheus.io/scrape_is"

    upgrade:
      autoMigratePV: false
      preUpgradePVMigrationSAName: ""

    security:
      # if sensitiveInfoInSecret is set to true then user has to provide the pre-created secretName and secret keys.
      # if sensitiveInfoInSecret is set to false then user has to provide the secrets in base64 format and BSSC chart creates and manages the secret.
      # sensitiveInfoInSecret can be enabled only if security.enabled is true
      sensitiveInfoInSecret:
        enabled: true
        credentialNameCert: altiplano-secrets-all-certs
        credentialNamePassword: altiplano-secrets
        keystoreJks: is_keystore.jks
        truststoreJks: is_truststore.jks
        clientKeystoreJks: is_client_keystore.jks
        clientCrtPem: is_client.crt
        clientKeyPem: is_client.key
        trustPass: is_truststore_password
        keyPass: is_keystore_password
        keycloakRootCaPem: trustchain-cert.pem
        secInternalUserYml: is_internal_user_yml
        kibanaServerUserPwd: kibana_is_password
        isInternalUser: indexsearch_client_username
        isInternalPass: indexsearch_client_password
      enable: true
      keycloak_auth: true
      sec_configmap:
        # refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#action-groups
        action_groups_yml: |-
          ---
          _meta:
            type: "actiongroups"
            config_version: 2

        # Refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#configuration
        config_yml: |-
          ---
          _meta:
            type: "config"
            config_version: 2
          config:
            dynamic:
              kibana:
                #To enable multitenancy set "multitenancy_enabled" to true. server_username is fixed to kibanaserver. When sensitiveInfoInSecrets is enabled then server_username will be populated internally so server_username can be kept empty and if sensitiveInfoInSecrets is disabled then server_username has to be kibanaserver user(It is is a user which makes requests to elasticsearch cluster from kibana server).
                multitenancy_enabled: false
                server_username: kibanaserver
              http:
                anonymous_auth_enabled: false
                xff:
                  enabled: false
                  internalProxies: '.+'
              authc:
                basic_internal_auth_domain:
                  http_enabled: true
                  transport_enabled: true
                  order: 0
                  http_authenticator:
                    type: "basic"
                    challenge: false   # Set this to false when keycloak authentication is enabled
                    config: {}
                  authentication_backend:
                    type: "intern"
                    config: {}
                openid_auth_domain_ui: # by default this is referring to 'master' realm in Keycloak, which is used for UI operations
                  http_enabled: true  # Set to true to enable keycloak authentication
                  transport_enabled: true
                  order: 1
                  http_authenticator:
                    type: openid
                    challenge: false
                    config:
                      subject_key: preferred_username
                      roles_key: roles
                      openid_connect_url: https://altiplano-sso:8443/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration
                      openid_connect_idp:
                        enable_ssl: true
                        verify_hostnames: false
                        trust_all: false      ## Set to true when istio is enabled
                        # if istio is enabled, do not remove the pemtrustedcas_filepath entry, set its value to "".
                        pemtrustedcas_filepath: "/etc/opensearch/config/certs/keycloakRootCaPem"
                  authentication_backend:
                      type: noop
                openid_auth_domain_nbi: # by default this is referring to 'system' realm in Keycloak, which is used for NBI/OSS operations
                  http_enabled: true  # Set to true to enable keycloak authentication
                  transport_enabled: true
                  order: 2
                  http_authenticator:
                    type: openid
                    challenge: false
                    config:
                      subject_key: preferred_username
                      roles_key: roles
                      openid_connect_url: https://altiplano-sso:8443/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_NBI_REALM }}/.well-known/openid-configuration
                      openid_connect_idp:
                        enable_ssl: true
                        verify_hostnames: false
                        trust_all: false      ## Set to true when istio is enabled
                        # if istio is enabled, do not remove the pemtrustedcas_filepath entry, set its value to "".
                        pemtrustedcas_filepath: "/etc/opensearch/config/certs/keycloakRootCaPem"
                  authentication_backend:
                      type: noop
                proxy_auth_domain:
                  http_enabled: false
                  transport_enabled: false
                  order: 4
                  http_authenticator:
                    type: "proxy"
                    challenge: false
                    config:
                      user_header: "x-proxy-user"
                      #roles_header: "x-proxy-roles"
                  authentication_backend:
                    type: "noop"
                    config: {}
                clientcert_auth_domain:
                  http_enabled: false
                  transport_enabled: false
                  order: 3
                  http_authenticator:
                    challenge: false
                    type: "clientcert"
                    config:
                      username_attribute: "cn"
                  authentication_backend:
                    type: "noop"
        # Refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#roles
        # Define your roles in this section.
        roles_yml: |-
          ---
          _meta:
            type: "roles"
            config_version: 2
          kibana_read_only:
            reserved: false

          security_rest_api_access:
            reserved: false

          indexsearch_dashboard_admin_role:
            cluster_permissions:
              - unlimited
            index_permissions:
              - index_patterns:
                - '*'
                allowed_actions:
                  - unlimited
          
          indexsearch_dashboard_readonly_role:
            cluster_permissions:
                - "cluster_composite_ops_ro"
            index_permissions:
              - index_patterns:
                  - "*"
                allowed_actions:
                    - "read"
                    - "get"
                    - "search"

        # Refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#role-mappings
        # Define rolesmapping in this section.
        roles_mapping_yml: |-
          ---
          _meta:
            type: "rolesmapping"
            config_version: 2
          indexsearch_dashboard_admin_role:
            reserved: true
            backend_roles:
            - "ALTIPLANO_INDEXSEARCH_DASHBOARD_ADMIN_ACCESS"
          all_access:
            reserved: false
            hidden: false
            backend_roles:
            - "admin"
            description: "Migrated from v6"
          own_index:
            reserved: false
            hidden: false
            users:
            - "*"
          kibana_user:
            reserved: false
            backend_roles:
            - "kibanauser"
            description: "Maps kibanauser to kibana_user role"
          readall:
            reserved: false
            backend_roles:
            - "readall"
          kibana_server:
            reserved: false
            users:
            - "kibanaserver"
          
          indexsearch_dashboard_readonly_role:
            reserved: true
            backend_roles:
              - "ALTIPLANO_INDEXSEARCH_DASHBOARD_ACCESS"  

        #Refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#tenants
        tenants_yml: |-
          ---
          _meta:
            type: "tenants"
            config_version: 2

  ## Overwrite values for dashboards
  #Follow BSSC userguide to create dashboards server certificates
  bssc-dashboards:
    nameOverride: altiplano-alarms-indexsearch-dashboards
    accessRoleLabel: internal-access
    initContainer:
      repo: fnms-ofd-init
      tag: nokia-2.1.0
      resources:
        limits:
          cpu: "300m"
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301
    security:
      sensitiveInfoInSecret:
        enabled: true
        credentialNameCert: altiplano-indexsearch-dashboards-certificate-secrets
        credentialNamePassword: altiplano-secrets
        dboServerCrt: indexsearch-dashboards-server-cert.pem
        dboServerKey: indexsearch-dashboards-server-key.pem
        dboIsPassword: kibana_is_password
        IsRootCaPem: indexsearch-dashboards-server-trustchain-cert.pem
        keycloakRootCaPem: sso-client-trustchain-cert.pem
        keycloakClientId: keycloak_client_id
        keycloakClientSecret: keycloak_client_secret
        IngressRootCaPem: ingress-client-trustchain-cert.pem

      enable: true
      keycloak_auth: true
      dashboards:
        is_ssl_verification_mode: certificate
    dboPluginsInit:
      resources:
        limits:
          #cpu: "50m"
          memory: "300Mi"
          ephemeral-storage: "1Gi"
        requests:
          cpu: "20m"
          memory: "80Mi"
          ephemeral-storage: "400Mi"
    dashboards:
      image:
        repo: fnms-indexsearch-dashboards
        tag: nokia-2.1.0
      #Containers' SecurityContext can be defined below
      updateStrategy:
        type: Recreate
      unifiedLogging:
        enabled: false
      certificate:
        enabled: false
      containerSecurityContext:
        seccompProfile:
          type: RuntimeDefault
      livenessProbe:
        probecheck:
          exec:
            command:
              - /bin/sh
              - -c
              - |
                if [ -z $SERVER_HOST ]; then
                   SERVER_HOST=$(head -1 /usr/share/opensearch-dashboards/data/hostname)
                fi
                curl -s -k https://${SERVER_HOST}:5601/api/status | jq .status.overall.state | grep -w green
        initialDelaySeconds: 210
        periodSeconds: 30
        failureThreshold: 5
      readinessProbe:
        probecheck:
          exec:
            command:
              - /bin/sh
              - -c
              - |
                if [ -z $SERVER_HOST ]; then
                   SERVER_HOST=$(head -1 /usr/share/opensearch-dashboards/data/hostname)
                fi
                curl -s -k https://${SERVER_HOST}:5601/api/status | jq .status.overall.state | grep -w green
        initialDelaySeconds: 30
        periodSeconds: 10
        failureThreshold: 3
      #hostAliases:
      #  - ip: "2001:db8:0:3:f816:3eff:fe5c:a179"
      #    hostnames:
      #      - "altiplano-ipv6"
      resources:
        limits:
          cpu: "1000m"
          memory: "2Gi"
        requests:
          cpu: "500m"
          memory: "1Gi"
      keycloakCheck: "https://altiplano-sso-headless:8443"
      indexSearchCheck: "https://altiplano-alarms-indexsearch:9200"
      connectTimeout: "5"
      updateStrategy:
        type: Recreate
      harmonizedLogging:
        enabled: false
      configMaps:
        dashboards_configmap_yml: |-
          ---
          # Donot change sever name and host. This is default configuration.
          server.name: dashboards
          logging.quiet: true
          server.ssl.supportedProtocols: ["TLSv1.2"]
          server.maxPayloadBytes: 26214400
          opensearch_security.cookie.secure: true
          # uncomment below section for keycloak authentication and provide required correct parameters
          opensearch_security.auth.type: "openid"
          {{- if .Values.global.K8S_PUBLIC_HOSTNAME }}
          opensearch_security.openid.connect_url: "https://{{ .Values.global.K8S_PUBLIC_HOSTNAME }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration"
          {{- else }}
          opensearch_security.openid.connect_url: "https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration"
          {{- end }}
          ### When sensitiveInfoInSecret is true then set opensearch_security.openid.client_id: , opensearch_security.openid.client_secret: , opensearch_security.openid.root_ca: to empty
          ### so that they are internally populated from the secrets.
          opensearch_security.openid.client_id:
          opensearch_security.openid.client_secret:
          opensearch_security.openid.root_ca:
          opensearch_security.openid.header: "Authorization"
          ### for kibana service on ingress port is not required
          {{- if .Values.global.K8S_PUBLIC_HOSTNAME }}
          {{- if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}
          opensearch_security.openid.base_redirect_url: "https://{{ .Values.global.K8S_PUBLIC_HOSTNAME }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}/altiplano-alarms-indexsearch-dashboards"
          {{- else }}
          opensearch_security.openid.base_redirect_url: "https://{{ .Values.global.K8S_PUBLIC_HOSTNAME }}/altiplano-alarms-indexsearch-dashboards"
          {{- end }}
          {{- else }}
          {{- if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}
          opensearch_security.openid.base_redirect_url: "https://{{ .Values.global.K8S_PUBLIC_IP }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}/altiplano-alarms-indexsearch-dashboards"
          {{- else }}
          opensearch_security.openid.base_redirect_url: "https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-alarms-indexsearch-dashboards"
          {{- end }}
          {{- end }}
          opensearch_security.openid.verify_hostnames: false
          elasticsearch.healthCheck.delay: 30000
          opensearch.requestTimeout: 900000
          opensearch_security.auth.unauthenticated_routes: ['/api/status']
      env:
        # if security enabled use https instead of http.
        # value is http://<elasticsearch_service_name>.<namespace>:9200.
        # Namespace is required only when elasticsearch and kibana are in different namespace
        # if security is enabled then uncomment SSL and certificate parameters. Do not change the certificate names.
        - name: "OPENSEARCH_HOSTS"
          value: "https://altiplano-alarms-indexsearch:9200"
        - name: "SERVER_SSL_ENABLED"
          value: "true"
        - name: "SERVER_SSL_CERTIFICATE"
          value: "/etc/opensearch-dashboards/certs/opensearch-dashboards.crt.pem"
        - name: "SERVER_SSL_KEY"
          value: "/etc/opensearch-dashboards/certs/opensearch-dashboards.key.pem"
        - name: "GEO_ACTIVE_SITE"
          value: "{{ .Values.global.ALTIPLANO_GEO_ACTIVE_SITE }}"
    nodeSelector: {}
    ingress:
      enabled: true
      # If security is enabled then uncomment 3 annotations. i.e ingress.class, ssl-passthrough, secure-backends.
      annotations:
        nginx.ingress.kubernetes.io/affinity: "cookie"
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/ssl-passthrough: "true"
        nginx.ingress.kubernetes.io/secure-backends: "true"
        nginx.ingress.kubernetes.io/limit-rps: *ingressLimitRps
        nginx.ingress.kubernetes.io/limit-burst-multiplier: *ingressLimitBurstMultiplier
        nginx.ingress.kubernetes.io/configuration-snippet: |
          more_set_headers "X-Frame-Options: DENY";

    nginxIncIngress:
      annotations:
        kubernetes.io/ingress.class: nginx
        nginx.org/mergeable-ingress-type: minion
        nginx.org/location-snippets: |
          add_header X-Frame-Options 'DENY';
          rewrite ^/altiplano-indexsearch-dashboards/(.*)$ /$1 break;
          proxy_set_header Authorization "";
          proxy_set_header X-Forwarded-Prefix         /altiplano-indexsearch-dashboards;
          #To remove the path prefix in URI
          rewrite "(?i)/altiplano-indexsearch-dashboards(/(.*)|$)" /$2 break;
          return 400;
        nginx.org/redirect-to-https: "true"
        nginx.org/ssl-services: altiplano-indexsearch-dashboards
      path: /altiplano-alarms-indexsearch-dashboards
    service:
      #if you are deploying more than one dashboards in the same namespace change the service name
      name: altiplano-alarms-indexsearch-dashboards
      # to access dashboards service via NodePort set service.type to NodePort and set ingress.enable parameter to false
      type: "ClusterIP"
    dbobaseurl:
      url: /altiplano-alarms-indexsearch-dashboards
      #Do not change cg(capture group) parameter below unless you want to change/modify nginx rewrite-target for kibana ingress
      cg: "(/(.*)|$)"

    ##Following parameters are added for configmap specific restore. Enable below flag if you need to restore kibana configmap via helm restore
    cbur:
      enabled: false
  ## Overwrite values for indexmgr
  bssc-indexmgr:
    nameOverride: altiplano-alarm-indexmgr
    accessRoleLabel: internal-access
    security:
      enable: true
      sensitiveInfoInSecret:
        enabled: true
        credentialNameCert: altiplano-indexmgr-certificate-secrets
        credentialNamePassword: altiplano-secrets
        indexmgrIsUsername: curator_is_username
        indexmgrIsPassword: curator_is_password
        ca_certificate: indexmgr-server-trustchain-cert.pem
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301
    indexmgr:
      #container securityContext can be defined below
      containerSecurityContext:
        seccompProfile:
          type: RuntimeDefault
      certificate:
        enabled: false
      unifiedLogging:
        enabled: false
      image:
        #repo: fnms-indexmgr
        #tag: nokia-2.1.0
        initRepo: fnms-ofd-init
        initTag: nokia-2.1.0
        vaultInitRepo: fnms-init-container
        vaultInitTag: nokia-2.0.1
      resources:
        limits:
          cpu: "120m"
          memory: "1Gi"
        requests:
          cpu: "100m"
          memory: "100Mi"
      schedule: "0 23 * * *"
      jobSpec:
        successfulJobsHistoryLimit: 0
      jobTemplateSpec:
        backoffLimit: 0
      configMaps:
        action_file_yml: |-
          ---
          actions:
            1:
              action: delete_indices
              description: "Delete alarms-history indices older than configured values in unit_count"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: alarms-history-
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 90
            2:
              action: delete_indices
              description: "Delete health-alarms indices older than configured values in unit_count"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: health-alarms-
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 90
            3:
              action: delete_snapshots
              description: "Delete snapshots older than configured values in unit_count"
              options:
                repository: is_backup
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 1
        # Having config_yaml WILL override the other config
        config_yml: |-
          ---
          client:
            # communicating elasticsearch via SG certificates
            certificate: ''
            #hosts is the elasticsearch service name and namespace is where the elasticsearch is deployed.
            hosts:
            #<elasticsearch_service_name.namespace>
            - altiplano-alarms-indexsearch
            #elasticsearch username and password.
            http_auth: ''
            master_only: 'false'
            port: 9200
            # ssl_no_validate property should be 'false' when you are using SG
            ssl_no_validate: 'false'
            timeout: '60'
            url_prefix: ''
            # use_ssl property should be true when you are using SG.
            use_ssl: 'true'
          logging:
            blacklist:
            - elasticsearch
            - urllib3
            logfile: ''
            logformat: default
            loglevel: INFO
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301

altiplano-standalone-fluentd:
  bssc-indexsearch:
    enabled: false
  bssc-dashboards:
    enabled: false
  bssc-indexmgr:
    enabled: false
  bssc-fluentd:
    enabled: true
    rbac:
      enabled: true
      # To read container logs (i.e. when enable_root_privilege is true), PSP (on kubernetes) / SCC (on openshift) are required to be created.
      # If not reading container logs, these can be set to false as 'restricted' PSP/SCC would be sufficient to run the chart.
      psp:
        create: false
      scc:
        create: false
    nameOverride: altiplano-fluentd
    accessRoleLabel: external-access
    timezone:
      # This value has precedence over "global.timeZoneEnv".
      # Example: To test IST(India Standard Time) which is UTC +5:30, you need to use "Asia/Calcutta".
      # Timezone full names can be found here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
      timeZoneEnv: *timeZoneEnv

    # Dual-stack config for services in fluentd chart. Chart level scope takes precedence over global level scope.
    #ipFamilyPolicy:  SingleStack | PreferDualStack | RequireDualStack
    ipFamilyPolicy:
    #ipFamilies: ["IPv4"] | ["IPv6"] | ["IPv4","IPv6"] | ["IPv6","IPv4"]
    ipFamilies: [ ]
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301
    fluentd:
      kind: DaemonSet
      custom:
        pod:
          labels:
            app: bssc-fluentd
      image:
        repo: fnms-fluent
        tag: nokia-4.1.8
        initRepo: fnms-ofd-init
        initTag: nokia-2.1.0
        vaultInitRepo: fnms-init-container
        vaultInitTag: nokia-2.0.1
      ImagePullPolicy: "IfNotPresent"
      certificate:
        enabled: false
      init_resources:
        limits:
          cpu: "300m"
          memory: "100Mi"
          ephemeral-storage: "1Gi"
        requests:
          cpu: "50m"
          memory: "50Mi"
          ephemeral-storage: "200Mi"
      replicas: 1
      podManagementPolicy: Parallel
      unifiedLogging:
        enabled: false
      # Example of job definition:
      # .---------------- minute (0 - 59)
      # |  .------------- hour (0 - 23)
      # |  |  .---------- day of month (1 - 31)
      # |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
      # |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
      # |  |  |  |  |
      # *  *  *  *  *
      CLEAN_STALE_PTS_METRICS_SCHEDULE: "*/15    "
      updateStrategy:
        type: RollingUpdate
      daemonsetSuffix: "-ds"
      EnvVars:
        IS_URL: "https://altiplano-indexsearch:9200"
        PTS_URL: "http://altiplano-pts-pushgateway:9091"
        OPENTSDB_URL: "http://altiplano-opentsdb:4242"
        CREATE_INDEX_TEMPLATE: "true"
        LOG_INDEX_PATTERN: "logstash"
        UAL_INDEX_PATTERN: "user-activity-log"
        NUMBER_OF_REPLICAS: "1"
        LOGSTASH_FORMAT: "true"
        IS_PROTO: "https"
        IS_IP: "altiplano-indexsearch"
        IS_PORT: "9200"

      #PodDisruptionBudgets for fluentd
      #This will ensure that configured number of pods are always up and try to prevent the evictions
      #https://kubernetes.io/docs/tasks/run-application/configure-pdb/
      #Note: Either minAvailable or maxUnavailable should be used. Both cannot be used at the same time.
      #To enable/disable creation of Pod Disruption budget for fluentd pods.
      #If kind is daemonset (default), this section is not applicable and PDB is not created. If kind is statefulset/deployment, PDB can be enabled.
      #Atleast 50% of the fluentd pods are recommended to be up to handle the incoming traffic. The minAavailable value can be changed based on need and incoming load.
      #As default replica of fluentd is 1 and pdb.minAvailable is 50% when kind is configured as Deployment/statefulset, it will not allow to drain the node and service will not get interrupted.
      #
      #If user still wants to drain the node:
      #option 1: If service outage is not acceptable - increase the num of replicas for fluentd so that the PDB conditions are met.
      #option 2: If service outage is acceptable - either disable PDB or set minAvailable to empty and maxUnavailable to 1 so that node can be drained. |  `50%` |
      pdb:
        enabled: false
        minAvailable: 50%
        maxUnavailable:

      #Sets PriorityClass for fluentd pods. If left blank or empty quotes, pods will be configured to cluster default PriorityClass.
      #fluentd.priorityClassName has higher precedence than priorityClassName in global scope
      priorityClassName: ""

      enable_root_privilege: false
      securityContext:
        enabled: true
        # runAsUser is the UID with which fluentd containers run. When enable_root_privilege: true fluentd will run as root user irrespective of value configured in runAsUser. If deploying in openshift environment, the value can be set as "auto".
        runAsUser: 1000
        runAsGroup: 1000
        # fsGroup is the gid that is assigned for the volumemounts mounted to the pod(fsGroup ID is used for block storage). If deploying in openshift environment, the value can be set as "auto".
        fsGroup: 998
        # The supplementalGroups ID applies to shared storage volumes. Uncomment below line to set supplemetary group.
        #supplementalGroups: [998]
        # Uncomment below lines lines to configure SELinux label to a container.
        # For more details refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        #seLinuxOptions:
        #  level: "s0:c23,c123"
        #Set 'privileged' to 'True' to read container logs (i.e. when 'enable_root_privilege' is true) on Openshift and NCS22 environement.
        privileged: False

      #Containers' SecurityContext can be defined below
      containerSecurityContext:
        #When enable_root_privilege is set to 'true', fluentd containers will run as root user (runAsNonRoot: false) irrespective of value configured in runAsNonRoot.
        runAsNonRoot: true
        ##Set 'allowPrivilegeEscalation' to 'True' when securityContext.privileged is set to True
        allowPrivilegeEscalation: false
        seccompProfile:
          type: RuntimeDefault

      resources:
        limits:
          cpu: 1000m
          memory: 1500Mi
          ephemeral-storage: "1Gi"
        requests:
          cpu: 350m
          memory: 1Gi
          ephemeral-storage: "1Gi"

      sensitiveInfoInSecret:
        enabled: true
        credentialNameCert: altiplano-fluentd-certificate-secrets
        credentialNamePassword: altiplano-secrets
        secretDataPassword:
          fluentd_indexsearch_username: fluentd_is_username
          fluentd_indexsearch_password: fluentd_is_password
        secretDataCert:
          fluentd_isroot_cert: fluentd-server-trustchain-cert.pem
          log_forward_cert: fluentd-server-trustchain-cert.pem

      fluentd_config: custom-value
      configFile: |
        <system>
          log_level info
        </system>

        <source>
          @type forward
          port 24224
          bind "0.0.0.0"
        </source>

        <source>
          @type forward
          port 24224
          bind "::"
        </source>

        # Generate FluentD metrics
        <source>
          @type prometheus
        </source>
        <source>
          @type prometheus_monitor
          <labels>
            host ${hostname}
          </labels>
          interval 15s
        </source>

        <match fluent.test.checker>
          @type null
        </match>

        <source>
          @type syslog
          port 42185
          bind "0.0.0.0"
          tag device
          message_format auto
          @label @SYSLOG
          keep_time_key true
        </source>
        <source>
          @type syslog
          port 42185
          bind "::"
          tag device
          message_format auto
          @label @SYSLOG
          keep_time_key true
        </source>

        <match **>
          @type relabel
          @label @NORMAL
        </match>

        <label @NORMAL>
            <match fluent.**>
              @type null
            </match>

            #this filter handles log events in Json format. If log format is not in Json (mostly from echo, see FNMS-7280), this logs a warning but still
            # moving to the next filters
            <filter *.*>
              @type parser
              key_name log
              reserve_data true
              <parse>
                @type json
              </parse>
              emit_invalid_record_to_error false
            </filter>

            <filter *.*>
              @type record_transformer

              #this allows ruby syntax in the below conversion
              enable_ruby true
              <record>
                # nil might be log from fluent itself
                container_name ${record["container_name"] != nil ? record["container_name"].sub('/','') : 'fluentd'}
              </record>
            </filter>

            <filter *.*>
              @type record_transformer

              #this allows ruby syntax in the below conversion
              enable_ruby true
              <record>
                #convert date from milli seconds to %Y-%m-%dT%H:%M:%S.%3N%z format. If 'date' is not available in log event (mostly from echo, see FNMS-7280),
                # then take from Fluentd local time
                date ${(record["date"] != nil && (record["date"].is_a? Integer) == true) ? Time.at(record["date"]/1000.0).strftime('%Y-%m-%dT%H:%M:%S.%3N%z') : Time.now.strftime('%Y-%m-%dT%H:%M:%S.%3N%z')}

                #if 'message' is not available in log event (mostly from echo, see FNMS-7280), then take from the 'log' field.
                message ${record["log"] == nil ? record : [ record["message"] != nil ? record["message"].to_s :  [( record["category"] != nil  && record["category"].include?('com.nokia.anv.logging.UALLoggerImpl')) ? "" :  [JSON.parse(record["log"]).key?("message") ? JSON.parse(record["log"])["message"].to_s : record["log"]  ] ]] }

                #if 'category' is not available in log event (mostly from echo, see FNMS-7280), then take from the container name.
                category ${record["category"] != nil ? record["category"] : record["container_name"]}

                metrics ${record["metric"] != nil ? record["metric"].start_with?("ibn.", "ipdr.exporter.collector.messages.sent.since.") ? "ibn" : "other" : "other"}
              </record>
              #remove log and fluentd fields
              remove_keys log,record,chunk_id,next_retry_seconds,retry_time,chunk,error
            </filter>

            <match *.*>
                @type rewrite_tag_filter
                capitalize_regex_backreference yes
                <rule>
                    key metrics
                    pattern /ibn/
                    tag logstash-ibn-metrics
                </rule>
                <rule>
                    key category
                    pattern ^(com\.nokia\.anv\.sm\.)(api\.MetricManager|consumer\.MetricAnnotationHandler)$
                    tag logstash-metric
                </rule>
                <rule>
                    key category
                    pattern ^(com\.nokia\.anv\.logging\.)(UALLoggerImpl)$
                    tag logstash-ual
                </rule>
                <rule>
                    key category
                    pattern .+
                    tag logstash-normal
                </rule>
            </match>

            <filter logstash-ual>
              @type record_transformer
              renew_record true

              #this allows ruby syntax in the below conversion
              enable_ruby true
              #keep only UAL related keys
              keep_keys date,invocation_time,user,session,application_name,operation,arguments,payload,result,delegate_user,delegate_session,container_name,container_id
            </filter>

            <match logstash-ibn-metrics>
                @type copy
                <store>
                    @type opentsdb_metrics
                    url "#{ENV['OPENTSDB_URL']}"
                    <buffer>
                        @type file
                        path /tmp/fluentd/buffer_opentsdb/
                        overflow_action drop_oldest_chunk
                        chunk_limit_size 16MB
                        queued_chunks_limit_size  4096
                        flush_thread_count 5
                        flush_interval 5s
                        retry_max_times 0
                        total_limit_size 50MB
                    </buffer>
                </store>
                <store>
                    @type rewrite_tag_filter
                    capitalize_regex_backreference yes
                    <rule>
                        key metrics
                        pattern /ibn/
                        tag logstash-metric
                    </rule>
                </store>
            </match>

            <match logstash-metric>
                @type prometheus_metrics
                url "#{ENV['PTS_URL']}"
                <buffer>
                    @type file
                    path /tmp/fluentd/buffer_pts/
                    overflow_action drop_oldest_chunk
                    chunk_limit_size 16MB
                    queued_chunks_limit_size  4096
                    flush_thread_count 1
                    flush_interval 5s
                    total_limit_size 50MB
                    retry_max_times 0
                </buffer>
            </match>

            <match logstash-ual>
                @type copy
                <store>
                    @type opensearch
                    reload_on_failure true
                    reconnect_on_error true
                    logstash_format "#{ENV['LOGSTASH_FORMAT']}"
                    type_name fluentd
                    ssl_verify false
                    ssl_version TLSv1_2
                    scheme "#{ENV['IS_PROTO']}"
                    host "#{ENV['IS_IP']}"
                    port "#{ENV['IS_PORT']}"
                    ca_file <<fluentd_isroot_cert>>
                    user <<fluentd_indexsearch_username>>
                    password <<fluentd_indexsearch_password>>
                    <buffer>
                         @type file
                          path /tmp/fluentd/buffer_ual/
                          overflow_action drop_oldest_chunk
                          chunk_limit_size 16MB
                          queued_chunks_limit_size  4096
                          flush_thread_count 5
                          flush_interval 5s
                          retry_wait 0s
                          retry_forever true
                          total_limit_size 50MB
                    </buffer>
                    time_key date
                    time_key_exclude_timestamp true
                    index_name "#{ENV['UAL_INDEX_PATTERN']}"
                    logstash_prefix "#{ENV['UAL_INDEX_PATTERN']}"
                    request_timeout 45s
                    suppress_type_name true
                </store>
                <store>
                    @type relabel
                    @label @UAL_LOG_FORWARD
                </store>
            </match>

            <match logstash-normal>
                @type copy
                <store>
                    @type opensearch
                    reload_on_failure true
                    reconnect_on_error true
                    logstash_format "#{ENV['LOGSTASH_FORMAT']}"
                    type_name fluentd
                    ssl_verify false
                    ssl_version TLSv1_2
                    scheme "#{ENV['IS_PROTO']}"
                    host "#{ENV['IS_IP']}"
                    port "#{ENV['IS_PORT']}"
                    ca_file <<fluentd_isroot_cert>>
                    user <<fluentd_indexsearch_username>>
                    password <<fluentd_indexsearch_password>>
                    <buffer>
                       @type file
                        path /tmp/fluentd/buffer_es/
                        overflow_action drop_oldest_chunk
                        chunk_limit_size 16MB
                        queued_chunks_limit_size  4096
                        flush_thread_count 5
                        flush_interval 5s
                        retry_wait 0s
                        retry_forever true
                        total_limit_size 50MB
                    </buffer>
                    time_key date
                    time_key_exclude_timestamp true
                    index_name "#{ENV['LOG_INDEX_PATTERN']}"
                    logstash_prefix "#{ENV['LOG_INDEX_PATTERN']}"
                    request_timeout 45s
                    suppress_type_name true
                </store>
                <store>
                    @type relabel
                    @label @NORMAL_LOG_FORWARD
                </store>
            </match>
        </label>

        <label @SYSLOG>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                keys ["APP_NAME", "APP_VERSION", "ENTITY_NAME", "ENTITY_TYPE", "FLOW_ID", "LEVEL", "MODULE_NAME", "IFTAG", "APP_PHASE", "INTERFACE_INDEX", "LAMGNT", "PROTOCOL_NAME", "PROTOCOL_TYPE"]
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                key_values ${record["message"].split(",").select{|m| record["keys"].include?m.split(":")[0].strip}.join(",")}
                MSG ${record["message"].split(",").select{|m| !record["keys"].include?m.split(":")[0].strip}.join(",").strip}
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                key_values ${ record["key_values"].size > 0 ? "{\"" + record["key_values"].strip.gsub('"','\"').gsub(/\s*:\s*/,'":"').gsub(/\s*,\s*/,'","') + "\"}" : "{}"}
                facility_severity ${tag_suffix[1]}
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                today_date ${Date.today()}
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                #Scenario1: where the syslog time is in Dec of previous year(say 2020) and fluent time is in Jan(say 2021) of current year. We want the previous year(2020) as syslog year
                #Scenario2: where the syslog time is in Jan of next year(say 2021) and fluent time is in Dec(say 2020) of current year. We want the next year(2021) as syslog year
                #All other scenarios: we want the current year as the syslog year
                syslog_year ${(Date.parse(record["time"]).month == 12 && record["today_date"].month == 1) ? record["today_date"].prev_year.year() : (Date.parse(record["time"]).month == 1 && record["today_date"].month == 12) ? record["today_date"].next_year.year() : record["today_date"].year}
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                #We want to prepend year only for RFC 3164 and not for RFC 5424,because the former already contains year.
                time ${record["time"].start_with?('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') ? record["time"].prepend(record["syslog_year"].to_s + " ") : record["time"]}
              </record>
              remove_keys today_date, syslog_year
            </filter>
            <filter **>
              @type parser
              format json
              key_name key_values
              reserve_data true
            </filter>
            <match **>
                @type copy
                <store>
                    @type opensearch
                    reload_on_failure true
                    reconnect_on_error true
                    ssl_verify false
                    ssl_version TLSv1_2
                    scheme "#{ENV['IS_PROTO']}"
                    host "#{ENV['IS_IP']}"
                    port "#{ENV['IS_PORT']}"
                    ca_file <<fluentd_isroot_cert>>
                    user <<fluentd_indexsearch_username>>
                    password <<fluentd_indexsearch_password>>
                    logstash_format "#{ENV['LOGSTASH_FORMAT']}"
                    logstash_prefix syslog
                    index_name syslog
                    buffer_type file
                    buffer_chunk_limit 16m
                    buffer_queue_limit 4096
                    buffer_path /tmp/fluentd/buffer_syslog/
                    retry_wait 10s
                    flush_interval 5s
                    suppress_type_name true
                </store>
                <store>
                    @type relabel
                    @label @DEVICE_LOG_FORWARD
                </store>
            </match>
        </label>


        <label @NORMAL_LOG_FORWARD>
            <match *.*>
                @type null
            </match>
        </label>
        <label @UAL_LOG_FORWARD>
            <match *.*>
                @type null
            </match>
        </label>
        <label @DEVICE_LOG_FORWARD>
            <match *.*>
                @type null
            </match>
        </label>

      # set the enabled value to true if some service to be exposed from fluentd like fluentd-promethues-plugin which exports fluentd metrics so that prometheus can scrap the metrics via this service and port
      # The below section added to enable/disable fluentd-prometheus service.
      service:
        enabled: true
        # if you want to provide your own name for service then provide the value in "custom_name"
        # Default value is template {{ "fullname" . }}
        # Delete the old chart and deploy new chart if you want to configure "custom_name" parameter.
        custom_name: "altiplano-fluentd-prometheus"
        accessRoleLabel: internal-access
        # type of service: None, ClusterIP
        type: ClusterIP
        # metricsPort is for getting fluentd prometheus metrics.
        # 24231 is the default port of fluentd-prometheus-plugin.
        # If metricsPort is changed, update same port in fluentd-prometheus configuration in the respective .conf file and in the prometheus annotation below as well.
        metricsPort: 24231
        annotations: { }
        # protocol for service, This parameter is user configurable.
        protocol: TCP
        appProtocol: tcp
        #If dual-stack is configured for fluentd, add bind :: in the prometheus source section in the fluentd configuration.

      # This section is added to enable/disable fluentd forward service.
      forward_service:
        enabled: true
        # if you want to provide your own name for service then provide the value in "custom_name"
        # Default value is template {{ "fullname" . }}-forwarder
        custom_name: "altiplano-fluentd"
        # source port for forwarder
        port: 24224
        fluentdNodePort: 30024
        syslogPort: 42185
        syslogNodePort: 30042
        # protocol for forwarder,This parameter is user configurable.
        protocol: TCP
        syslogProtocol: UDP
        # appProtocol provides a way to specify an application protocol for each service port, this param is user configurable and the default value is 'tcp'. When tls (ssl) is configured for fluentd forwarder plugin, appProtocol can be set to "tls".
        appProtocol: tcp
        # type of service: None, ClusterIP
        type: NodePort
        annotations: { }
        #If dual-stack is configured for fluentd, add bind :: in the forwarder source section in the fluentd configuration. If using singlestack IPv4 or if trying to access using IPv4 address of the service,then set bind address to 0.0.0.0 (default value).

      # volume_mount_enable: true mounts the directories under volumes: The below volumes are required for reading container logs. set the flag to false when fluentd is running as non-root user.
      volume_mount_enable: false

      ## Node labels for pod assignment
      ### ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
      nodeSelector: { }

      ## Toleration is asking the K8S schedule to ignore a taint
      ### ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      #This toleration is provided to have fluentd pods to be scheduled on all nodes including control nodes that may have a taint NoExecute to fulfill daemonset behaviour.
      #This can be set to empty for deployment/statefulset kind.
      #To configure it to empty, set tolerations: []
      tolerations:
        - operator: 'Exists'
          effect: 'NoExecute'

      livenessProbe:
        initialDelaySeconds: 300
        periodSeconds: 20
        timeoutSeconds: 1
        successThreshold: 1
        failureThreshold: 3
      readinessProbe:
        initialDelaySeconds: 30
        periodSeconds: 15
        timeoutSeconds: 1
        successThreshold: 1
        failureThreshold: 3

      ## Pod scheduling preferences.
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      affinity: { }

    cbur:
      enabled: false

    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301

    cronjob:
      schedule: "*/15 * * * *"
      concurrencyPolicy: "Forbid"
      failedJobsHistoryLimit: "0"
      successfulJobsHistoryLimit: "0"
      suspend: "False"

altiplano-indexsearch:
  bssc-indexsearch:
    nameOverride: altiplano-indexsearch
    #Containers' SecurityContext can be defined below
    containerSecurityContext:
      seccompProfile:
        type: RuntimeDefault
    custom:
      manager:
        pod:
          annotations:
            kubectl.kubernetes.io/default-container: is-manager
            kubectl.kubernetes.io/default-logs-container: is-manager
      data:
        pod:
          annotations:
            kubectl.kubernetes.io/default-container: is-data
            kubectl.kubernetes.io/default-logs-container: is-data
      client:
        pod:
          annotations:
            kubectl.kubernetes.io/default-container: is-client
            kubectl.kubernetes.io/default-logs-container: is-client
    initContainer:
      repo: fnms-ofd-init
      tag: nokia-2.1.0
      resources:
        limits:
          cpu: "300m"
          memory: "200Mi"
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    accessRoleLabel: internal-access
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301
    certificates:
      secrets:
        altiplano_keystore_secrets: altiplano-keystore-secrets
      fileNames:
        altiplano_keystore_password: keystore-password
        indexsearch_server_key_pem: indexsearch-server-key.pem
        indexsearch_server_key_pass: indexsearch-server-key.pass
        indexsearch_server_cert_pem: indexsearch-server-cert.pem
        indexsearch_server_keystore_jks: server.jks
        indexsearch_server_trustchain_cert_pem: indexsearch-server-trustchain-cert.pem
        altiplano_sso_trustchain_cert_pem: sso-client-trustchain-cert.pem
        indexsearch_server_truststore_jks: trustchain.jks
        client_key_pem: client-key.pem
        client_key_pass: client-key.pass
        client_cert_pem: client-cert.pem
        client_keystore_jks: client.jks
        client_trustchain_cert_pem: indexsearch-server-trustchain-cert.pem
        client_truststore_jks: trustchain-client.jks
    manager:
      livenessProbe:
        initialDelaySeconds: "30"
        periodSeconds: "20"
        timeoutSeconds: "10"
        failureThreshold: "28"
      replicas: 1
      resources:
        limits:
          cpu: "500m"
          memory: "2Gi"
        requests:
          cpu: "100m"
          memory: "1500Mi"
      java_opts: "-Xms1g -Xmx1g -Dlog4j2.formatMsgNoLookups=true"
      nodeSelector: {}
      image:
        repo: fnms-indexsearch
        tag: nokia-2.1.1

    client:
      livenessProbe:
        initialDelaySeconds: "30"
        periodSeconds: "20"
        timeoutSeconds: "10"
        failureThreshold: "28"
      readinessProbe:
        initialDelaySeconds: "30"
        timeoutSeconds: "3"
      replicas: 1
      #hostAliases:
      #  - ip: "2001:db8:0:3:f816:3eff:fe5c:a179"
      #    hostnames:
      #      - "altiplano-ipv6"
      resources:
        limits:
          cpu: "500m"
          memory: "2Gi"
        requests:
          cpu: "100m"
          memory: "1500Mi"
      java_opts: "-Xms1g -Xmx1g -Dlog4j2.formatMsgNoLookups=true"
      nodeSelector: {}

    data:
      livenessProbe:
        initialDelaySeconds: "30"
        periodSeconds: "20"
        timeoutSeconds: "10"
        failureThreshold: "28"
      replicas: 1
      resources:
        limits:
          cpu: "1200m"
          memory: "6Gi"
        requests:
          cpu: "300m"
          memory: "3Gi"
      java_opts: "-Xms3g -Xmx3g -Dlog4j2.formatMsgNoLookups=true"
      nodeSelector: {}

    jobs:
      secAdminUpgradeJob:
        resources:
          requests:
            cpu: 200m
            memory: 500Mi
          limits:
            cpu: 500m
            memory: 1Gi

    securityContext:
      enabled: true
      runAsUser: 1000
      runAsGroup: 1000
      fsGroup: 1000
    #     env:
    #       - name: "PATH_REPO"
    #         value: "/indexsearch-backup"
    persistence:
      enabled: *persistence
      storageClassName: *storageClass
      accessMode: ReadWriteOnce
      # Size of persistent storage of data pod to store the indexsearch data.
      size: 15Gi
      # Size of persistent storage for master pod to persist cluster state
      managerStorage: 1Gi
      backup:
        enabled: *persistence
        # Size of non-cbur backup persistent storage
        backupStorage: 25Gi
      # set auto_delete to true when the PV also has to be deleted on deletion of the release.
      # this will delete all the previous data stored in the persistent volume.
      # When local storage is used only PVC will get deleted not PV.
      auto_delete: false

    #network_host is set to _site_ by default. To know more about network_host and configure this parameter, please refer https://www.elastic.co/guide/en/elasticsearch/reference/7.0/modules-network.html#network-interface-values
    #For IPv6 environment, this can be set to "_global:ipv6_". If the network interface is known, you can set it to "_[networkInterface]:ipv6_". For ex: "_eth0:ipv6_"
    network_host: "_site_"

    # Backup and restore supports only with Cinder and GlusterFS
    backup_restore:
      ## For production servers this number should likely be much larger.
      size: 20Gi
      storageClassName: *storageClass
      restoreSystemIndices: false
      # When backupGlobalState flag is set to true, existing cluster settings, templates etc will be backed up.
      backupGlobalState: true
      # When restoreGlobalState flag is set to true, existing cluster settings, templates etc whose names match those in the snapshot could be ovewritten.
      restoreGlobalState: true
    cbur:
      enabled: *cburEnable
      apiVersion: *cburApiVersion
      #an integer. This value only applies to statefulset. The value can be 0,1 or 2.
      #Recommended value of brOption is 0.
      brOption: 0
      #the maximum copy you want to saved.
      maxCopy: *backupRetain
      #Modes supported now: "local","NETBKUP","AVAMAR","CEPHS3","AWSS3", case insensitive
      backendMode: *globalBackend
      #Set below parameters to true for auto enabling cron job
      autoEnableCron: true
      #Set below parameter to true in case you want cronjob to be automatically deleted/updated based on autoEnableCron or cronJob parameters
      autoUpdateCron: false
      ## allows user to schedule backups
      cronJob: "0 1 * * *"

      cbura:
        imageRepo: cbur/cbur-agent
        imageTag: 1.2.0-alpine-580
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: "1"
            memory: "2Gi"
          requests:
            cpu: "500m"
            memory: "1Gi"
        #tmp_size is the mounted volume size of /tmp directory for cbur-sidecar.
        #The value should be around double the size of backup_restore.size
        tmp_size: 40Gi
        # Set below parameter to retore the indices with desired suffix.
        # CBUR restore parameters:
        # 1. CBUR_RESTORE_SUFFIX --> Suffix to be added to the indices that are restored.
        # 2. CBUR_RESTORE_SUFFIX_WITHTIME --> If flag is true, time(epoch format) will be added to the suffix of the restored indices. This is applicable only if CBUR_RESTORE_SUFFIX is present.
        # 3. CBUR_RESTORE_INDICES_OVERWRITE --> If this flag is true, the indices which are common in snapshot and the system will be overwritten . This is applicable only if CBUR_RESTORE_SUFFIX is empty. If flag is set to false and there are common indices in snapshot and system , the restore operation will not be performed and the script will exit.
        # 4. CBUR_RESTORE_DELETE_INDICES_NOTPARTOF_RESTORE_SNAPSHOT --> If this flag is true, the indices which are in the system and not part of snapshot will be deleted .
        # 5. RESTORE_SYSTEM_INDICES --> If this parameter is configured in configmap, it will take precedence to the value set to restoreSystemIndices in value.yaml and all system indices will be restored if value is set to true.
      cbur_restore_parameters: |-
        ---
        CBUR_RESTORE_SUFFIX:
        CBUR_RESTORE_SUFFIX_WITHTIME: false
        CBUR_RESTORE_INDICES_OVERWRITE: true
        CBUR_RESTORE_DELETE_INDICES_NOTPARTOF_RESTORE_SNAPSHOT: true
      # Set this flag to 'true' if you want CBUR to backup indexsearch configmaps
      backup_configmap: false

    # One can deploy mulitple elasticsearch in same namespace with different elasticsearch service names.
    service:
      name: "altiplano-indexsearch"
      type: "ClusterIP"
      #client_nodeport: 30092

      #Set prometheus_metrics to true to scrape metrics from indexsearch
      prometheus_metrics:
        enabled: true
        #If security is enabled, you will have to create a custom scrape job in cpro chart. Refer bssc user-guide for the same.
        #Prometheus annotation for scraping metrics from indexsearch https endpoints. If this annotation is modified, make sure to add the same name in custom scrape job created in cpro chart.
        pro_annotation_https_scrape: "prometheus.io/scrape_is"

    upgrade:
      autoMigratePV: false
      preUpgradePVMigrationSAName: ""

    security:
      # if sensitiveInfoInSecret is set to true then user has to provide the pre-created secretName and secret keys.
      # if sensitiveInfoInSecret is set to false then user has to provide the secrets in base64 format and BSSC chart creates and manages the secret.
      # sensitiveInfoInSecret can be enabled only if security.enabled is true
      sensitiveInfoInSecret:
        enabled: true
        credentialNameCert: altiplano-secrets-all-certs
        credentialNamePassword: altiplano-secrets
        keystoreJks: is_keystore.jks
        truststoreJks: is_truststore.jks
        clientKeystoreJks: is_client_keystore.jks
        clientCrtPem: is_client.crt
        clientKeyPem: is_client.key
        trustPass: is_truststore_password
        keyPass: is_keystore_password
        keycloakRootCaPem: trustchain-cert.pem
        secInternalUserYml: is_internal_user_yml
        kibanaServerUserPwd: kibana_is_password
        isInternalUser: indexsearch_client_username
        isInternalPass: indexsearch_client_password
      enable: true
      keycloak_auth: true
      sec_configmap:
        # refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#action-groups
        action_groups_yml: |-
          ---
          _meta:
            type: "actiongroups"
            config_version: 2

        # Refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#configuration
        config_yml: |-
          ---
          _meta:
            type: "config"
            config_version: 2
          config:
            dynamic:
              kibana:
                #To enable multitenancy set "multitenancy_enabled" to true. server_username is fixed to kibanaserver. When sensitiveInfoInSecrets is enabled then server_username will be populated internally so server_username can be kept empty and if sensitiveInfoInSecrets is disabled then server_username has to be kibanaserver user(It is is a user which makes requests to elasticsearch cluster from kibana server).
                multitenancy_enabled: false
                server_username: kibanaserver
              http:
                anonymous_auth_enabled: false
                xff:
                  enabled: false
                  internalProxies: '.+'
              authc:
                basic_internal_auth_domain:
                  http_enabled: true
                  transport_enabled: true
                  order: 0
                  http_authenticator:
                    type: "basic"
                    challenge: false   # Set this to false when keycloak authentication is enabled
                    config: {}
                  authentication_backend:
                    type: "intern"
                    config: {}
                openid_auth_domain_ui: # by default this is referring to 'master' realm in Keycloak, which is used for UI operations
                  http_enabled: true  # Set to true to enable keycloak authentication
                  transport_enabled: true
                  order: 1
                  http_authenticator:
                    type: openid
                    challenge: false
                    config:
                      subject_key: preferred_username
                      roles_key: roles
                      openid_connect_url: https://altiplano-sso:8443/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration
                      openid_connect_idp:
                        enable_ssl: true
                        verify_hostnames: false
                        trust_all: false      ## Set to true when istio is enabled
                        # if istio is enabled, do not remove the pemtrustedcas_filepath entry, set its value to "".
                        pemtrustedcas_filepath: "/etc/opensearch/config/certs/keycloakRootCaPem"
                  authentication_backend:
                      type: noop
                openid_auth_domain_nbi: # by default this is referring to 'system' realm in Keycloak, which is used for NBI/OSS operations
                  http_enabled: true  # Set to true to enable keycloak authentication
                  transport_enabled: true
                  order: 2
                  http_authenticator:
                    type: openid
                    challenge: false
                    config:
                      subject_key: preferred_username
                      roles_key: roles
                      openid_connect_url: https://altiplano-sso:8443/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_NBI_REALM }}/.well-known/openid-configuration
                      openid_connect_idp:
                        enable_ssl: true
                        verify_hostnames: false
                        trust_all: false      ## Set to true when istio is enabled
                        # if istio is enabled, do not remove the pemtrustedcas_filepath entry, set its value to "".
                        pemtrustedcas_filepath: "/etc/opensearch/config/certs/keycloakRootCaPem"
                  authentication_backend:
                      type: noop
                proxy_auth_domain:
                  http_enabled: false
                  transport_enabled: false
                  order: 4
                  http_authenticator:
                    type: "proxy"
                    challenge: false
                    config:
                      user_header: "x-proxy-user"
                      #roles_header: "x-proxy-roles"
                  authentication_backend:
                    type: "noop"
                    config: {}
                clientcert_auth_domain:
                  http_enabled: false
                  transport_enabled: false
                  order: 3
                  http_authenticator:
                    challenge: false
                    type: "clientcert"
                    config:
                      username_attribute: "cn"
                  authentication_backend:
                    type: "noop"
        # Refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#roles
        # Define your roles in this section.
        roles_yml: |-
          ---
          _meta:
            type: "roles"
            config_version: 2
          kibana_read_only:
            reserved: false

          security_rest_api_access:
            reserved: false

          indexsearch_dashboard_admin_role:
            cluster_permissions:
              - unlimited
            index_permissions:
              - index_patterns:
                - '*'
                allowed_actions:
                  - unlimited
          
          indexsearch_dashboard_readonly_role:
            cluster_permissions:
                - "cluster_composite_ops_ro"
            index_permissions:
              - index_patterns:
                  - "*"
                allowed_actions:
                    - "read"
                    - "get"
                    - "search"

        # Refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#role-mappings
        # Define rolesmapping in this section.
        roles_mapping_yml: |-
          ---
          _meta:
            type: "rolesmapping"
            config_version: 2
          indexsearch_dashboard_admin_role:
            reserved: true
            backend_roles:
            - "ALTIPLANO_INDEXSEARCH_DASHBOARD_ADMIN_ACCESS"
          all_access:
            reserved: false
            hidden: false
            backend_roles:
            - "admin"
            description: "Migrated from v6"
          own_index:
            reserved: false
            hidden: false
            users:
            - "*"
          kibana_user:
            reserved: false
            backend_roles:
            - "kibanauser"
            description: "Maps kibanauser to kibana_user role"
          readall:
            reserved: false
            backend_roles:
            - "readall"
          kibana_server:
            reserved: false
            users:
            - "kibanaserver"
          
          indexsearch_dashboard_readonly_role:
            reserved: true
            backend_roles:
              - "ALTIPLANO_INDEXSEARCH_DASHBOARD_ACCESS" 

        #Refer https://opendistro.github.io/for-elasticsearch-docs/docs/security/access-control/api/#tenants
        tenants_yml: |-
          ---
          _meta:
            type: "tenants"
            config_version: 2

  ## Overwrite values for dashboards
  #Follow BSSC to create dashboards server certificates
  bssc-dashboards:
    nameOverride: altiplano-indexsearch-dashboards
    fnmsinitContainer:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    initContainer:
      repo: fnms-ofd-init
      tag: nokia-2.1.0
      resources:
        limits:
          cpu: "300m"
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    accessRoleLabel: internal-access
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301
    security:
      sensitiveInfoInSecret:
        enabled: true
        credentialNameCert: altiplano-indexsearch-dashboards-certificate-secrets
        credentialNamePassword: altiplano-secrets
        dboServerCrt: indexsearch-dashboards-server-cert.pem
        dboServerKey: indexsearch-dashboards-server-key.pem
        dboIsPassword: kibana_is_password
        IsRootCaPem: indexsearch-dashboards-server-trustchain-cert.pem
        keycloakRootCaPem: sso-client-trustchain-cert.pem
        keycloakClientId: keycloak_client_id
        keycloakClientSecret: keycloak_client_secret
        IngressRootCaPem: ingress-client-trustchain-cert.pem

      enable: true
      keycloak_auth: true
      dashboards:
        is_ssl_verification_mode: certificate
    dboPluginsInit:
      resources:
        limits:
          #cpu: "50m"
          memory: "300Mi"
          ephemeral-storage: "1Gi"
        requests:
          cpu: "20m"
          memory: "80Mi"
          ephemeral-storage: "400Mi"
    dashboards:
      image:
        repo: fnms-indexsearch-dashboards
        tag: nokia-2.1.0
      unifiedLogging:
        enabled: false
      updateStrategy:
        type: Recreate
      certificate:
        enabled: false
      #Containers' SecurityContext can be defined below
      containerSecurityContext:
        seccompProfile:
          type: RuntimeDefault
      livenessProbe:
        probecheck:
          exec:
            command:
              - /bin/sh
              - -c
              - |
                if [ -z $SERVER_HOST ]; then
                   SERVER_HOST=$(head -1 /usr/share/opensearch-dashboards/data/hostname)
                fi
                curl -s -k https://${SERVER_HOST}:5601/api/status | jq .status.overall.state | grep -w green
        initialDelaySeconds: 210
        periodSeconds: 30
        failureThreshold: 5
      readinessProbe:
        probecheck:
          exec:
            command:
              - /bin/sh
              - -c
              - |
                if [ -z $SERVER_HOST ]; then
                   SERVER_HOST=$(head -1 /usr/share/opensearch-dashboards/data/hostname)
                fi
                curl -s -k https://${SERVER_HOST}:5601/api/status | jq .status.overall.state | grep -w green
        initialDelaySeconds: 30
        periodSeconds: 10
        failureThreshold: 3
      securityContext:
        enabled: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      #hostAliases:
      #  - ip: "2001:db8:0:3:f816:3eff:fe5c:a179"
      #    hostnames:
      #      - "altiplano-ipv6"
      resources:
        limits:
          cpu: "1000m"
          memory: "2Gi"
        requests:
          cpu: "500m"
          memory: "1Gi"
      keycloakCheck: "https://altiplano-sso-headless:8443"
      indexSearchCheck: "https://altiplano-indexsearch:9200"
      connectTimeout: "5"
      updateStrategy:
        type: Recreate
      harmonizedLogging:
        enabled: false
      configMaps:
        dashboards_configmap_yml: |-
          ---
          # Donot change sever name and host. This is default configuration.
          server.name: dashboards
          logging.quiet: true
          server.ssl.supportedProtocols: ["TLSv1.2"]
          server.maxPayloadBytes: 26214400
          opensearch_security.cookie.secure: true
          # uncomment below section for keycloak authentication and provide required correct parameters
          opensearch_security.auth.type: "openid"
          {{- if .Values.global.K8S_PUBLIC_HOSTNAME }}
          {{- if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}
          opensearch_security.openid.connect_url: "https://{{ .Values.global.K8S_PUBLIC_HOSTNAME }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration"
          {{- else }}
          opensearch_security.openid.connect_url: "https://{{ .Values.global.K8S_PUBLIC_HOSTNAME }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration"
          {{- end }}
          {{- else }}
          {{- if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}
          opensearch_security.openid.connect_url: "https://{{ .Values.global.K8S_PUBLIC_IP }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration"
          {{- else if .Values.global.EXTERNAL_SERVICE_IP }}
          opensearch_security.openid.connect_url: "https://{{ .Values.global.EXTERNAL_SERVICE_IP }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration"
          {{- else }}
          opensearch_security.openid.connect_url: "https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/.well-known/openid-configuration"
          {{- end }}
          {{- end }}
          ### When sensitiveInfoInSecret is true then set opensearch_security.openid.client_id: , opensearch_security.openid.client_secret: , opensearch_security.openid.root_ca: to empty
          ### so that they are internally populated from the secrets.
          opensearch_security.openid.client_id:
          opensearch_security.openid.client_secret:
          opensearch_security.openid.root_ca:
          opensearch_security.openid.header: "Authorization"
          ### for kibana service on ingress port is not required
          {{- if .Values.global.K8S_PUBLIC_HOSTNAME }}
          {{- if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}
          opensearch_security.openid.base_redirect_url: "https://{{ .Values.global.K8S_PUBLIC_HOSTNAME }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}/altiplano-indexsearch-dashboards"
          {{- else }}
          opensearch_security.openid.base_redirect_url: "https://{{ .Values.global.K8S_PUBLIC_HOSTNAME }}/altiplano-indexsearch-dashboards"
          {{- end }}
          {{- else }}
          {{- if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}
          opensearch_security.openid.base_redirect_url: "https://{{ .Values.global.K8S_PUBLIC_IP }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}/altiplano-indexsearch-dashboards"
          {{- else }}
          opensearch_security.openid.base_redirect_url: "https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-indexsearch-dashboards"
          {{- end }}
          {{- end }}
          opensearch_security.openid.verify_hostnames: false
          elasticsearch.healthCheck.delay: 30000
          opensearch.requestTimeout: 900000
          opensearch_security.auth.unauthenticated_routes: ['/api/status']
      env:
        # if security enabled use https instead of http.
        # value is http://<elasticsearch_service_name>.<namespace>:9200.
        # Namespace is required only when elasticsearch and kibana are in different namespace
        # if security is enabled then uncomment SSL and certificate parameters. Do not change the certificate names.
        - name: "OPENSEARCH_HOSTS"
          value: "https://altiplano-indexsearch:9200"
        - name: "SERVER_SSL_ENABLED"
          value: "true"
        - name: "SERVER_SSL_CERTIFICATE"
          value: "/etc/opensearch-dashboards/certs/opensearch-dashboards.crt.pem"
        - name: "SERVER_SSL_KEY"
          value: "/etc/opensearch-dashboards/certs/opensearch-dashboards.key.pem"
        - name: "GEO_ACTIVE_SITE"
          value: "{{ .Values.global.ALTIPLANO_GEO_ACTIVE_SITE }}"
    nodeSelector: {}
    ingress:
      enabled: true
      # If security is enabled then uncomment 3 annotations. i.e ingress.class, ssl-passthrough, secure-backends.
      annotations:
        nginx.ingress.kubernetes.io/affinity: "cookie"
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        kubernetes.io/ingress.class: nginx
        nginx.ingress.kubernetes.io/ssl-passthrough: "true"
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        nginx.ingress.kubernetes.io/secure-backends: "true"
        nginx.ingress.kubernetes.io/limit-rps: *ingressLimitRps
        nginx.ingress.kubernetes.io/limit-burst-multiplier: *ingressLimitBurstMultiplier
        nginx.ingress.kubernetes.io/configuration-snippet: |
          more_set_headers "X-Frame-Options: DENY";
          rewrite ^(/altiplano-indexsearch-dashboards)$ $1/app/home permanent;
    nginxIncIngress:
      annotations:
        kubernetes.io/ingress.class: nginx
        nginx.org/mergeable-ingress-type: minion
        nginx.org/location-snippets: |
          add_header X-Frame-Options 'DENY';
          rewrite ^/altiplano-indexsearch-dashboards/(.*)$ /$1 break;
          proxy_set_header Authorization "";
          proxy_set_header X-Forwarded-Prefix         /altiplano-indexsearch-dashboards;
          #To remove the path prefix in URI
          rewrite "(?i)/altiplano-indexsearch-dashboards(/(.*)|$)" /$2 break;
          return 400;
        nginx.org/redirect-to-https: "true"
        nginx.org/ssl-services: altiplano-indexsearch-dashboards
      path: /altiplano-indexsearch-dashboards
    service:
      #if you are deploying more than one dashboards in the same namespace change the service name
      name: altiplano-indexsearch-dashboards
      # to access dashboards service via NodePort set service.type to NodePort and set ingress.enable parameter to false
      type: "ClusterIP"
    dbobaseurl:
      url: /altiplano-indexsearch-dashboards
      #Do not change cg(capture group) parameter below unless you want to change/modify nginx rewrite-target for kibana ingress
      cg: "(/(.*)|$)"

    ##Following parameters are added for configmap specific restore. Enable below flag if you need to restore dashboards configmap via helm restore
    cbur:
      enabled: false
  ## Overwrite values for indexmgr
  bssc-indexmgr:
    nameOverride: altiplano-indexmgr
    security:
      enable: true
      sensitiveInfoInSecret:
        enabled: true
        credentialNameCert: altiplano-indexmgr-certificate-secrets
        credentialNamePassword: altiplano-secrets
        indexmgrIsUsername: curator_is_username
        indexmgrIsPassword: curator_is_password
        ca_certificate: indexmgr-server-trustchain-cert.pem
    accessRoleLabel: internal-access
    indexmgr:
      #container securityContext can be defined below
      containerSecurityContext:
        seccompProfile:
          type: RuntimeDefault
      certificate:
        enabled: false
      unifiedLogging:
        enabled: false
      image:
        #repo: fnms-indexmgr
        #tag: nokia-2.1.0
        initRepo: fnms-ofd-init
        initTag: nokia-2.1.0
        vaultInitRepo: fnms-init-container
        vaultInitTag: nokia-2.0.1
      resources:
        limits:
          cpu: "120m"
          memory: "1Gi"
        requests:
          cpu: "100m"
          memory: "100Mi"
      schedule: "0 23 * * *"
      jobSpec:
        successfulJobsHistoryLimit: 0
      jobTemplateSpec:
        backoffLimit: 0
      securityContext:
        enabled: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      configMaps:
        action_file_yml: |-
          ---
          actions:
            1:
              action: delete_indices
              description: "Delete logstash indices older than configured values in unit_count"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: logstash-
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 15
            2:
              action: delete_indices
              description: "Delete alarms-history indices older than configured values in unit_count"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: alarms-history-
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 90
            3:
              action: delete_indices
              description: "Delete health-alarms indices older than configured values in unit_count"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: health-alarms-
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 90
            4:
              action: delete_indices
              description: "Delete inventory indices older than configured values in unit_count"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: inv_
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y-%m-%d'
                unit: days
                unit_count: 7
            5:
              action: delete_indices
              description: "Delete user activity log indices older than configured values in unit_count"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: user-activity-log-
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 30
            6:
              action: delete_indices
              description: "Delete syslog indices older than configured values in unit_count"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: syslog-
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 15
            7:
              action: delete_snapshots
              description: "Delete snapshots older than configured values in unit_count"
              options:
                repository: is_backup
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: age
                source: name
                direction: older
                timestring: '%Y.%m.%d'
                unit: days
                unit_count: 1
            8:
              action: delete_indices
              description: "Delete logstash indices if it consumed more than disk-space"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: logstash-
              - filtertype: space
                source: name
                use_age: True
                timestring: '%Y.%m.%d'
                disk_space: 40
            9:
              action: delete_indices
              description: "Delete user activity log indices if it consumed more than disk-space"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: user-activity-log-
              - filtertype: space
                source: name
                use_age: True
                timestring: '%Y.%m.%d'
                disk_space: 40
            10:
              action: delete_indices
              description: "Delete syslog indices if it consumed more than disk-space"
              options:
                timeout_override:
                continue_if_exception: True
                disable_action: False
                ignore_empty_list: True
              filters:
              - filtertype: pattern
                kind: prefix
                value: syslog-
              - filtertype: space
                source: name
                use_age: True
                timestring: '%Y.%m.%d'
                disk_space: 40
        # Having config_yaml WILL override the other config
        config_yml: |-
          ---
          client:
            # communicating elasticsearch via SG certificates
            certificate: ''
            #hosts is the elasticsearch service name and namespace is where the elasticsearch is deployed.
            hosts:
            #<elasticsearch_service_name.namespace>
            - altiplano-indexsearch
            #elasticsearch username and password.
            http_auth: ''
            master_only: 'false'
            port: 9200
            # ssl_no_validate property should be 'false' when you are using SG
            ssl_no_validate: 'false'
            timeout: '60'
            url_prefix: ''
            # use_ssl property should be true when you are using SG.
            use_ssl: 'true'
          logging:
            blacklist:
            - elasticsearch
            - urllib3
            logfile: ''
            logformat: default
            loglevel: INFO
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301
  bssc-fluentd:
    rbac:
      enabled: true
      # To read container logs (i.e. when enable_root_privilege is true), PSP (on kubernetes) / SCC (on openshift) are required to be created.
      # If not reading container logs, these can be set to false as 'restricted' PSP/SCC would be sufficient to run the chart.
      psp:
        create: false
      scc:
        create: false
    nameOverride: altiplano-fluentd
    accessRoleLabel: external-access
    timezone:
      # This value has precedence over "global.timeZoneEnv".
      # Example: To test IST(India Standard Time) which is UTC +5:30, you need to use "Asia/Calcutta".
      # Timezone full names can be found here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
      timeZoneEnv: *timeZoneEnv

    # Dual-stack config for services in fluentd chart. Chart level scope takes precedence over global level scope.
    #ipFamilyPolicy:  SingleStack | PreferDualStack | RequireDualStack
    ipFamilyPolicy:
    #ipFamilies: ["IPv4"] | ["IPv6"] | ["IPv4","IPv6"] | ["IPv6","IPv4"]
    ipFamilies: [ ]
    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301
    fluentd:
      kind: DaemonSet
      custom:
        pod:
          labels:
            app: bssc-fluentd
      image:
        repo: fnms-fluent
        tag: nokia-4.1.8
        initRepo: fnms-ofd-init
        initTag: nokia-2.1.0
        vaultInitRepo: fnms-init-container
        vaultInitTag: nokia-2.0.1
        fnmsInitRepo: fnms-init-container
        fnmsInitTag: nokia-2.0.1
      ImagePullPolicy: "IfNotPresent"
      certificate:
        enabled: false
      init_resources:
        limits:
          cpu: "300m"
          memory: "100Mi"
          ephemeral-storage: "1Gi"
        requests:
          cpu: "50m"
          memory: "50Mi"
          ephemeral-storage: "200Mi"
      replicas: 1
      podManagementPolicy: Parallel
      unifiedLogging:
        enabled: false
      # Example of job definition:
      # .---------------- minute (0 - 59)
      # |  .------------- hour (0 - 23)
      # |  |  .---------- day of month (1 - 31)
      # |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
      # |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
      # |  |  |  |  |
      # *  *  *  *  *
      CLEAN_STALE_PTS_METRICS_SCHEDULE: "*/15    "
      updateStrategy:
        type: RollingUpdate
      daemonsetSuffix: "-ds"
      EnvVars:
        IS_URL: "https://altiplano-indexsearch:9200"
        PTS_URL: "http://altiplano-pts-pushgateway:9091"
        OPENTSDB_URL: "http://altiplano-opentsdb:4242"
        CREATE_INDEX_TEMPLATE: "true"
        LOG_INDEX_PATTERN: "logstash"
        UAL_INDEX_PATTERN: "user-activity-log"
        NUMBER_OF_REPLICAS: "1"
        LOGSTASH_FORMAT: "true"
        IS_PROTO: "https"
        IS_IP: "altiplano-indexsearch"
        IS_PORT: "9200"

      #PodDisruptionBudgets for fluentd
      #This will ensure that configured number of pods are always up and try to prevent the evictions
      #https://kubernetes.io/docs/tasks/run-application/configure-pdb/
      #Note: Either minAvailable or maxUnavailable should be used. Both cannot be used at the same time.
      #To enable/disable creation of Pod Disruption budget for fluentd pods.
      #If kind is daemonset (default), this section is not applicable and PDB is not created. If kind is statefulset/deployment, PDB can be enabled.
      #Atleast 50% of the fluentd pods are recommended to be up to handle the incoming traffic. The minAavailable value can be changed based on need and incoming load.
      #As default replica of fluentd is 1 and pdb.minAvailable is 50% when kind is configured as Deployment/statefulset, it will not allow to drain the node and service will not get interrupted.
      #
      #If user still wants to drain the node:
      #option 1: If service outage is not acceptable - increase the num of replicas for fluentd so that the PDB conditions are met.
      #option 2: If service outage is acceptable - either disable PDB or set minAvailable to empty and maxUnavailable to 1 so that node can be drained. |  `50%` |
      pdb:
        enabled: false
        minAvailable: 50%
        maxUnavailable:

      #Sets PriorityClass for fluentd pods. If left blank or empty quotes, pods will be configured to cluster default PriorityClass.
      #fluentd.priorityClassName has higher precedence than priorityClassName in global scope
      priorityClassName: ""

      enable_root_privilege: false
      securityContext:
        enabled: true
        # runAsUser is the UID with which fluentd containers run. When enable_root_privilege: true fluentd will run as root user irrespective of value configured in runAsUser. If deploying in openshift environment, the value can be set as "auto".
        runAsUser: 1000
        runAsGroup: 1000
        # fsGroup is the gid that is assigned for the volumemounts mounted to the pod(fsGroup ID is used for block storage). If deploying in openshift environment, the value can be set as "auto".
        fsGroup: 998
        # The supplementalGroups ID applies to shared storage volumes. Uncomment below line to set supplemetary group.
        #supplementalGroups: [998]
        # Uncomment below lines lines to configure SELinux label to a container.
        # For more details refer to https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        #seLinuxOptions:
        #  level: "s0:c23,c123"
        #Set 'privileged' to 'True' to read container logs (i.e. when 'enable_root_privilege' is true) on Openshift and NCS22 environement.
        privileged: False

      #Containers' SecurityContext can be defined below
      containerSecurityContext:
        #When enable_root_privilege is set to 'true', fluentd containers will run as root user (runAsNonRoot: false) irrespective of value configured in runAsNonRoot.
        runAsNonRoot: true
        ##Set 'allowPrivilegeEscalation' to 'True' when securityContext.privileged is set to True
        allowPrivilegeEscalation: false
        seccompProfile:
          type: RuntimeDefault

      resources:
        limits:
          cpu: 1000m
          memory: 1500Mi
          ephemeral-storage: "1Gi"
        requests:
          cpu: 350m
          memory: 1Gi
          ephemeral-storage: "1Gi"

      sensitiveInfoInSecret:
        enabled: true
        credentialNameCert: altiplano-fluentd-certificate-secrets
        credentialNamePassword: altiplano-secrets
        secretDataPassword:
          fluentd_indexsearch_username: fluentd_is_username
          fluentd_indexsearch_password: fluentd_is_password
        secretDataCert:
          fluentd_isroot_cert: fluentd-server-trustchain-cert.pem
          log_forward_cert: fluentd-server-trustchain-cert.pem

      fluentd_config: custom-value
      configFile: |
        <system>
          log_level info
        </system>

        <source>
          @type forward
          port 24224
          bind "0.0.0.0"
        </source>

        <source>
          @type forward
          port 24224
          bind "::"
        </source>

        # Generate FluentD metrics
        <source>
          @type prometheus
        </source>
        <source>
          @type prometheus_monitor
          <labels>
            host ${hostname}
          </labels>
          interval 15s
        </source>

        <match fluent.test.checker>
          @type null
        </match>

        <source>
          @type syslog
          port 42185
          bind "0.0.0.0"
          tag device
          message_format auto
          @label @SYSLOG
          keep_time_key true
        </source>
        <source>
          @type syslog
          port 42185
          bind "::"
          tag device
          message_format auto
          @label @SYSLOG
          keep_time_key true
        </source>

        <match **>
          @type relabel
          @label @NORMAL
        </match>

        <label @NORMAL>
            <match fluent.**>
              @type null
            </match>

            #this filter handles log events in Json format. If log format is not in Json (mostly from echo, see FNMS-7280), this logs a warning but still
            # moving to the next filters
            <filter *.*>
              @type parser
              key_name log
              reserve_data true
              <parse>
                @type json
              </parse>
              emit_invalid_record_to_error false
            </filter>

            <filter *.*>
              @type record_transformer

              #this allows ruby syntax in the below conversion
              enable_ruby true
              <record>
                # nil might be log from fluent itself
                container_name ${record["container_name"] != nil ? record["container_name"].sub('/','') : 'fluentd'}
              </record>
            </filter>

            <filter *.*>
              @type record_transformer

              #this allows ruby syntax in the below conversion
              enable_ruby true
              <record>
                #convert date from milli seconds to %Y-%m-%dT%H:%M:%S.%3N%z format. If 'date' is not available in log event (mostly from echo, see FNMS-7280),
                # then take from Fluentd local time
                date ${(record["date"] != nil && (record["date"].is_a? Integer) == true) ? Time.at(record["date"]/1000.0).strftime('%Y-%m-%dT%H:%M:%S.%3N%z') : Time.now.strftime('%Y-%m-%dT%H:%M:%S.%3N%z')}

                #if 'message' is not available in log event (mostly from echo, see FNMS-7280), then take from the 'log' field.
                message ${record["log"] == nil ? record : [ record["message"] != nil ? record["message"].to_s :  [( record["category"] != nil  && record["category"].include?('com.nokia.anv.logging.UALLoggerImpl')) ? "" :  [JSON.parse(record["log"]).key?("message") ? JSON.parse(record["log"])["message"].to_s : record["log"]  ] ]] }

                #if 'category' is not available in log event (mostly from echo, see FNMS-7280), then take from the container name.
                category ${record["category"] != nil ? record["category"] : record["container_name"]}

                metrics ${record["metric"] != nil ? record["metric"].start_with?("ibn.", "ipdr.exporter.collector.messages.sent.since.") ? "ibn" : "other" : "other"}
              </record>
              #remove log and fluentd fields
              remove_keys log,record,chunk_id,next_retry_seconds,retry_time,chunk,error
            </filter>

            <match *.*>
                @type rewrite_tag_filter
                capitalize_regex_backreference yes
                <rule>
                    key metrics
                    pattern /ibn/
                    tag logstash-ibn-metrics
                </rule>
                <rule>
                    key category
                    pattern ^(com\.nokia\.anv\.sm\.)(api\.MetricManager|consumer\.MetricAnnotationHandler)$
                    tag logstash-metric
                </rule>
                <rule>
                    key category
                    pattern ^(com\.nokia\.anv\.logging\.)(UALLoggerImpl)$
                    tag logstash-ual
                </rule>
                <rule>
                    key category
                    pattern .+
                    tag logstash-normal
                </rule>
            </match>

            <filter logstash-ual>
              @type record_transformer
              renew_record true

              #this allows ruby syntax in the below conversion
              enable_ruby true
              #keep only UAL related keys
              keep_keys date,invocation_time,user,session,application_name,operation,arguments,payload,result,delegate_user,delegate_session,container_name,container_id
            </filter>

            <match logstash-ibn-metrics>
                @type copy
                <store>
                    @type opentsdb_metrics
                    url "#{ENV['OPENTSDB_URL']}"
                    <buffer>
                        @type file
                        path /tmp/fluentd/buffer_opentsdb/
                        overflow_action drop_oldest_chunk
                        chunk_limit_size 16MB
                        queued_chunks_limit_size  4096
                        flush_thread_count 5
                        flush_interval 5s
                        retry_max_times 0
                        total_limit_size 50MB
                    </buffer>
                </store>
                <store>
                    @type rewrite_tag_filter
                    capitalize_regex_backreference yes
                    <rule>
                        key metrics
                        pattern /ibn/
                        tag logstash-metric
                    </rule>
                </store>
            </match>

            <match logstash-metric>
                @type prometheus_metrics
                url "#{ENV['PTS_URL']}"
                connection_pool_size 1
                connection_pool_timeout 5
                <buffer>
                    @type file
                    path /tmp/fluentd/buffer_pts/
                    overflow_action drop_oldest_chunk
                    chunk_limit_size 16MB
                    queued_chunks_limit_size 32
                    flush_thread_count 1
                    flush_interval 1s
                    total_limit_size 512MB
                    retry_max_times 3
                    retry_wait 5s
                    retry_randomize false
                </buffer>
            </match>

            <match logstash-ual>
                @type copy
                <store>
                    @type opensearch
                    reload_on_failure true
                    reconnect_on_error true
                    logstash_format "#{ENV['LOGSTASH_FORMAT']}"
                    type_name fluentd
                    ssl_verify false
                    ssl_version TLSv1_2
                    scheme "#{ENV['IS_PROTO']}"
                    host "#{ENV['IS_IP']}"
                    port "#{ENV['IS_PORT']}"
                    ca_file <<fluentd_isroot_cert>>
                    user <<fluentd_indexsearch_username>>
                    password <<fluentd_indexsearch_password>>
                    <buffer>
                         @type file
                          path /tmp/fluentd/buffer_ual/
                          overflow_action drop_oldest_chunk
                          chunk_limit_size 16MB
                          queued_chunks_limit_size  4096
                          flush_thread_count 5
                          flush_interval 5s
                          retry_wait 0s
                          retry_forever true
                          total_limit_size 50MB
                    </buffer>
                    time_key date
                    time_key_exclude_timestamp true
                    index_name "#{ENV['UAL_INDEX_PATTERN']}"
                    logstash_prefix "#{ENV['UAL_INDEX_PATTERN']}"
                    request_timeout 45s
                    suppress_type_name true
                </store>
                <store>
                    @type relabel
                    @label @UAL_LOG_FORWARD
                </store>
            </match>

            <match logstash-normal>
                @type copy
                <store>
                    @type opensearch
                    reload_on_failure true
                    reconnect_on_error true
                    logstash_format "#{ENV['LOGSTASH_FORMAT']}"
                    type_name fluentd
                    ssl_verify false
                    ssl_version TLSv1_2
                    scheme "#{ENV['IS_PROTO']}"
                    host "#{ENV['IS_IP']}"
                    port "#{ENV['IS_PORT']}"
                    ca_file <<fluentd_isroot_cert>>
                    user <<fluentd_indexsearch_username>>
                    password <<fluentd_indexsearch_password>>
                    <buffer>
                       @type file
                        path /tmp/fluentd/buffer_es/
                        overflow_action drop_oldest_chunk
                        chunk_limit_size 16MB
                        queued_chunks_limit_size  4096
                        flush_thread_count 5
                        flush_interval 5s
                        retry_wait 0s
                        retry_forever true
                        total_limit_size 50MB
                    </buffer>
                    time_key date
                    time_key_exclude_timestamp true
                    index_name "#{ENV['LOG_INDEX_PATTERN']}"
                    logstash_prefix "#{ENV['LOG_INDEX_PATTERN']}"
                    request_timeout 45s
                    suppress_type_name true
                </store>
                <store>
                    @type relabel
                    @label @NORMAL_LOG_FORWARD
                </store>
            </match>
        </label>

        <label @SYSLOG>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                keys ["APP_NAME", "APP_VERSION", "ENTITY_NAME", "ENTITY_TYPE", "FLOW_ID", "LEVEL", "MODULE_NAME", "IFTAG", "APP_PHASE", "INTERFACE_INDEX", "LAMGNT", "PROTOCOL_NAME", "PROTOCOL_TYPE"]
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                key_values ${record["message"].split(",").select{|m| record["keys"].include?m.split(":")[0].strip}.join(",")}
                MSG ${record["message"].split(",").select{|m| !record["keys"].include?m.split(":")[0].strip}.join(",").strip}
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                key_values ${ record["key_values"].size > 0 ? "{\"" + record["key_values"].strip.gsub('"','\"').gsub(/\s*:\s*/,'":"').gsub(/\s*,\s*/,'","') + "\"}" : "{}"}
                facility_severity ${tag_suffix[1]}
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                today_date ${Date.today()}
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                #Scenario1: where the syslog time is in Dec of previous year(say 2020) and fluent time is in Jan(say 2021) of current year. We want the previous year(2020) as syslog year
                #Scenario2: where the syslog time is in Jan of next year(say 2021) and fluent time is in Dec(say 2020) of current year. We want the next year(2021) as syslog year
                #All other scenarios: we want the current year as the syslog year
                syslog_year ${(Date.parse(record["time"]).month == 12 && record["today_date"].month == 1) ? record["today_date"].prev_year.year() : (Date.parse(record["time"]).month == 1 && record["today_date"].month == 12) ? record["today_date"].next_year.year() : record["today_date"].year}
              </record>
            </filter>
            <filter **>
            @type record_transformer
            enable_ruby true
              <record>
                #We want to prepend year only for RFC 3164 and not for RFC 5424,because the former already contains year.
                time ${record["time"].start_with?('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') ? record["time"].prepend(record["syslog_year"].to_s + " ") : record["time"]}
              </record>
              remove_keys today_date, syslog_year
            </filter>
            <filter **>
              @type parser
              format json
              key_name key_values
              reserve_data true
            </filter>
            <match **>
                @type copy
                <store>
                    @type opensearch
                    reload_on_failure true
                    reconnect_on_error true
                    ssl_verify false
                    ssl_version TLSv1_2
                    scheme "#{ENV['IS_PROTO']}"
                    host "#{ENV['IS_IP']}"
                    port "#{ENV['IS_PORT']}"
                    ca_file <<fluentd_isroot_cert>>
                    user <<fluentd_indexsearch_username>>
                    password <<fluentd_indexsearch_password>>
                    logstash_format "#{ENV['LOGSTASH_FORMAT']}"
                    logstash_prefix syslog
                    index_name syslog
                    buffer_type file
                    buffer_chunk_limit 16m
                    buffer_queue_limit 4096
                    buffer_path /tmp/fluentd/buffer_syslog/
                    retry_wait 10s
                    flush_interval 5s
                    suppress_type_name true
                </store>
                <store>
                    @type relabel
                    @label @DEVICE_LOG_FORWARD
                </store>
            </match>
        </label>


        <label @NORMAL_LOG_FORWARD>
            <match *.*>
                @type null
            </match>
        </label>
        <label @UAL_LOG_FORWARD>
            <match *.*>
                @type null
            </match>
        </label>
        <label @DEVICE_LOG_FORWARD>
            <match *.*>
                @type null
            </match>
        </label>

      # set the enabled value to true if some service to be exposed from fluentd like fluentd-promethues-plugin which exports fluentd metrics so that prometheus can scrap the metrics via this service and port
      # The below section added to enable/disable fluentd-prometheus service.
      service:
        enabled: true
        # if you want to provide your own name for service then provide the value in "custom_name"
        # Default value is template {{ "fullname" . }}
        # Delete the old chart and deploy new chart if you want to configure "custom_name" parameter.
        custom_name: "altiplano-fluentd-prometheus"
        accessRoleLabel: internal-access
        # type of service: None, ClusterIP
        type: ClusterIP
        # metricsPort is for getting fluentd prometheus metrics.
        # 24231 is the default port of fluentd-prometheus-plugin.
        # If metricsPort is changed, update same port in fluentd-prometheus configuration in the respective .conf file and in the prometheus annotation below as well.
        metricsPort: 24231
        annotations: { }
        # protocol for service, This parameter is user configurable.
        protocol: TCP
        appProtocol: tcp
        #If dual-stack is configured for fluentd, add bind :: in the prometheus source section in the fluentd configuration.

      # This section is added to enable/disable fluentd forward service.
      forward_service:
        enabled: true
        # if you want to provide your own name for service then provide the value in "custom_name"
        # Default value is template {{ "fullname" . }}-forwarder
        custom_name: "altiplano-fluentd"
        # source port for forwarder
        port: 24224
        fluentdNodePort: 30024
        syslogPort: 42185
        syslogNodePort: 30042
        # protocol for forwarder,This parameter is user configurable.
        protocol: TCP
        syslogProtocol: UDP
        # appProtocol provides a way to specify an application protocol for each service port, this param is user configurable and the default value is 'tcp'. When tls (ssl) is configured for fluentd forwarder plugin, appProtocol can be set to "tls".
        appProtocol: tcp
        # type of service: None, ClusterIP
        type: NodePort
        annotations: { }
        #If dual-stack is configured for fluentd, add bind :: in the forwarder source section in the fluentd configuration. If using singlestack IPv4 or if trying to access using IPv4 address of the service,then set bind address to 0.0.0.0 (default value).

      # volume_mount_enable: true mounts the directories under volumes: The below volumes are required for reading container logs. set the flag to false when fluentd is running as non-root user.
      volume_mount_enable: false

      ## Node labels for pod assignment
      ### ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
      nodeSelector: { }

      ## Toleration is asking the K8S schedule to ignore a taint
      ### ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      #This toleration is provided to have fluentd pods to be scheduled on all nodes including control nodes that may have a taint NoExecute to fulfill daemonset behaviour.
      #This can be set to empty for deployment/statefulset kind.
      #To configure it to empty, set tolerations: []
      tolerations:
        - operator: 'Exists'
          effect: 'NoExecute'

      livenessProbe:
        initialDelaySeconds: 300
        periodSeconds: 20
        timeoutSeconds: 1
        successThreshold: 1
        failureThreshold: 3
      readinessProbe:
        initialDelaySeconds: 30
        periodSeconds: 15
        timeoutSeconds: 1
        successThreshold: 1
        failureThreshold: 3

      ## Pod scheduling preferences.
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      affinity: { }

    cbur:
      enabled: false

    kubectl:
      image:
        repo: tools/kubectl
        tag: 1.28.7-20240301

    cronjob:
      schedule: "*/15 * * * *"
      concurrencyPolicy: "Forbid"
      failedJobsHistoryLimit: "0"
      successfulJobsHistoryLimit: "0"
      suspend: "False"

altiplano-mariadb:
  #enabled: true
  ## RBAC enabled flag, scc is not required for cmdb.
  env:
    secrets:
      IS_USERNAME: fluentd_is_username
      IS_PASSWORD: fluentd_is_password
  rbac:
    enabled: true
    ## psp creation is needed when istio enabled but cni is disabled.
    psp:
      create: false
  ## TZ environment variable in the container (eg, America/Chicago or GST-6)
  timeZoneEnv: *timeZoneEnv

  # cert-manager certificate resource configuration (previously under
  # certMangager) must now be defined at each workload under "certificate"
  # (ie, clients.mariadb.certificate, mariadb.certificate, maxscale.certificate)
  # certManager.enabled at root level dictates enablement or disablement, default is true (with no
  # value entered). To disable it, set it to false.
  # Certificates API Version "apiVersion" defaults to "cert-manager.io/v1"
  certManager:
    enabled: false

  custom:
    pod:
      annotations:
        kubectl.kubernetes.io/default-container: mariadb
        kubectl.kubernetes.io/default-logs-container: mariadb

  ###### site-specific params #####
  ##
  ## Built-in User configuration
  ## Define the built-in user credentials to be used by mariadb/maxscale and
  ## metrics.  Passwords can be provided via one of two ways:
  ##  1.  Explicitly specified here by specifying:
  ##      - username       : user name to be configured
  ##      - password       : a base64 encoded password
  ##  2.  Providing a pre-created secret:
  ##      - credentialName : name of pre-created secret with the BOTH the
  ##                         'username' and 'password' keys present.
  ## NOTE: (a) All passwords provided here MUST be base64 encoded.
  ##       (b) Any passwords not set (and credentialName not provided)
  ##           will be auto-generated.
  ##       (c) If credentialName is provided, username/password will be ignored.
  users:
    root:
      password:
      credentialName: db-user-root-secret
      ## If root user should be allowed from external hosts
      allowExternal: false

    ## MariaDB Replication
    ## The replication user name and password to use in the database.
    ## IMPORTANT: When deploying a geo-redundant solution
    ##            (geo_redundancy.enabled=true), you must use the same
    ##            replication.username/replication.password for both sites.
    ##            Therefore you must either specify username/password below
    ##            or provide a secret providing credentials (credentialName).
    replication:
      credentialName: db-user-replication-secret

    ## MariaDB Metrics
    ## The mariadb metrics user name and password to use in the database.
    mariadbMetrics:
      #username: exporter
      #password:
      credentialName: db-user-root-secret

    ## MaxScale
    ## The maxscale user name and password to use in the database.
    ## IMPORTANT: When deploying a geo-redundant solution
    ##            (geo_redundancy.enabled=true), you must use the same
    ##            maxscale.username/maxscale.password for both sites.
    ##            Therefore you must either specify username/password below
    ##            or provide a secret providing credentials (credentialName).
    maxscale:
      credentialName: db-user-maxscale-secret

    ## MaxScale Metrics
    ## The maxscale metrics user name and password to use for the maxctrl API
    maxscaleMetrics:
      credentialName: db-user-maxscale-metrics-secret

  ## Service account to use instead of generating one.
  ## Only set if a sufficient SA has already been pre-created
  ## Can also be set in global
  serviceAccountName:

  ## istio environment
  istio:
    enabled: false

  ###### site-specific params #####
  ## auth is used to configure SSL or Zero Trust authentication for all metrics
  ## containers. Metrics authentication should only be enabled if either:
  ## - scrape requests are required to be secure (using TLS), or
  ## - scrape requests are required to be zero trust (using bearer token).
  auth:
    enabled:
    tls:
      secretRef:
        name: altiplano-mariadb-certificate-secrets
        keyNames:
          caCrt: "mariadb-server-trustchain-cert.pem"
          tlsKey: "mariadb-server-key.pem"
          tlsCrt: "mariadb-server-cert.pem"

  ## If cluster_name not specified, the release name will be used
  cluster_name: "mariadb"
  ## Cluster Type is one of master-slave, master-master, galera, simplex
  cluster_type: "simplex"

  ## Cluster domain (tail end of the hostname) used in raw k8s installs
  # E.g. myrel-cmdb-mysql.default.svc.cluster.local)
  clusterDomain: "cluster.local"

  ## Speficies the type of anti-affinity for scheduling pods to nodes.
  ## If hard, pods cannot be scheduled together on nodes, if soft,
  ## best-effort to avoid sharing nodes will be done
  nodeAntiAffinity: soft

  ## Indicates if passwords should be displayed by the helm NOTES - which
  ## are displayed when helm install completes.
  ## displayPasswords is one of never, if-generated, always
  displayPasswords: "never"

  ## Values on how to expose services
  services:
    ##
    ## MySQL service exposes the mysql database service
    ##
    mysql:
      ## If not set, will be <release>-mysql
      name: altiplano-mariadb
      ## By default, set to ClusterIP to expose database only within cluster
      ## Set as NodePort to expose database externally
      #type: NodePort
      #nodePort: 30033

      sessionAffinity:
        enabled: false
        #timeout: 60

      ## mariadb-exporter metrics port (used if mariadb.metrics is enabled)
      exporter_port: 9104

      ## MaxScale Read-Write-Split Service
      ## when enabled, port defaults to 3306, TLS based on mariadb.tls.enabled.
      rwSplit:
        enabled: true
        tlsPort: 3308
        #nonTlsPort:

      ## MaxScale Read-Only Service:
      ## when enabled, port defaults to 3307, TLS based on mariadb.tls.enabled.
      readOnly:
        enabled: true
        tlsPort:  3307
        #nodePort:

      ## MaxScale Master-Only Service:
      ## when enabled, port defaults to 3308, TLS based on mariadb.tls.enabled.
      masterOnly:
        enabled: true
        tlsPort: 3306
        nonTlsPort:
        #nodePort:

    ##
    ## Admin exposes the admin container DB (redis.io) interface to all
    ## cluster pods.  (only if not Simplex)
    ##
    admin:
      ## If not set, will be <release>-admin
      #name:
      type: ClusterIP

  ## MariaDB Client Certificate
  clients:
    mariadb:
      tls:
        secretRef:
          name: altiplano-mariadb-certificate-secrets
          keyNames:
            caCrt: mariadb-server-trustchain-cert.pem
            tlsKey: mariadb-client-key.pem
            tlsCrt: mariadb-client-cert.pem
          threshold: 7
          polling: 3600
      certificate:
        enabled: false

  ##
  ## Values specific to the MariaDB (server)
  ##
  mariadb:
    image:
      name: "fnms-mariadb"
      tag: "nokia-2.0.3"
      vaultInitRepo: fnms-init-container
      vaultInitTag: nokia-2.0.1
      flavor: custom
      pullPolicy: IfNotPresent
    ## List of users to be created
    accessRoleLabel: external-access
    users:
      - credentialName: db-user-altiplano-secret
      - credentialName: db-user-wfm-secret
      - credentialName: db-user-mistral-secret
      - credentialName: db-user-amsadminusr-secret
      - credentialName: db-user-fmcgdaluser-secret
      - credentialName: db-user-debug-client-secret
      - credentialName: db-user-ontes-secret
      - credentialName: db-user-cswl-secret
      - credentialName: db-user-ood-secret
      - credentialName: db-user-ood-system-secret
      - credentialName: db-user-ood-cls-secret
      - credentialName: db-user-app-wnp-secret
      - credentialName: db-user-app-pcp-secret
      - credentialName: db-user-app-passive-topology-secret
      - credentialName: db-user-app-sbs-secret

    ## The number of MariaDB pods to create
    count: 1

    ## Pod securityContext override for mariadb pods
    ## Use disabled: true to disable the use of a pod-level security context
    podSecurityContext:
      runAsUser: 1771
      runAsGroup: 1771
      fsGroup: 1771

    ## Container securityContext override for mariadb container
    ## Use disabled: true to disable the use of a container-level security context
    containerSecurityContext:
      runAsUser: 1771
      runAsGroup: 1771
      readOnlyRootFilesystem: true
      allowPrivilegeEscalation: false

    ## Pod Discruption Budget (PDB) for mariadb pods
    ## Defines limits to the the number of concurrent disruptions that the
    ## mariadb database pods can experience.
    ## The following are the options that can be set:
    ##   enabled        - Enable/disable PDB for mariadb statefulset
    ##   minAvailable   - The number of pods from that set that must still be
    ##                    available after the eviction, even in the absence of
    ##                    the evicted pod. minAvailable can be either an absolute
    ##                    number or a percentage.
    ##   maxUnavailable - The number of pods from that set that can be
    ##                    unavailable after the eviction. It can be either an
    ##                    absolute number or a percentage
    ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
    ##      https://kubernetes.io/docs/tasks/run-application/configure-pdb/
    pdb:
      enabled: false

    ## Additional command line arguments to mariadb mysqld daemon
    ## ref: https://mariadb.com/kb/en/mysqld-options/
    ## eg, to not start the database with safe-user-create, comment the following:
    mysqldOpts: --safe-user-create=ON

    ## Termination grace period (in seconds).  Defaults to 120 seconds.
    ## Set higher if a Master switchover can take longer than 120 seconds.
    #terminationGracePeriodSeconds: 30

    ## Perform automatic tc-heuristic-recover of the database on pod restarts.
    ## Set to one of the following:
    ##   rollback - perform auto rollback of uncommitted transactions
    ##   commit   - perform auto commit of uncommitted transactions
    ##   none     - disable automatic tc-hauristic-recover
    ## Defaults to rollback for non-simplex deployments, none for simplex.
    #heuristic_recover: rollback

    ## Interval (in seconds) when clean-logs should be performed on Master to
    ## cleanup binlogs that have been replicated to all slaves
    clean_log_interval: 3600

    ## Enable/disable server audit logging.
    ## If enabled, set events which will be logged.
    ## ref: https://mariadb.com/kb/en/library/mariadb-audit-plugin-log-settings/
    audit_logging:
      enabled: false
      events: "CONNECT,QUERY_DCL,QUERY_DDL"

    tls:
      enabled: *useTls
      secretRef:
        name: altiplano-mariadb-certificate-secrets
        keyNames:
          caCrt: mariadb-server-trustchain-cert.pem
          tlsKey: mariadb-server-key.pem
          tlsCrt: mariadb-server-cert.pem
        threshold:  7
        polling: 3600

    ## Resource QOS (per MariaDB container)
    resources:
      requests:
        memory: 1.5Gi
        cpu: 100m
      limits:
        memory: 2.5Gi
        cpu: 1

    ## Node tolerations for mariadb scheduling to nodes with taints
    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    ## Node labels for mariadb pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    nodeSelector: {}
    ## BCMT nodeAffinity rules (mutually-exclusive with nodeSelector above):
    ## Default to not enabled.
    nodeAffinity:
      enabled: false
      key: is_worker
      value: true

    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      enabled: *persistence
      accessMode: ReadWriteOnce
      size: 5Gi
      storageClass: *storageClass
      resourcePolicy: delete
      preserve_pvc: true
      backup:
        enabled: *persistence
        storageClass: *storageClass
        accessMode: ReadWriteOnce
        size: 5Gi
        #dir: /var/lib/backup #This path is used in manual backup
        dir: /mariadb/backup #New backup path for MariaDB used from CBUR

    ## A customized mysqld.conf to import
    mysqld_site_conf: |-
      [mysqld]
      version = 24.6.1-REL_256-altiplano-infra-MariaDB
      skip-host-cache
      skip-name-resolve
      datadir		= /mariadb/data
      lc_messages_dir	= /usr/share/mysql
      lc_messages	= en_US
      skip-external-locking
      #
      # * Fine Tuning
      #
      max_connections		= 200
      connect_timeout		= 5
      wait_timeout		= 1800
      max_allowed_packet	= 100M
      thread_cache_size       = 128
      sort_buffer_size	= 4M
      bulk_insert_buffer_size	= 16M
      tmp_table_size		= 32M
      max_heap_table_size	= 32M
      #
      # * MyISAM
      #
      # This replaces the startup script and checks MyISAM tables if needed
      # the first time they are touched. On error, make copy and try a repair.
      myisam_recover_options = BACKUP
      key_buffer_size		= 128M
      #open-files-limit	= 2000
      table_open_cache	= 400
      myisam_sort_buffer_size	= 512M
      concurrent_insert	= 2
      read_buffer_size	= 2M
      read_rnd_buffer_size	= 1M
      #
      # * Query Cache Configuration
      #
      # Cache only tiny result sets, so we can fit more in the query cache.
      query_cache_limit		= 128K
      query_cache_size		= 64M
      # for more write intensive setups, set to DEMAND or OFF
      #query_cache_type		= DEMAND
      #
      # * Logging and Replication
      #
      # Both location gets rotated by the cronjob.
      # Be aware that this log type is a performance killer.
      # As of 5.1 you can enable the log at runtime!
      #general_log_file        = /var/log/mysql/mysql.log
      #general_log             = 1
      #
      # Error logging goes to syslog due to /etc/mysql/conf.d/mysqld_safe_syslog.cnf.
      #
      # we do want to know about network errors and such
      #log_warnings		= 2
      #
      # Enable the slow query log to see queries with especially long duration
      #slow_query_log[={0|1}]
      slow_query_log_file	= /var/log/mysql/mariadb-slow.log
      long_query_time = 10
      #log_slow_rate_limit	= 1000
      #log_slow_verbosity	= query_plan

      #log-queries-not-using-indexes
      #log_slow_admin_statements
      #
      # The following can be used as easy to replay backup logs or for replication.
      # note: if you are setting up a replication slave, see README.Debian about
      #       other settings you may need to change.
      #server-id		= 1
      #report_host		= master1
      #auto_increment_increment = 2
      #auto_increment_offset	= 1
      #log_bin			= /var/log/mysql/mariadb-bin
      #log_bin_index		= /var/log/mysql/mariadb-bin.index
      # not fab for performance, but safer
      #sync_binlog		= 1
      expire_logs_days	= 1
      max_binlog_size         = 100M
      # slaves
      #relay_log		= /var/log/mysql/relay-bin
      #relay_log_index	= /var/log/mysql/relay-bin.index
      #relay_log_info_file	= /var/log/mysql/relay-bin.info
      #log_slave_updates
      #read_only
      #
      # If applications support it, this stricter sql_mode prevents some
      # mistakes like inserting invalid dates etc.
      #sql_mode		= NO_ENGINE_SUBSTITUTION,TRADITIONAL
      #
      # * InnoDB
      #
      # InnoDB is enabled by default with a 10MB datafile in /var/lib/mysql/.
      # Read the manual for more InnoDB related options. There are many!
      default_storage_engine	= InnoDB
      # you can't just change log file size, requires special procedure
      #innodb_log_file_size	= 50M
      innodb_buffer_pool_size	= 256M
      innodb_log_buffer_size	= 8M
      innodb_file_per_table	= 1
      innodb_open_files	= 400
      innodb_io_capacity	= 400
      innodb_flush_method	= O_DIRECT
      innodb_read_only_compressed = OFF
      tls_version = TLSv1.2,TLSv1.3

    # data-at-rest params
    # - secret:     this is a kube secret that contains objects that are identified by the keyFile and keyFileKey parameters.
    # - keyFile:    (required) this is the name of the key-file in <secret> used to encrypt/decrypt the database. By default,
    #               this file is plain-text. The installer may encrypt this file - in this case the keyFileKey param must be specified.
    # - keyFileKey: (optional) this is the name of the file in <secret> to decrypt "keyFile". This is required only if the
    #               "keyFile" is encrypted. Use the following command to encrypt "keyFile" (eg: filename.plain -> filename.enc):
    #                   # openssl enc -aes-256-cbc -md sha1 -pass file:<keyFileKey> \
    #                       -in ./filename.plain \
    #                       -out ./filename.enc
    # - keyFileId:  (optional) this specifies a particular key in "keyFile". If not present, then assume keyid "1".
    encryption:
      enabled: false
      secret:
      keyFile:
      keyFileKey:
      keyFileId:

    ##values for fluentd side car
    fluentd_sidecar:
      image:
        name: "fnms-fluent"
        tag: "nokia-4.1.8"
        pullPolicy: IfNotPresent
      securityContext:
        readOnlyRootFilesystem: true
        runAsUser: 1000
        runAsGroup: 1771
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 256Mi
      secrets:
        certSecret: altiplano-mariadb-certificate-secrets
        passwordSecret: altiplano-secrets
      KAFKA_BOOTSTRAP_SERVERS: altiplano-kafka-headless:9092
      KAFKA_TOPIC: ALTIPLANO_INTERNAL_maxscale-alarm
      LOG_INDEX_PATTERN: logstash
      IS_HOST: altiplano-indexsearch
      IS_PORT: "9200"
      IS_SCHEME: https
      fluent_conf: |-
        <system>
          log_level error
        </system>

        <source>
          @type tail
          path /logs/mariadb/activity.log,/logs/mariadb/mariadb-monitor.log
          read_from_head true
          pos_file /tmp/fluentd/activity-log.pos
          keep_time_key true
          tag activity.log
          format /^{([^{]*){(.*?:)"(?<message>[^}]*)"},(.*?:)"(?<container_name>[^.]*).\w*",(.*?:)"(?<process>[^,]*)",(.*?:)"(?<service>[^,]*)",(.*?:)"(?<level>[^,]*)",(.*?:)"(?<date>[^}]*)"}$/
        </source>
        <source>
          @type tail
          path /logs/mariadb/err.log
          read_from_head true
          pos_file /tmp/fluentd/err-log.pos
          keep_time_key true
          tag err.log
          format /^(?<date>[^\[]*)\s\[(?<level>[^\]]*)\]\s(?<message>.*)/
        </source>
        <source>
          @type tail
          path /logs/mariadb/mariadb-monitor.log
          read_from_head true
          pos_file /tmp/fluentd//mariadb-monitor.log.pos
          keep_time_key true
          tag nokia.logging.json
          @label @alarm.log
          format json
        </source>
        <label @alarm.log>
          <match nokia.logging.json>
            @type rewrite_tag_filter
            <rule>
              key "type"
              pattern ^(.+)$
              tag "nokia.logging.$1"
            </rule>
          </match>
          <match nokia.logging.alarm>
            @type copy
            <store>
              @type stdout
            </store>
            <store>
              @type kafka2
              <format>
                @type json
              </format>
              <buffer topic>
                @type file
                path /tmp/fluentd/kafka-buffer/nokia.logging.all.alarm
                overflow_action drop_oldest_chunk
                chunk_limit_size 16MB
                queued_chunks_limit_size  4096
                flush_thread_count 1
                flush_interval 5s
                retry_wait 0s
                retry_forever true
                total_limit_size 50MB
              </buffer>
              ssl_ca_cert "/etc/.certificates/mariadb-server-trustchain-cert.pem"
              ssl_client_cert "/etc/.certificates/mariadb-client-cert.pem"
              ssl_client_cert_key "/etc/.certificates/mariadb-client-key.pem"
              ssl_client_cert_key_password "/etc/.certificates/client_key_pass"
              ssl_verify_hostname false
              get_kafka_client_log true
              max_send_retries 10
              brokers "{{ .Values.mariadb.fluentd_sidecar.KAFKA_BOOTSTRAP_SERVERS }}"
              default_topic "{{ .Values.mariadb.fluentd_sidecar.KAFKA_TOPIC }}"
            </store>
          </match>
          <match nokia.logging.*>
            @type null
          </match>
        </label>
        <filter err.log>
          @type record_transformer
          enable_ruby true
          <record>
            container_name "#{ENV['CONTAINER_NAME']}"
            container_ip "#{ENV['POD_IP']}"
            date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S %L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
            level ${record["level"]}
            message ${record["message"]}
            level ${record['level'] == "Note" ? "INFO" : record['level'] == "ERR" || record['level'] == "error" ? "ERROR" : record['level'] == "Warning" || record['level'] == "ALERT" ? "WARN" : record['level'] }
          </record>
          renew_record true
        </filter>
        <filter activity.log>
          @type record_transformer
          enable_ruby true
          <record>
            container_ip "#{ENV['POD_IP']}"
            level ${record['level'] == "Note" ? "INFO" : record['level'] == "ERR" || record['level'] == "error" ? "ERROR" : record['level'] == "Warning" || record['level'] == "ALERT" ? "WARN" : record['level'] }
          </record>
          renew_record true
          keep_keys container_name,container_ip,date,level,message,process,service
        </filter>
        <match *.log>
          @type opensearch
          reload_on_failure true
          reconnect_on_error true
          suppress_type_name true
          logstash_format true
          type_name fluentd
          ssl_verify false
          ssl_version TLSv1_2
          scheme {{ .Values.mariadb.fluentd_sidecar.IS_SCHEME  }}
          host {{ .Values.mariadb.fluentd_sidecar.IS_HOST  }}
          port {{ .Values.mariadb.fluentd_sidecar.IS_PORT  }}
          ca_file "/etc/.certificates/mariadb-server-trustchain-cert.pem"
          user "#{ENV['IS_USERNAME']}"
          password "#{ENV['IS_PASSWORD']}"
          <buffer>
            @type file
            path /tmp/fluentd/buffer_mariadb/
            overflow_action drop_oldest_chunk
            chunk_limit_size 16MB
            queued_chunks_limit_size  4096
            flush_thread_count 5
            flush_interval 5s
            retry_wait 0s
            retry_forever true
            total_limit_size 50MB
          </buffer>
          time_key date
          time_key_exclude_timestamp true
          logstash_prefix {{ .Values.mariadb.fluentd_sidecar.LOG_INDEX_PATTERN }}
          request_timeout 45s
        </match>
        <match>
          @type null
        </match>

    ## metrics
    metrics:
      enabled: true
      image:
        name: "cmdb/cmdb-mysqld-exporter"
        tag: "0.15.1-2.292-rocky8"
        flavor: custom
        pullPolicy: IfNotPresent
        accessRoleLabel: internal-access

      ## Note that prometheus.io/port is specified in mariadb-metrics-service
      annotations:
        prometheus.io/scrape: "true"

      ## Resource QOS (per MariaDB-Metrics container)
      resources:
        requests:
          memory: 256Mi
          cpu: 250m
        limits:
          memory: 256Mi
          cpu: 250m

    ## Zero Trust Proxy
    auth:
      enabled: false
      image:
        name: "osdb/csfdb-zt-proxy"
        tag: "1.1-4.40-rocky8"
        flavor: custom
        pullPolicy: IfNotPresent

    ## Grafana dashboard
    dashboard:
      enabled: false

    ## Backup/restore options
    backupRestore:
      fullBackupInterval: 0
      # Preserve replicated tables during restore. Used for restore between datacenters.
      preserve: false
      ## This option affects how much memory is allocated for preparing a backup
      ## with mariabackup --prepare during restore.
      ## Its purpose is similar to innodb_buffer_pool_size.  The default value is 512MB,
      ## and if you have enough available memory, 1GB
      ## to 2GB is a good recommended value.Multiples are supported providing the
      ## unit (e.g. 1MB, 1M, 1GB, 1G).
      useMemory: 2GB

  ##
  ## Values specific to the kubectl
  ##
  kubectlImage: "fnms-kubectl"
  kubectlTag: nokia-1.2.0

  ##
  ## Values specific to the MaxScale (proxy)
  ##
  maxscale:
    enabled: false
    admin_ssl_version: TLSv12
    ssl_version: TLSv12
    env:
      secrets:
        IS_USERNAME: fluentd_is_username
        IS_PASSWORD: fluentd_is_password
    image:
      name: "fnms-maxscale"
      tag: "nokia-2.0.2"
      flavor: custom
      pullPolicy: IfNotPresent
    accessRoleLabel: external-access
    metrics:
      enabled: false
      image:
        name: "cmdb/cmdb-maxctrl-exporter"
        tag: "0.1.0-24.288-rocky8"
        flavor: custom
        pullPolicy: IfNotPresent
    fluentd_sidecar:
      image:
        name: "fnms-fluent"
        tag: "nokia-4.1.8"
        pullPolicy: IfNotPresent
      securityContext:
        runAsUser: 1000
        runAsGroup: 1772
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 256Mi
      secrets:
        certSecret: altiplano-mariadb-certificate-secrets
        passwordSecret: altiplano-secrets
      KAFKA_BOOTSTRAP_SERVERS: altiplano-kafka-headless:9092
      KAFKA_TOPIC: ALTIPLANO_INTERNAL_maxscale-alarm
      IS_PROTO: https
      IS_IP: altiplano-indexsearch
      IS_PORT: 9200
      LOG_INDEX_PATTERN: logstash
      fluent_conf: |
        <system>
          log_level warn
        </system>
    ## The number of MaxScale pods
    count: 1
    resources:
      requests:
        memory: 1Gi
        cpu: 1
        ephemeral-storage: 1Gi
      limits:
        memory: 7Gi
        cpu: 2
        ephemeral-storage: 1Gi
    ## use TLS/SSL for maxscale admin interface
    tls:
      enabled: *useTls
      secretRef:
        name: altiplano-mariadb-certificate-secrets
        keyNames:
          caCrt: mariadb-server-trustchain-cert.pem
          tlsKey: mariadb-server-key.pem
          tlsCrt: mariadb-server-cert.pem
        threshold: 7
        polling: 3600
        use_common_name: true
    certificate:
      enabled: false

    ## Zero Trust Proxy
    auth:
      enabled: false
      image:
        name: "osdb/csfdb-zt-proxy"
        tag: "1.1-4.40-rocky8"
        flavor: custom
        pullPolicy: IfNotPresent

    maxscale_site_conf: |-
      [maxscale]
      threads = 5
      query_retries = 2
      query_retry_timeout = 10s
      query_classifier_cache_size=1500Mi
      [RWSplit-Service]
      connection_keepalive = 1750s
      [RO-Service]
      connection_keepalive = 1750s
      [Master-Service]
      connection_keepalive = 1750s

    nodeAffinity:
      enabled: false

    nodeSelector: {}

  geo_redundancy:
    ## Only enable geo_redundancy for multiple datacenters
    enabled: false
  
  ##
  ## Values specific to the CMDB Administrative container
  ## (used for lifecycle and administrative Jobs)
  ##
  admin:
    image:
      name: "fnms-mariadb-admin"
      tag: "nokia-2.0.4"
      flavor: custom
      pullPolicy: IfNotPresent
    count: 1
    accessRoleLabel: internal-access

    fluentd_sidecar:
      image:
        name: "fnms-fluent"
        tag: "nokia-4.1.8"
        pullPolicy: IfNotPresent
      securityContext:
        runAsUser: 1000
        runAsGroup: 1773
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 256Mi
      secrets:
        certSecret: altiplano-mariadb-certificate-secrets
        passwordSecret: altiplano-secrets
      KAFKA_BOOTSTRAP_SERVERS: altiplano-kafka-headless:9092
      KAFKA_TOPIC: ALTIPLANO_INTERNAL_maxscale-alarm
      LOG_INDEX_PATTERN: logstash
      IS_HOST: altiplano-indexsearch
      IS_PORT: "9200"
      IS_SCHEME: https
      fluent_conf: |-
        <system>
          log_level error
        </system>

        <source>
          @type tail
          path /logs/admin/replica-monitor.log
          read_from_head true
          pos_file /tmp/fluentd/replica-monitor-log.pos
          keep_time_key true
          tag nokia.logging.json
          @label @alarm.log
          format json
        </source>
        <source>
          @type tail
          path /logs/admin/orchestrator.log,/logs/admin/creds-monitor.log,/logs/admin/alarm-monitor.log
          read_from_head true
          pos_file /tmp/fluentd/admin-orchestrator-log.pos
          keep_time_key true
          tag admin-orchestrator.log
          format /^(.*?:){(.*?:)"(?<message>[^}]*)"}(.*?:)"(?<container_name>[^.]*).\w*",(.*?:)"(?<process>[^,]*)",(.*?:)"(?<service>[^,]*)",(.*?:)"(?<level>[^,]*)",(.*?:)"(?<date>[^}]*)"}/
        </source>
        <source>
          @type tail
          path /logs/admin/alarm-monitor.log
          read_from_head true
          pos_file /tmp/fluentd/admin-alarm-monitor-log.pos
          keep_time_key true
          tag admin-alarm-monitor.log
          format /^(.*?:{)(.*?:)"(?<alarm_data>[^,]*)",(.*?:)"(?<alarm_event_type>[^,]*)"(.*?:)(?<alarm_id>[^,]*)(.*?:)(.*?:)"(?<alarm_name>[^,]*)"(.*?:)"(?<probable_cause>[^,]*)"(.*?:)"(?<severity>[^,]*)"(.*?:)"(?<alarm_task>[^"]*)"(.*?:)"(?<alarm_text>[^}]*)"}(.*?:)"(?<container_name>[^.]*).\w*"(.*?:)"(?<level>[^,]*)"(.*?:)"(?<process>[^,]*)"(.*?:)"(?<service>[^,]*)"(.*?:)"(?<date>[^,]*)"(.*:?)"(?<type>[^}]*)"}/
        </source>
        <filter admin-orchestrator.log>
          @type record_transformer
          enable_ruby true
          <record>
            container_ip "#{ENV['POD_IP']}"  
          </record>
          renew_record true
          keep_keys container_name,container_ip,date,level,message,process,service
        </filter>
        <filter admin-alarm-monitor.log>
          @type record_transformer
          enable_ruby true
          <record>
            container_ip "#{ENV['POD_IP']}"  
          </record>
          renew_record true
          keep_keys container_name,container_ip,date,level,type,alarm_data,alarm_event_type,alarm_id,alarm_name,probable_cause,severity,alarm_task,alarm_text,process,service
        </filter>

        <label @alarm.log>
          <match nokia.logging.json>
            @type rewrite_tag_filter
            <rule>
              key "type"
              pattern ^(.+)$
              tag "nokia.logging.$1"
            </rule>
          </match>
          <match nokia.logging.alarm>
            @type copy
            <store>
              @type stdout
            </store>
            <store>
              @type kafka2
              <format>
                @type json
              </format>
              <buffer topic>
                @type file
                path /tmp/fluentd/kafka-buffer/nokia.logging.all.alarm
                overflow_action drop_oldest_chunk
                chunk_limit_size 16MB
                chunk_limit_records 1
                queued_chunks_limit_size  4096
                flush_thread_count 1
                flush_mode immediate
                retry_wait 0s
                retry_forever true
                total_limit_size 50MB
              </buffer>
              ssl_ca_cert "/etc/.certificates/mariadb-server-trustchain-cert.pem"
              ssl_client_cert "/etc/.certificates/mariadb-client-cert.pem"
              ssl_client_cert_key "/etc/.certificates/mariadb-client-key.pem"
              ssl_client_cert_key_password "/etc/.certificates/client_key_pass"
              ssl_verify_hostname false
              get_kafka_client_log true
              max_send_retries 10
              brokers "{{ .Values.admin.fluentd_sidecar.KAFKA_BOOTSTRAP_SERVERS }}"
              default_topic "{{ .Values.admin.fluentd_sidecar.KAFKA_TOPIC }}"
            </store>
          </match>
          <match nokia.logging.*>
            @type null
          </match>
        </label>
        <match *.log>
          @type opensearch
          reload_on_failure true
          reconnect_on_error true
          suppress_type_name true
          logstash_format true
          type_name fluentd
          ssl_verify false
          ssl_version TLSv1_2
          scheme {{ .Values.admin.fluentd_sidecar.IS_SCHEME }}
          host {{ .Values.admin.fluentd_sidecar.IS_HOST }}
          port {{ .Values.admin.fluentd_sidecar.IS_PORT }}
          ca_file "/etc/.certificates/mariadb-server-trustchain-cert.pem"
          user "#{ENV['IS_USERNAME']}"
          password "#{ENV['IS_PASSWORD']}"
          <buffer>
            @type file
            path /tmp/fluentd/buffer_mariadb/
            overflow_action drop_oldest_chunk
            chunk_limit_size 16MB
            queued_chunks_limit_size  4096
            flush_thread_count 5
            flush_interval 5s
            retry_wait 0s
            retry_forever true
            total_limit_size 50MB
          </buffer>
          time_key date
          time_key_exclude_timestamp true
          logstash_prefix {{ .Values.admin.fluentd_sidecar.LOG_INDEX_PATTERN }}
          request_timeout 45s
        </match>
        <match>
          @type null
        </match>
    ## Resource QOS (per Admin container)
    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 512Mi
        cpu: 500m

    ## Node tolerations for admin scheduling to nodes with taints
    ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    ## Node labels for admin pod assignment
    ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    nodeSelector: {}
    ## BCMT nodeAffinity rules (mutually-exclusive with nodeSelector above):
    ## Default to not enabled
    nodeAffinity:
      enabled: false
      key: is_worker
      value: true

    ## A recovery flag.  If changed, will trigger a heal of the database to occur
    #recovery: none

    ## Set to true to change update config behavior to cause statefulset update
    ## (resulting in rolling pod restarts) when mariadb and maxscale configuration
    ## is changed.  Default behavior is to not affect statefulset and services
    ## will be restarted within the pod.
    configAnnotation: false

    ## Should auto-heal be enabled?
    ## This will enable automated heal operations on cluster (Galera supported).
    ## the pauseDelay is how long (seconds) to wait to re-enable auto-heal after
    ## the cluster comes [back] up.
    autoHeal:
      enabled: true
      pauseDelay: 900

    ## Should slave-rebuild be enabled?
    ## This will enable automated slave rebuild operation in master-slave
    ## topology for a failed Master that does not come back as a Slave within
    ## the rebuildDelay time.  The configurable parameters are as follows:
    ##   preferredDonor   - preferred donor to use for rebuild:
    ##                      slave =  use most up-to-date Slave for donor server
    ##                               (will use Master if no suitable Slave found)
    ##                      master = use Master for donor server
    ##   allowMasterDonor - allow Master to be used as Donor?
    ##                      (only valied for preferredDonor = slave)
    ##   timeout          - time to allow rebuild to complete before abort
    ##   parallel         - number of donor threads to use for rebiuld
    ##   useMemory        - amount of memory (eg, 512M) to use for joining server.
    ##                      NOTE: do not set to more than half of
    ##                            resources.limits.memory
    rebuildSlave:
      enabled: true
      onErrorsIO: 1236
      preferredDonor: slave
      allowMasterDonor: true
      timeout: 300
      parallel: 2
      useMemory: 256M

    restoreServer:
      enabled: false

    ## Auto password change job timeout.
    ## When a user credential secret password is updated, an automatic pwchange
    ## job will be initiated.  This defines the timeout (in seconds) to allow
    ## the pwchange job to complete.
    pwchangeTimeout: 1800

    ## Set to 'yes' for quick install deployment.  This will tell the post-install
    ## hook to not wait for SDC to come up and not wait for pods to start.
    ## It is highly recommended that this value be left blank however if you are
    ## deploying a large number of charts you can set this to 'yes' to speed up
    ## deployment.  Setting to 'yes' will not discover pods that fail to start
    ## and may not set initial SDC values.
    quickInstall: ""

    ## If set, administrative jobs will be more verbose to stdout (kubectl logs)
    debug: false
    ## If debug is true, you can add a termination delay to all of the pre/port
    ## job hooks as follows (time in seconds).  To terminate early, create a
    ## /tmp/exit file in the container [with exit value if want to override].
    ## Delay will not apply to pre-delete job since that is where old jobs are
    ## deleted and it will delete itself in the process.
    #jobDelay: 0

    ## Specify timeout (in seconds) for each job execution.  If job timeout
    ## occurs. the job will fail, however the job process will run to completion.
    ## Set to 0 to disable job timing operations.
    preInstallTimeout: 120
    postInstallTimeout: 900
    preUpgradeTimeout: 180
    postUpgradeTimeout: 1800
    preRollbackTimeout: 180
    postRollbackTimeout: 300
    preDeleteTimeout: 120
    postDeleteTimeout: 180
    preBackupTimeout: 180
    postBackupTimeout: 120
    preRestoreTimeout: 120
    postRestoreTimeout: 600

    ## The activeDeadlineSeconds applies to the duration of the job, no matter
    ## how many Pods are created.  Once a Job reaches activeDeadlineSeconds, the
    ## Job and all of its Pods are terminated.  The result is that the job has a
    ## status with reason: DeadlineExceeded.  It's used in pre-upgrade hook to
    ## prevent too many failed pods being started in failure cases.
    activeDeadlineSeconds: 120

    ## Name of a Secret (in our same namespace) to be used for performing
    ## the password change of database user(s).
    ## The secret can contain any number of data items, confirming to the
    ## convention of user_<username>, and the contents of the
    ## If changed, and the named secret exists, the password change will be
    ## attempted.  After a successful password change, the secret will be
    ## automatically deleted. If the password change fails, the secret will
    ## be marked as invalid, thus causing any future upgrades to ignore
    ## this secret.  Future attempts (after failure) must recreate the
    ## secret for it to be processed.
    #pwChangeSecret: ""

  hooks:
    ## Exposes the hook-delete-policy.  By default, this is set to delete the
    ## hooks only upon success or prior to creation.  This can also be unset to
    ## avoid hook deletion for troubleshooting and debugging purposes
    deletePolicy: "hook-succeeded,before-hook-creation"
    postInstallJob:
      activeDeadlineSeconds: 900
    postUpgradeJob:
      activeDeadlineSeconds: 1800

    ## Here you can disable the pre/post-upgrade or pre-post-install jobs.
    ## This is useful for umbrella charts where you don't want the CMDB upgrade
    ## jobs to run when the parent chart upgrade is performed.
    #preInstallJob: "disable"
    #postInstallJob: "disable"
    #preUpgradeJob: "disable"
    #postUpgradeJob: "disable"
    #preRollbackJob: "disable"
    #postRollbackJob: "disable"
    #preDeleteJob: "disable"
    #postDeleteJob: "disable"

  ## ---------------------------
  ## CBUR Parameters
  ## ---------------------------

  cbur:
    enabled: *cburEnable
    apiVersion: *cburApiVersion
    image:
      name: "cbur/cbur-agent"
      tag: 1.2.0-alpine-580
      pullPolicy: "IfNotPresent"

    persistence:
      cburtmp:
        enabled: *cburEnable
        #NOTE cburtmp will contain both tarball and uncompressed files, so the storage space should be bigger than the sum of data volumes to backup
        size: 10Gi
        storageClass: *storageClass
        accessMode: ReadWriteOnce
        dir: /tmp

    ## HBP 3.6: emptyDir volume size need to be configurable.
    ## By default, it's twice of data storage size, only needed for volume backup.
    ##
    emptyDir:
      sizeLimit: 40Gi

    ## Supported values: volume( default ) or snapshot
    ## For volume type, it's the normal backup and restore procedure based on mariabackup.
    ## For snapshot type, the datadir pvc will be volume snapshotted during backup and
    ##   a new datadir pvc will be recreated based on the volumesnapshot during restore.
    volumeBackupType: volume

    ## Specify the name of a VolumeSnapshotClass
    volumeSnapshotClassName: cinder-csi-snapclass

    ## "true":  CBUR-m saves the contents of the volume snapshot to backend.
    saveVolumeContentToBackend: true

    ## "false": CBUR-m takes charge of deleting and re-attaching pvc, pod(s) also will be deleted
    ##          and re-created during deleting/re-attaching pvc.
    createVolumeOnly: false

    ## "false": CBUR-m will delete the volume snapshot in cloud provided storage after saving volume content to backend.
    keepLocalSnapshotIfSaveToBackend: false

    ## "true":  CBUR-m will always retrieve volume content from backend regardless snapshot exists in cloud provided storage.
    retrieveVolumeContentFromBackend: true

    ## The wait duration in seconds for filesystem flushed to the disk to be ready for snapshot
    snapshotWaitDisk: 60

    ## defines the backup storage options, i.e. local, NetBackup, S3, Avamar.
    backendMode: *globalBackend
    ## specifies if the backup scheduling cron job should be auto-scheduled
    autoEnableCron: true
    ## specifies cron update will be triggered automatically by BrPolicy update
    autoUpdateCron: false
    ## allows user to schedule backups
    cronSpec: "0 1 * * *"
    ## defines how many backup copies should be saved
    maxiCopy: *backupRetain
    ## should backups be encrypted?
    dataEncryption: false
    ## Interface to BrPolicy to ignore file changes during building tarball for
    ## target app's volumes.
    ignoreFileChanged: false
    ## "true": will select latest slave pod then master pod if no slave pod is found.
    ## "false": will select master pod
    selectPod: false

    ## Define the tables to be preserved without replication coordination.
    ## The tables are seperated by ",", i.e.,"mydb.mydbtable2,mydb2.mydbtable3"
    tablesPreserve:
    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 2Gi
        cpu: 1

    ## Exposes the BrHook parameters which may be used to control the execution
    ## and sequencing of the hooks per CBUR logic.
    brhookType: brpolicy
    brhookWeight: 0
    brhookEnable: true
    brhookTimeout: 600

    ## Ex: prebackup_mariabackup_args: "--tables-exclude=rick.local,art.local"
    ## Beware: "helm backup" will NOT automatically pick up changes to the
    ## "*_mariabackup_args" parameters when changed after installation.
    ## In this case, use "kubectl edit BrPolicy <cmdb-fullname>-mariadb" to
    ## manually reflect any changes.
    prebackup_mariabackup_args:
    postrestore_mariabackup_args:

    prebackup: 0
    postbackup: 0
    prerestore: 0
    postrestore: 0

    # BrHook Jobs
    preBackupHook:
      name: "prebackup-brhook"
      containerName: "pre-backup-admin"
      timeout: 180
    postBackupHook:
      name: "postbackup-brhook"
      containerName: "post-backup-admin"
      timeout: 120
    preRestoreHook:
      name: "prerestore-brhook"
      containerName: "pre-restore-admin"
      timeout: 240
    postRestoreHook:
      name: "postrestore-brhook"
      containerName: "post-restore-admin"
      timeout: 900

altiplano-kafka:
  #enabled: true
  imageRepo: "ckaf/ckaf-kafka"
  imageTag: 9.0.2-rocky8-jre17-7.4.1-70
  accessRoleLabel: internal-access
  persistence:
    enabled: *persistence
  Replicas: 1
  kubectlImageRepo: "tools/kubectl"
  kubectlTag: 1.28.7-rocky8-nano-20240301
  defaultSecurity:
    enabled: true
    runAsUser: 1000
    fsGroup: 1000
    runAsGroup: 1000
  security:
    enabled: true
    runAsUserInitZookeeperConnectionCheck: 65534
    runAsGroupInitZookeeperConnectionCheck: 65534
    runAsUser: 999
    fsGroup: 998
    runAsGroup: 997
    readOnlyRootFilesystem: true

  LogRetentionHours: "24"
  LogLevel: "ERROR"

  fullnameOverride: "altiplano-kafka"
  env:
    secrets:
      IS_USERNAME: fluentd_is_username
      IS_PASSWORD: fluentd_is_password
  fluentd_sidecar:
    image:
      name: "fnms-fluent"
      tag: "nokia-4.1.8"
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 512Mi
      requests:
        cpu: 10m
        memory: 256Mi
    secrets:
      certSecret: altiplano-kafka-certificate-secrets
      passwordSecret: altiplano-secrets
    securityContext:
      readOnlyRootFilesystem: true
      runAsUser: 1000
      runAsGroup: 997
    IS_SCHEME: https
    IS_HOST: altiplano-indexsearch
    IS_PORT: 9200
    LOG_INDEX_PATTERN: logstash
    fluent_conf: |-
    
      <system>
        log_level error
      </system>
      
      <source>
        @type tail
        path /logs/server.log
        read_from_head true
        pos_file /tmp/fluentd/server-log.pos
        keep_time_key true
        tag kafka.log
        <parse>
          @type multiline
          format_firstline /"time":"\d{4}-\d{1,2}-\d{1,2}T\d{1,2}:\d{1,2}:\d{1,2}.\d{1,3}Z"/
          format1 /^(.*?,){2}[^:]*:"(?<level>[^,]*)",(.*?,){2}[^:]*:"(?<date>[^,]*)",[^:]*:"(?<timezone>[^,]*)",[^:]*:[^:]*:"(?<thread>.*)\s-\s(?<category>[^ ]*)\s-\s(?<message>.*)"}}(?<throwable>.*)/
        </parse>
      </source>
      <filter kafka.log>
        @type record_transformer
        enable_ruby true
        <record>
          container_name "#{ENV['CONTAINER_NAME']}"
          container_ip "#{ENV['MY_POD_IP']}"
        </record>
        renew_record true
        keep_keys container_name,container_ip,date,level,thread,category,message,throwable
        remove_keys timezone
      </filter>
      <match kafka.log>
        @type opensearch
        reload_on_failure true
        reconnect_on_error true
        logstash_format true
        type_name fluentd
        ssl_verify false
        ssl_version TLSv1_2
        scheme {{ .Values.fluentd_sidecar.IS_SCHEME }}
        host {{ .Values.fluentd_sidecar.IS_HOST }}
        port {{ .Values.fluentd_sidecar.IS_PORT }}
        ca_file "/etc/.certificates/kafka-server-trustchain-cert.pem"
        user "#{ENV['IS_USERNAME']}"
        password "#{ENV['IS_PASSWORD']}"
        <buffer>
          @type file
          path /tmp/fluentd/buffer_kafka
          overflow_action drop_oldest_chunk
          chunk_limit_size 16MB
          queued_chunks_limit_size  4096
          flush_thread_count 5
          flush_interval 5s
          retry_wait 0s
          retry_forever true
          total_limit_size 50MB
        </buffer>
        time_key date
        time_key_exclude_timestamp true
        suppress_type_name true
        logstash_prefix {{ .Values.fluentd_sidecar.LOG_INDEX_PATTERN }}
        request_timeout 45s
      </match>
      <match **>
        @type null
      </match>
  resources:
    requests:
      cpu: 200m
      memory: 1Gi
      ephemeral-storage: 1G
    limits:
      cpu: 1000m
      memory: 1500Mi
      ephemeral-storage: 1G
  kafkaNodeSelector:
    enable: false
  #   nodeLabel:
  #     "role" : "Infra"
  antiAffinity: "soft"
  DataStorage: "12Gi"
  LogStorage: "10Gi"
  OffsetsTopicReplicationFactor: "1"
  KafkaHeapOpts: "-Xmx512m -Xms256m"
  MessageMaxBytes: "314572800"
  configurationOverrides:
    ssl.endpoint.identification.algorithm: "" #To skip hostname verification in ssl
    offsets.retention.minutes: "1440"
  ingress:
    enableExternalAccess: true
    accessRoleLabel: external-access
    externalListeners:
      - name: "EXTERNAL"
        securityMode: SSL
        startPortRangeOnEdgeNode: "9093"
        externalServiceName: "{{ .Values.global.K8S_PUBLIC_IP }}"
        uniqueADL: false
        ADLprefix: ""
        IstioGateway: { }
        IngressConfigMap:
          citmPrefixName: "{{ .Release.Name }}-altiplano-ingress"
  listenerSecurityMode:
    internalSecurityMode: SSL #change to PLAINTEXT for non-ssl mode
  ssl:
    enabled: *useTls
    secret_name: "altiplano-secrets-all-certs"
    keystore_key: server.jks
    truststore_key: trustchain.jks
    truststore_passwd_key: trustchain_jks_pass
    keystore_passwd_key: server_jks_pass
    keystore_key_passwd_key: server_jks_pass
    enabledProtocols: TLSv1.2,TLSv1.3
    protocol: TLSv1.2
    keyStoreType: JKS
    trustStoreType: JKS
    SecurityInterBrokerProtocol: SSL
    securityProtocols: SSL
    secureRamdomImpl: SHA1PRNG
    clientAuth: required
  #Liveliness and Readiness probe configurations
  livenessProbe:
    initialDelaySeconds: 300
    timeoutSeconds: 120
    failureThreshold: 3
    periodSeconds: 10
  readinessProbe:
    initialDelaySeconds: 30
    timeoutSeconds: 30
    failureThreshold: 3
    periodSeconds: 10
  deleteKafkaJob:
    auto_remove_kf_pvc: false
  cbur:
    enabled: false
  # To ensure that logs and other data use a common timezone configure timeZoneEnv.Defaults to UTC
  timeZoneEnv: *timeZoneEnv
  init:
    imageRepo: "ckaf/ckaf-kafka-init"
    imageTag: 9.1.0-rocky8-jre17-4.1.0-8675
    vaultInitRepo: fnms-init-container
    vaultInitTag: nokia-2.0.1
    securityContext:
      runAsNonRoot: true
      runAsUser: 999
      runAsGroup: 997
  initContainer:
    image:
      name: fnms-init-container
      tag: nokia-2.0.1
      pullPolicy: IfNotPresent
  certificates:
    secrets:
      altiplano_keystore_secrets: altiplano-keystore-secrets
    fileNames:
      kafka_server_key_pem: kafka-server-key.pem
      kafka_server_key_pass: kafka-server-key.pass
      kafka_server_cert_pem: kafka-server-cert.pem
      altiplano_keystore_password: keystore-password
      kafka_server_keystore_jks: server.jks
      kafka_server_trustchain_cert_pem: kafka-server-trustchain-cert.pem
      kafka_server_truststore_jks: trustchain.jks
  #Jmx Exporter
  JmxExporter:
    imageRepo: "cpro/cpro-jmx-exporter"
    imageTag: 4.1.0-rocky8-0.20.0-3612
  # Configure PodDisruptionBudget here
  pdb:
    minAvailable: 0

  ckaf-zookeeper:
    kubectlImageRepo: "tools/kubectl"
    kubectlTag: 1.28.7-rocky8-nano-20240301
    imageRepo: "ckaf/ckaf-zookeeper"
    imageTag: 9.0.2-rocky8-jre17-3.6.4-70
    vaultInitRepo: fnms-init-container
    vaultInitTag: nokia-2.0.1
    persistence:
      enabled: *persistence
    cbur:
      enabled: false
    fullnameOverride: "altiplano-zookeeper"
    imagePullPolicy: "IfNotPresent"
    servers: 1
    accessRoleLabel: internal-access
    resources:
      requests:
        cpu: 100m
        memory: 1Gi
        ephemeral-storage: 1G
      limits:
        cpu: 1000m
        memory: 2Gi
        ephemeral-storage: 1G
    ingress:
      enableExternalAccess: false
      accessRoleLabel: external-access
      edgeNodePort: "2181"
      citmPrefixName: "{{ .Release.Name }}-altiplano-ingress"
    zookeeperNodeSelector:
      enable: false
    #  nodeLabel:
    #    "role" : "Infra"

    heap: "768m"
    dataStorage: "12Gi"
    logStorage: "1Gi"
    zookeeperClientPort: 2181
    preAllocSize: 64000
    configurationOverrides:
      maxClientCnxns: 400
    deleteZookeeperJob:
      auto_remove_zk_pvc: false
    # To ensure that logs and other data use a common timezone configure timeZoneEnv.Defaults to UTC
    timeZoneEnv: *timeZoneEnv
    pdb:
      minAvailable: 0
    env:
      secrets:
        IS_USERNAME: fluentd_is_username
        IS_PASSWORD: fluentd_is_password
    zk_fluentd_sidecar:
      image:
        name: "fnms-fluent"
        tag: "nokia-4.1.8"
        pullPolicy: IfNotPresent
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 256Mi
      secrets:
        certSecret: altiplano-zookeeper-certificate-secrets
        passwordSecret: altiplano-secrets
      securityContext:
        readOnlyRootFilesystem: true
        runAsUser: 1000
        runAsGroup: 997
      IS_SCHEME: https
      IS_HOST: altiplano-indexsearch
      IS_PORT: 9200
      LOG_INDEX_PATTERN: logstash
      fluent_conf: |-
    
        <system>
          log_level error
        </system>
      
        <source>
          @type tail
          path /logs/zookeeper/zookeeper.log
          read_from_head true
          pos_file /tmp/fluentd/zookeeper-log.pos
          keep_time_key true
          tag zookeeper.log
          <parse>
            @type multiline
            format_firstline /"time":"\d{4}-\d{1,2}-\d{1,2}T\d{1,2}:\d{1,2}:\d{1,2}.\d{1,3}Z"/
            format1 /^(.*?,){2}[^:]*:"(?<level>[^,]*)",(.*?,){2}[^:]*:"(?<date>[^,]*)",[^:]*:"(?<timezone>[^,]*)",[^:]*:[^:]*:"(?<thread>.*)\s-\s(?<category>[^ ]*)\s-\s(?<message>.*)"}}(?<throwable>.*)/
          </parse>
        </source>
        <filter zookeeper.log>
          @type record_transformer
          enable_ruby true
          <record>
            container_name "#{ENV['CONTAINER_NAME']}"
            container_ip "#{ENV['MY_POD_IP']}"
          </record>
          renew_record true
          keep_keys container_name,container_ip,date,level,thread,category,message,throwable
          remove_keys timezone
        </filter>
        <match zookeeper.log>
          @type opensearch
          reload_on_failure true
          reconnect_on_error true
          logstash_format true
          type_name fluentd
          ssl_verify false
          ssl_version TLSv1_2
          scheme {{ .Values.zk_fluentd_sidecar.IS_SCHEME }}
          host {{ .Values.zk_fluentd_sidecar.IS_HOST }}
          port {{ .Values.zk_fluentd_sidecar.IS_PORT }}
          ca_file "/etc/.certificates/zookeeper-server-trustchain-cert.pem"
          user "#{ENV['IS_USERNAME']}"
          password "#{ENV['IS_PASSWORD']}"
          <buffer>
            @type file
            path /tmp/fluentd/buffer_zookeeper
            overflow_action drop_oldest_chunk
            chunk_limit_size 16MB
            queued_chunks_limit_size  4096
            flush_thread_count 5
            flush_interval 5s
            retry_wait 0s
            retry_forever true
            total_limit_size 50MB
          </buffer>
          time_key date
          time_key_exclude_timestamp true
          suppress_type_name true
          logstash_prefix {{ .Values.zk_fluentd_sidecar.LOG_INDEX_PATTERN }}
          request_timeout 45s
        </match>
        <match **>
          @type null
        </match>
    JmxExporter:
      imageRepo: "cpro/cpro-jmx-exporter"
      imageTag: "4.1.0-rocky8-0.20.0-3612"

altiplano-grafana:
  #enabled: true
  tls:
    enabled: false
  certManager:
    enabled: false
  custom:
    pod:
      annotations:
        kubectl.kubernetes.io/default-container: altiplano-grafana
        kubectl.kubernetes.io/default-logs-container: altiplano-grafana
  accessRoleLabel: internal-access
  replicas: 1
  pdb:
    enabled: false
    minAvailable: 1
    #maxUnavailable: 0
  timeZoneName: *timeZoneEnv
  #hostAliases:
  #  - hostnames:
  #      - "altiplano-ipv6"
  #    ip: "2001:db8:0:3:f816:3eff:fe5c:a179"
  grafana:
    name: grafana
    server_cert: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM5ekNDQWQrZ0F3SUJBZ0lKQU1BMTBFMmdUNHl1TUEwR0NTcUdTSWIzRFFFQkN3VUFNQkl4RURBT0JnTlYKQkFNTUJ6QXVNQzR3TGpBd0hoY05NVGt3T1RBNU1ETTBOVFF5V2hjTk1qa3dPVEEyTURNME5UUXlXakFTTVJBdwpEZ1lEVlFRRERBY3dMakF1TUM0d01JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBCnAyL1E2dVRoVUhCUGFGVWk1OFRJUGd5VGlFWTd4cmhaQVpRT2NVVXZNWU04cUJuMndDOHVwSGs0WnpwRHlhbkUKdHc0c2ZLNDl6ZHNINmZ5UlR1ODdWU3RVWWcrWnFHUXdadTJrQVRPd2pKR0ZRbXhJaWErR2YrWGRjMnlXYm5MMApyOVRKcTVjSlMrL2dQWFhnZmJBU0FHQXl2THFKRDBOa3M2RUx2MUhLNytnTStVcmFUeSt0MXVPMndoRDdLRE5NCnlVRThyc2NFQkxrTmU1TTRNcGI5UklpWFlWblhPZFI0dnMxRjM1dlJuTSt4SjY5TFRPZTJkRXNZS2ptL2Q0QkgKd0dRS2dqdDVScWpxR2hxZGJGRURiRnQ5VFBiZmR4UmF6cFRXcGpEaXR6em13aUZaWkxpcW1xekMvZmVkSGcrNgpYU2M0Z1VXUjlJYU8yS0g2cWNDTHp3SURBUUFCbzFBd1RqQWRCZ05WSFE0RUZnUVVHUTlINWVFdkEvYTJVTVlJCjVzaFpzZ1F1SWFzd0h3WURWUjBqQkJnd0ZvQVVHUTlINWVFdkEvYTJVTVlJNXNoWnNnUXVJYXN3REFZRFZSMFQKQkFVd0F3RUIvekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBallERXRTU3JaWUJHNnVhMGRZYVZpOXZYOHMvQgpsNkgreDRESGE4dXE3OFZCdGRGNmxMVUdrUVo2eUFsNTNZUzdUUFAvbVZsMHpWS0hCUWNWdmowdXRoK1FBSnUvCjRvcmF6UUNMQzNXUWczZ0d0cmRFRTd1UXRKZGZRUFVoZjZsT3FzT2c1V1hPc0tMUUVFWTNYYmh2VDlsUWF5eUcKajVSWXkrTzBCd1A3bVBDU0U2bHh4ZGV0R0tFU2FTYjU2bytieEYvNGFFQnpYYU9JT2RGMEg2aDhxQXcxNitKSApDREZlSE8yVHE2aFdoeVorOUl0dUdpdkVtQlJmTUZGTXB1RlZ0V0hZQ3B2STdNajZtYUtIZWlsRUkzS3BLWjdGCi9ZYXJkZGtkeW92L0FKS2ErQlBGNGVUZ1AvbittUi9OZEV1STJFWllGL2huSkwrVWJNZEhvWHMvenc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0t"
    server_key: "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"
  keycloak:
    cert: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURrakNDQW5xZ0F3SUJBZ0lFRjJFNFNEQU5CZ2txaGtpRzl3MEJBUXNGQURCeE1SQXdEZ1lEVlFRR0V3ZFYKYm10dWIzZHVNUkF3RGdZRFZRUUlFd2RWYm10dWIzZHVNUkF3RGdZRFZRUUhFd2RWYm10dWIzZHVNUkF3RGdZRApWUVFLRXdkVmJtdHViM2R1TVJBd0RnWURWUVFMRXdkVmJtdHViM2R1TVJVd0V3WURWUVFERXd3eE1DNDNOaTQ0Ck5DNHhPVEl3SGhjTk1UZ3dPRE14TVRreU5USTJXaGNOTkRnd09ESXpNVGt5TlRJMldqQnhNUkF3RGdZRFZRUUcKRXdkVmJtdHViM2R1TVJBd0RnWURWUVFJRXdkVmJtdHViM2R1TVJBd0RnWURWUVFIRXdkVmJtdHViM2R1TVJBdwpEZ1lEVlFRS0V3ZFZibXR1YjNkdU1SQXdEZ1lEVlFRTEV3ZFZibXR1YjNkdU1SVXdFd1lEVlFRREV3d3hNQzQzCk5pNDROQzR4T1RJd2dnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUUNndlFhQXdKdDkKVmJjeVJGY2kwUFpJc3VkZ1p4cmtheFBEK1ppd0NKVTlLRUU3dUY2b2YyQTRwTFRudm9aMWZwVUNQQTBteWxEYwp0OU1Xdk5qMVJoQlBKaTBlY2YvT1BVYThoUUJLRUlJakhCM2dGb3l5TnhmN0hFczREcHltY2ZCN3ErN0FBWDQxCjhJOGhqWUdwT3NJa2pwUVE3Rk5NYmxBdHRPRG96OEFmanVTRHEvcDMvVHZ3aUIyRGxBeERxWXFmZ0Q1MysyTmoKU2EwTHFsZjl6TWxIcW5TcTJDbVpUbTdib3pGMWpSYjdCT2hOaTUwOWQ2cTl4alQzbmFGTE1QeUJuMXNBT3gwRgpNWW9aWUVaY1VwckdTa0I1RWxFTnhWZ09WY1FvZEdxOTQvb0I3SFo1NDFZVW5zdDBZRzA0YXVMZlppSzhpUDhLCjRaMm93dzBuQU1lekFnTUJBQUdqTWpBd01BOEdBMVVkRVFRSU1BYUhCQXBNVk1Bd0hRWURWUjBPQkJZRUZEeHoKanZWWUVoTWN0NDd5RE84RE5GQmpsZVdLTUEwR0NTcUdTSWIzRFFFQkN3VUFBNElCQVFDRTBWcDYvY0xrVytEMQpQMnNsWWliSTNDNnh1OXc1SHVkbUZkd2poeHZMNkZmNWVWMmJkblZMN1JPbmRZV2RHTjdINjhPR3Z6OVoza1ZjCjdaaDlmMzl4Z2RyRk1kTHBZSGZmeXJiSE82ZUNQMUpKVmUzalV6LzBPT2lzeVg2eFlsM2lpZGZySWJuN0VWT00KSWNmMkhFd2tDcDYzZ0drV2I3emg3QXlDeDE1cVBkSkR3UnlOWThwTnZ2WmdZM0Rab09MVXNRVnNSMWJJME15aApiSlp0OWdwOUtJRlRueTRUeWVNYSt0U01GV0t2ZTB1UnRCaUxuNi8xK2NPNkdiQk0rWDB2Q0dnSjJQNGM1ZmoxCnQvcUZrLzgvUEYyRW9TS3hkQzBrSXdWVjFxb28zMXdlS1ByQmlqRUZtalIxb2Q5dTFhVmQveFM4U2R4dTBlRCsKM2h1bjBKcXEKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQ=="
  helm3: true
  image:
    distro:
      imageRepo: cpro/grafana-registry1/cpro-grafana
      imageTag: 2.0.4-10.2.1-163
      __defaultFlavor: distroless
      imagePullPolicy: IfNotPresent
    utility:
      imageRepo: cpro/grafana-registry1/cpro-grafana-util
      imageTag: 3.0.4-163
      imagePullPolicy: IfNotPresent
      # Do not change the below and __defaultFlavor values
      __defaultFlavor: rocky8
    python:
      imageRepo: cpro/grafana-registry1/cpro-grafana-kiwigrid
      imageTag: 3.0.4-1.25.2-163
      imagePullPolicy: IfNotPresent
      # Do not change the below parameters starting with __
      __defaultFlavor: rocky8-python3.8
  init:
    image:
      name: fnms-init-container
      tag: nokia-2.0.1
      pullPolicy: IfNotPresent
  envFromSecret: "altiplano-secrets"
  sensitiveDataSecretName: "altiplano-grafana-secrets"
  terminationGracePeriodSeconds: "0"
  adminUser: $(grafana-admin-user)
  adminPassword: $(grafana-admin-password)
  service:
    type: ClusterIP
      #ports:
    #grafanaNodePort: 30030
  sidecar:
    resources:
      limits:
        cpu: 100m
        memory: 200Mi
      requests:
        cpu: 50m
        memory: 50Mi
    SKIP_TLS_VERIFY: false
    enableUniqueFilenames: false
    dashboards:
      enabled: true
      # label that the configmaps with dashboards are marked with
      label: "altiplano_dashboard"
      # folder in the pod that should hold the collected dashboards
      folder: "/home/dashboards"
      folderAnnotation: "grafana_folderpath"
      resource: "both"
      # The configuration will be in the form of key-value pairs
      # supporting all environment variables provided by Kiwigrid.
      config:
      # seconds to wait before watching resources again when an error occurs
      - name: ERROR_THROTTLE_SLEEP
        value: "300"
    provider:
      name: sidecarProvider
      disableDelete: true
      allowUiUpdates: true
  datasources:
    # The configuration will be in the form of key-value pairs
    # supporting all environment variables provided by Kiwigrid.
    config:
    # seconds to wait before watching resources again when an error occurs
    - name: ERROR_THROTTLE_SLEEP
      value: "300"
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://altiplano-pts-server:80
          access: proxy
          isDefault: true
          editable: true
        - name: opentsdb
          type: opentsdb
          url: http://altiplano-opentsdb:4242
          jsonData:
            tsdbVersion: 2
          access: proxy
          isDefault: false
          editable: true
        - name: opensearch
          type: elasticsearch
          url: https://altiplano-indexsearch:9200
          database: "[logstash-]YYYY.MM.DD"
          jsonData:
            interval: Daily
            timeField: "date"
            esVersion: "8.0.0"
            tlsSkipVerify: true
          access: proxy
          isDefault: false
          editable: true
          basicAuth: true
          basicAuthUser: ${ALTIPLANO_INTERNAL_IS_USER}
          basicAuthPassword: ${ALTIPLANO_INTERNAL_IS_PASSWORD}
  rbac:
    create: true
    psp:
      create: false
  cmdb:
    enabled: false
  helmDeleteImage:
    imagerocky:
      imageRepo: tools/kubectl
      imageTag: 1.28.7-nano-20240301
      __defaultFlavor: rocky8
      imagePullPolicy: IfNotPresent
  securityContext: 
    runAsNonRoot: true
    runAsUser: 65534
    runAsGroup: 65534
    fsGroup: 65534
    supplementalGroups: [65534]
    seccompProfile:
      type: RuntimeDefault 
  cbur:
    enabled: false
  pluginsSideCar:
    enabled: false
  SetDatasource:
    enabled: false
  SetDashboard:
    enabled: false
  persistence:
    enabled: false
    annotations:
      helm.sh/resource-policy: keep
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/disable-annotation-validation: "true"
      nginx.ingress.kubernetes.io/proxy-pass-suffix: $uri
      nginx.ingress.kubernetes.io/limit-rps: *ingressLimitRps
      nginx.ingress.kubernetes.io/limit-burst-multiplier: *ingressLimitBurstMultiplier
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers "X-Frame-Options: DENY";
        rewrite ^ $request_uri;
        rewrite "(?i)/altiplano-grafana(/|$)(.*)" /$2 break;
        return 400;
    #kubernetes.io/tls-acme: "true"
    path: /altiplano-grafana/?(.*)
    #hosts:
    #- "*"
    tls: [ ]
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
  nginxIncIngress:
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.org/mergeable-ingress-type: minion
      nginx.org/location-snippets: |
        add_header X-Frame-Options DENY;
        proxy_set_header X-Forwarded-Prefix         /altiplano-grafana;
        rewrite "(?i)/altiplano-grafana(/|$)(.*)" /$2 break;
        return 400;
      nginx.org/redirect-to-https: "true"
    path: /altiplano-grafana
  grafana_ini:
    feature_toggles:
      enable: "envelopeEncryption"
    security:
      admin_user: "{{ .Values.adminUser }}"
      admin_password: "{{ .Values.adminPassword }}"
    server:
      root_url: https://{{ ternary "[" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}{{ ternary .Values.global.K8S_PUBLIC_IP .Values.global.K8S_PUBLIC_HOSTNAME (not .Values.global.K8S_PUBLIC_HOSTNAME) }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}/altiplano-grafana
      serve_from_sub_path: false
    auth:
      signout_redirect_url: https://{{ ternary "[" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}{{ ternary .Values.global.K8S_PUBLIC_IP .Values.global.K8S_PUBLIC_HOSTNAME (not .Values.global.K8S_PUBLIC_HOSTNAME) }}{{ ternary "]" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/logout?client_id=ALTIPLANO&post_logout_redirect_uri=https://{{ ternary "[" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}{{ ternary .Values.global.K8S_PUBLIC_IP .Values.global.K8S_PUBLIC_HOSTNAME (not .Values.global.K8S_PUBLIC_HOSTNAME) }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}{{ .Values.ingress.path }}

      #signout_redirect_url: https://{{ if and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP) }}[{{ .Values.global.K8S_PUBLIC_IP }}{{ else if .Values.global.EXTERNAL_SERVICE_IP }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ else if not .Values.global.K8S_PUBLIC_HOSTNAME }}[{{ .Values.global.K8S_PUBLIC_IP }}{{ else }}{{ .Values.global.K8S_PUBLIC_HOSTNAME }}{{ end }}{{ if or (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) (and .Values.global.INGRESS_HTTPSPORT_ENABLED (not .Values.global.K8S_PUBLIC_HOSTNAME)) }}]{{ else }}{{ end }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/logout?client_id=ALTIPLANO&post_logout_redirect_uri=https://{{ if and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP) }}[{{ .Values.global.K8S_PUBLIC_IP }}{{ else if not .Values.global.K8S_PUBLIC_HOSTNAME }}[{{ .Values.global.K8S_PUBLIC_IP }}{{ else }}{{ .Values.global.K8S_PUBLIC_HOSTNAME }}{{ end }}{{ if .Values.global.INGRESS_HTTPSPORT_ENABLED }}:{{ .Values.global.INGRESS_HTTPSPORT_ENABLED }}{{ else if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}{{ end }}{{ if or (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) (and .Values.global.INGRESS_HTTPSPORT_ENABLED (not .Values.global.K8S_PUBLIC_HOSTNAME)) }}]{{ else }}{{ end }}{{ .Values.ingress.path }}

      oauth_auto_login: true
    auth.generic_oauth:
      enabled: true
      name: master
      tls_skip_verify_insecure: true
      email_attribute_path: (contains(keys(@), 'user')) && (contains(user, 'email')) && user.email || preferred_email || preferred_username
      allow_sign_up: true
      client_id: ALTIPLANO
      client_secret: ${KC_CLIENT_SECRET}
      scopes: openid
      auth_url: https://{{ ternary "[" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}{{ ternary .Values.global.K8S_PUBLIC_IP .Values.global.K8S_PUBLIC_HOSTNAME (not .Values.global.K8S_PUBLIC_HOSTNAME) }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/auth
      token_url: https://{{ ternary "[" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}{{ ternary .Values.global.K8S_PUBLIC_IP .Values.global.K8S_PUBLIC_HOSTNAME (not .Values.global.K8S_PUBLIC_HOSTNAME) }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/token
      api_url: https://{{ ternary "[" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}{{ ternary .Values.global.K8S_PUBLIC_IP .Values.global.K8S_PUBLIC_HOSTNAME (not .Values.global.K8S_PUBLIC_HOSTNAME) }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/userinfo
      introspect_url: https://{{ ternary "[" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}{{ ternary .Values.global.K8S_PUBLIC_IP .Values.global.K8S_PUBLIC_HOSTNAME (not .Values.global.K8S_PUBLIC_HOSTNAME) }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/token/introspect

      #auth_url: https://{{ if and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP) }}[{{ .Values.global.K8S_PUBLIC_IP }}{{ else if .Values.global.EXTERNAL_SERVICE_IP }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ else if not .Values.global.K8S_PUBLIC_HOSTNAME }}{{ .Values.global.K8S_PUBLIC_IP }}{{ else }}{{ .Values.global.K8S_PUBLIC_HOSTNAME }}{{ end }}{{ if .Values.global.INGRESS_HTTPSPORT_ENABLED }}:{{ .Values.global.INGRESS_HTTPSPORT_ENABLED }}{{ else if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}{{ end }}{{ if or (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) (and .Values.global.INGRESS_HTTPSPORT_ENABLED (not .Values.global.K8S_PUBLIC_HOSTNAME)) }}]{{ else }}{{ end }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/auth
      #token_url: https://{{ if and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP) }}[{{ .Values.global.K8S_PUBLIC_IP }}{{ else if .Values.global.EXTERNAL_SERVICE_IP }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ else if not .Values.global.K8S_PUBLIC_HOSTNAME }}{{ .Values.global.K8S_PUBLIC_IP }}{{ else }}{{ .Values.global.K8S_PUBLIC_HOSTNAME }}{{ end }}{{ if .Values.global.INGRESS_HTTPSPORT_ENABLED }}:{{ .Values.global.INGRESS_HTTPSPORT_ENABLED }}{{ else if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}{{ end }}{{ if or (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) (and .Values.global.INGRESS_HTTPSPORT_ENABLED (not .Values.global.K8S_PUBLIC_HOSTNAME)) }}]{{ else }}{{ end }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/token
      #api_url: https://{{ if and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP) }}[{{ .Values.global.K8S_PUBLIC_IP }}{{ else if .Values.global.EXTERNAL_SERVICE_IP }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ else if not .Values.global.K8S_PUBLIC_HOSTNAME }}{{ .Values.global.K8S_PUBLIC_IP }}{{ else }}{{ .Values.global.K8S_PUBLIC_HOSTNAME }}{{ end }}{{ if .Values.global.INGRESS_HTTPSPORT_ENABLED }}:{{ .Values.global.INGRESS_HTTPSPORT_ENABLED }}{{ else if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}{{ end }}{{ if or (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) (and .Values.global.INGRESS_HTTPSPORT_ENABLED (not .Values.global.K8S_PUBLIC_HOSTNAME)) }}]{{ else }}{{ end }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/userinfo
      #introspect_url: https://{{ if and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP) }}[{{ .Values.global.K8S_PUBLIC_IP }}{{ else if .Values.global.EXTERNAL_SERVICE_IP }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ else if not .Values.global.K8S_PUBLIC_HOSTNAME }}{{ .Values.global.K8S_PUBLIC_IP }}{{ else }}{{ .Values.global.K8S_PUBLIC_HOSTNAME }}{{ end }}{{ if .Values.global.INGRESS_HTTPSPORT_ENABLED }}:{{ .Values.global.INGRESS_HTTPSPORT_ENABLED }}{{ else if .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}:{{ .Values.global.INGRESS_CONTROLLER_HTTPSPORT }}{{ end }}{{ if or (and (not .Values.global.K8S_PUBLIC_HOSTNAME) (contains ":" .Values.global.K8S_PUBLIC_IP)) (and .Values.global.INGRESS_HTTPSPORT_ENABLED (not .Values.global.K8S_PUBLIC_HOSTNAME)) }}]{{ else }}{{ end }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/token/introspect

      role_attribute_path: contains(grafanauserrole[*], '/grafanaAdmin') && 'Admin' || contains(grafanauserrole[*], '/GrafanaEditor') && 'Editor' || 'Viewer'
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 400m
      memory: 400Mi
  livenessProbe:
    timeoutSeconds: 10
  nodeSelector: {}

altiplano-webdav:
  #enabled: true
  image:
    registry: #artifactory.net.nokia.com
    #repository: #fnms-file-server
    tag: nokia-1.2.25
  init:
    repository: fnms-init-container
    tag: nokia-2.0.1
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
    #type: Recreate
  service:
    type: ClusterIP
    #ports:
      #webdavNodePort: 30080
      #httpsNodePort: 30081
  accessRoleLabel: external-access
  persistence:
    enabled: *persistence
    storageClass: *storageClass
    accessMode: ReadWriteOnce
    # Size of the persistent storage of webdav pod
    size: 10Gi
  env:
    secrets:
      USERNAME: admin
      PASSWORD: nokiafnms@123
    timeZoneEnv: *timeZoneEnv
    # This ENV variable is used when multiple Network Virtualizers are managed from the same Access Controller.
    # The ENV variable value is a string containing the name of the directory shared by all Network Virtualizers to store device software files
    # When the ENV variable is empty, each Virtualizer use their own directory on WebDAV
    OPTIMIZED_DEVICE_SW_FILE_STORAGE_DIRECTORY_ON_WEBDAV: ""
    open: {} # remove the braces for the below values to take effect
      #HTTPS_ENABLED: false #All other values are considered true
  ingress:
    <<: *ingressconfig
    #path: /altiplano-webdav
  nginxIncIngress:
    <<: *nginxIncIngressConfig
  resources:
    requests:
      cpu: 50m
      memory: 100Mi
    limits:
      cpu: 1
      memory: 4Gi
  cbur:
    enabled: *cburEnable
    apiVersion: *cburApiVersion
    #the maximum copy you want to saved.
    maxiCopy: *backupRetain
    #Modes supported now: "local","NETBKUP","AVAMAR","CEPHS3","AWSS3", case insensitive
    backendMode: *globalBackend
    #specifies if the backup scheduling cron job should be auto-scheduled
    autoEnableCron: true
    #specifies cron update will be triggered automatically by BrPolicy update
    autoUpdateCron: false
    # allows user to schedule backups
    cronSpec: "0 1 * * *"
    ## should backups be encrypted?
    dataEncryption: false
    persistence:
      cburtmp:
        enabled: *cburEnable
        storageClass: *storageClass
        accessMode: ReadWriteOnce
        # Size of the persistent storage of webdav-cbur
        size: 20Gi
        dir: /tmp
    cbura:
      imageRepo: cbur/cbur-agent
      imageTag: 1.2.0-alpine-580
      imagePullPolicy: IfNotPresent
      resources:
        limits:
          cpu: "1"
          memory: "2Gi"
        requests:
          cpu: "500m"
          memory: "1Gi"

altiplano-pts:
  useCentos7Image: false
  usePodNamePrefixAlways: false
  init:
    image:
      name: fnms-init-container
      tag: nokia-2.0.1
      pullPolicy: IfNotPresent
  image:
    distro:
      repo: cpro/registry4/cpro-prometheus-metrics
      tag: 3.0.1-3220
      imagePullPolicy: IfNotPresent
    python:
      repo: cpro/registry4/cpro-prometheus-util
      tag: 3.0.1-3220
      imagePullPolicy: IfNotPresent
  helmDeleteImage:
    image:
      imageRepo: tools/kubectl
      imageTag: 1.28.7-nano-20240301
      __defaultFlavor: rocky8
      imagePullPolicy: IfNotPresent
  timeZoneName: *timeZoneEnv
  managedBy: ""
  #enabled: true
  ## If the flag is set to true, then server component scrape the metrics within listed namespaces
  ## else scrape the metrics at cluster level.
  ## Namespace listed in values_restricted.yaml file.
  ## If the flag is set to true, then restserver component is forbidden to access
  ## the configmap or any other resources of prometheus server from different namespaces and role/rolebinding is created
  ## else clusterrole/clusterrolebinding is created and able to access the configmaps
  ## and other resources of prometheus server across the different namespaces.
  restrictedToNamespace: *restrictedNamespaceEnabled
  ## If true, high avaibility feature will be enabled
  ## altermanger and server could create 2 instances
  ## If false, altermanger and server could create only 1 instance
  initChownData:
    enabled: false
  configmapReload:
    distroImage:
      imageRepo: cpro/registry4/cproconfigmap-reload
      imageTag: ${cproconfigmap-reload.version}
      imagePullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 10m
        memory: 32Mi
      requests:
        cpu: 10m
        memory: 32Mi
  ha:
    enabled: false
  tls:
    enabled: false
  pushgateway:
    enabled: true
    fullnameOverride: altiplano-pts-pushgateway
    accessRoleLabel: internal-access
    podAnnotations:
      kubectl.kubernetes.io/default-container: altiplano-pts-pushgateway
      kubectl.kubernetes.io/default-logs-container: altiplano-pts-pushgateway
    certificate:
      enabled: false
    pdb:
      enabled: false
      minAvailable: 1
      #maxUnavailable: 0
    ## PushGateway workload labels and annotations
    annotations: {}
    labels:
      accessRoleLabel: internal-access
    extraArgs:
      push.disable-consistency-check: ""
    nodeSelector: {}
    persistentVolume:
      enabled: false
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 150m
        memory: 100Mi
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
      runAsGroup: 65534
      runAsUser: 65534
      fsGroup: 65534
  webhook4fluentd:
    enabled: false
    certificate:
      enabled: false
  networkPolicy:
    enabled: false
  kubeStateMetrics:
    ## If false, kube-state-metrics will not be installed
    enabled: true
    podAnnotations:
      kubectl.kubernetes.io/default-container: altiplano-pts-kube-state-metrics
      kubectl.kubernetes.io/default-logs-container: altiplano-pts-kube-state-metrics
    certificate:
      enabled: false
    pdb:
      enabled: false
      minAvailable: 1
      #maxUnavailable: 0
    ## Kube-state-metrics workload labels & annotations
    annotations: {}
    labels:
      accessRoleLabel: internal-access
    args:
      metricLabelsAllowlist:
        - pods=[*]
      namespace: *altiplanoNamespace
      ##Uncomment bellow lines if restrictedToNamespace is true
      #collectors:
        #configmaps: true
        #cronjobs: true
        #daemonsets: true
        #deployments: true
        #endpoints: true
        #horizontalpodautoscalers: true
        #ingresses: true
        #jobs: true
        #limitranges: true
        #networkpolicies: true
        #persistentvolumeclaims: true
        #poddisruptionbudgets: true
        #pods: true
        #replicasets: true
        #replicationcontrollers: true
        #resourcequotas: true
        #secrets: true
        #services: true
        #statefulsets: true
        #certificatesigningrequests: false
        #mutatingwebhookconfigurations: false
        #namespaces: false
        #nodes: false
        #persistentvolumes: false
        #storageclasses: false
        #validatingwebhookconfigurations: false
        #verticalpodautoscalers: false
        #volumeattachments: false
    nodeSelector: {}
    resources:
      limits:
        cpu: 50m
        memory: 200Mi
      requests:
        cpu: 10m
        memory: 100Mi
    securityContext:
      runAsNonRoot: true
      seccompProfile:
        type: RuntimeDefault
      runAsGroup: 65534
      runAsUser: 65534
      fsGroup: 65534
  nodeExporter:
    ## If false, node-exporter will not be installed
    ## set to false if restrictedToNamespace is true
    enabled: true
    dnsPolicy: ClusterFirstWithHostNet
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    podAnnotations:
      kubectl.kubernetes.io/default-container: altiplano-pts-node-exporter
      kubectl.kubernetes.io/default-logs-container: altiplano-pts-node-exporter
    ## Node-exporter workload labels and annotations
    annotations: {}
    labels:
      accessRoleLabel: internal-access
    certificate:
      enabled: false
    tls_auth_config:
      secretForWebConfig: "altiplano-pts-node-exporter-secrets"
      basic_auth:
        enabled: true
      tls:
        enabled: false # tls is enabled via tls_server_config and extraConfigmapMounts config below
    web_yml: |-
      http_server_config:
       http2: false
      basic_auth_users:
       prometheus: $2y$10$EYxs8IOG46m9CtpB/XlPxO1ei7E4BjAen0SUv6di7mD4keR/8JO6m
      tls_server_config:
       client_ca_file: /etc/ssl/certs/ca.crt
       key_file: /etc/ssl/certs/server.key
       cert_file: /etc/ssl/certs/server.crt
       client_auth_type: "VerifyClientCertIfGiven"
       # List of supported cipher suites for TLS versions up to TLS 1.2. If empty,
       # Go default cipher suites are used. Available cipher suites are documented
       # in the go documentation:
       # https://golang.org/pkg/crypto/tls/#pkg-constants
       #
       # Note that only the cipher returned by the following function are supported:
       # https://pkg.go.dev/crypto/tls#CipherSuites
       cipher_suites:
         - "TLS_AES_128_GCM_SHA256"
         - "TLS_AES_256_GCM_SHA384"
         - "TLS_CHACHA20_POLY1305_SHA256"
    extraConfigmapMounts:
      - name: certs-configmap
        mountPath: /etc/ssl/certs
        configMap: certs-configmap
        readOnly: true
    podSecurityContext:
      runAsNonRoot: true
      runAsGroup: 65534
      runAsUser: 65534
      fsGroup: 65534
      seccompProfile:
        type: RuntimeDefault
    nodeSelector: {}
    resources:
      limits:
        cpu: 150m
        memory: 200Mi
      requests:
        cpu: 50m
        memory: 100Mi
    # Additional node-exporter container arguments
    # Update "web.listen-address" If you have updated podHostPort & podContainerPort
    # web.listen-address value should be same as  podHostPort & podContainerPort
    # please disable timex collector by uncommenting no-collector.timex when scc is false in the openshift  **
    extraArgs:
      web.listen-address: ":9100"
      #no-collector.timex:
    service:
      podContainerPort: 9100
      podHostPort: 9100
  zombieExporter:
    ## If false, zombie-exporter will not be installed
    enabled: false
    certificate:
      enabled: false
  alertmanager:
    enabled: true
    certificate:
      enabled: false
    labels:
      accessRoleLabel: internal-access
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
      runAsGroup: 65534
      fsGroup: 65534
      seccompProfile:
        type: RuntimeDefault
    persistentVolume:
      enabled: false
    service:
      type: NodePort
  server:
    enabled: true
    accessRoleLabel: external-access
    fullnameOverride: altiplano-pts-server
    configmapReload:
      name: altiplano-pts-server-configmap-reload
    cproUtil:
      name: altiplano-pts-util

    ## If the restrictedToNamespace flag is set to true, then list the namespaces to monitor in comma-separated value
    ## Example: namespaceList: ['test1','test2']
    namespaceList: [ *altiplanoNamespace ]
    podAnnotations:
      kubectl.kubernetes.io/default-container: altiplano-pts-server
      kubectl.kubernetes.io/default-logs-container: altiplano-pts-server
    certificate:
      enabled: false
    pdb:
      enabled: false
      minAvailable: 1
      #maxUnavailable: 0
    ## Server workload labels and annotations
    labels:
      accessRoleLabel: internal-access
    annotations: {}
    persistentVolume:
      enabled: *persistence
      storageClass: *storageClass
      annotations:
        "helm.sh/resource-policy": keep
      size: 16Gi
    cbur:
      enabled: false
    ## Prometheus data retention
    retention:
      ## How long to retain samples in storage. Units supported: s, m, h, d, w, y. Default is 15d.
      time: "15d"
    nodeSelector: {}
    resources:
      limits:
        cpu: 500m
        memory: 5Gi
      requests:
        cpu: 160m
        memory: 1Gi
    securityContext:
      runAsNonRoot: true
      runAsGroup: 65534
      runAsUser: 65534
      fsGroup: 65534
      seccompProfile:
        type: RuntimeDefault
    extraSecretMounts:
      - name: secrets
        mountPath: /etc/secrets
        secretName: altiplano-secrets
        readOnly: true
    extraConfigmapMounts:
      - name: certs-configmap
        mountPath: /etc/ssl/certs
        configMap: certs-configmap
        readOnly: true
    service:
      servicePort: 80
      nodePort: 30008
      type: NodePort
  restserver:
    enabled: false
    certificate:
      enabled: false
  serverFiles:
    prometheus.yml:
      global:
        scrape_interval: 15s
      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets:
                - localhost:9090
          honor_labels: true

          kubernetes_sd_configs:
            - role: endpoints
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          metric_relabel_configs:
            - action: labeldrop
              regex: __cpro_dummy_label_to_drop

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: prometheus
            - source_labels: [__meta_kubernetes_pod_container_name]
              action: drop
              regex: istio-proxy
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: kubernetes_pod_name


        # Uncomment the below lines if etcd metrics are to be scrapped in BCMT environment.
        #- job_name: bcmt-etcd
        #  kubernetes_sd_configs:
        #    - role: node
        #  metrics_path: /metrics
        #  scheme: https
        #  tls_config:
        #    ca_file: /etc/etcd/ssl/ca.crt
        #    cert_file: /etc/etcd/ssl/tls.crt
        #    key_file: /etc/etcd/ssl/tls.key
        #    insecure_skip_verify: true
        #  metric_relabel_configs:
        #    - action: labeldrop
        #      regex: __cpro_dummy_label_to_drop
        #  relabel_configs:
        #  - action: keep
        #    regex: true
        #    source_labels:
        #    - __meta_kubernetes_node_label_is_control
        #  - action: replace
        #    regex: (.*)
        #    replacement: $1:2379
        #    source_labels:
        #    - __meta_kubernetes_node_address_InternalIP
        #    target_label: __address__
        #  - action: replace
        #    regex: (.*)
        #    replacement: $1
        #    source_labels:
        #    - job
        #    target_label: component

        - job_name: 'kubernetes-apiservers'

          kubernetes_sd_configs:
            - role: endpoints
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          # Keep only the default/kubernetes service endpoints for the https port. This
          # will add targets for each API server which Kubernetes adds an endpoint to
          # the default/kubernetes service.
          metric_relabel_configs:
            - action: labeldrop
              regex: __cpro_dummy_label_to_drop

          relabel_configs:
            - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
              action: keep
              regex: default;kubernetes;https
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace

        - job_name: 'kubernetes-nodes'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          metric_relabel_configs:
            - action: labeldrop
              regex: __cpro_dummy_label_to_drop

          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/${1}/proxy/metrics
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace


        - job_name: 'kubernetes-nodes-cadvisor'

          # Default to scraping over https. If required, just disable this or change to
          # `http`.
          scheme: https

          # This TLS & bearer token file config is used to connect to the actual scrape
          # endpoints for cluster components. This is separate to discovery auth
          # configuration because discovery & scraping are two separate concerns in
          # Prometheus. The discovery auth config is automatic if Prometheus runs inside
          # the cluster. Otherwise, more config options have to be provided within the
          # <kubernetes_sd_config>.
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            # If your node certificates are self-signed or use a different CA to the
            # master CA, then disable certificate verification below. Note that
            # certificate verification is an integral part of a secure infrastructure
            # so this should only be disabled in a controlled environment. You can
            # disable certificate verification by uncommenting the line below.
            #
            insecure_skip_verify: true
          bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          kubernetes_sd_configs:
            - role: node
              namespaces:
                names: [ *altiplanoNamespace ]
          # This configuration will work only on kubelet 1.7.3+
          # As the scrape endpoints for cAdvisor have changed
          # if you are using older version you need to change the replacement to
          # replacement: /api/v1/nodes/${1}:4194/proxy/metrics
          # more info here https://github.com/coreos/prometheus-operator/issues/633
          metric_relabel_configs:
            - action: labeldrop
              regex: __cpro_dummy_label_to_drop

          relabel_configs:
            - action: labelmap
              regex: __meta_kubernetes_node_label_(.+)
            - target_label: __address__
              replacement: kubernetes.default.svc:443
            - source_labels: [__meta_kubernetes_node_name]
              regex: (.+)
              target_label: __metrics_path__
              replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace

        - job_name: 'prometheus-pushgateway'
          honor_labels: true
          scheme: http
          kubernetes_sd_configs:
            - role: service
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          metric_relabel_configs:
            - action: labeldrop
              regex: __cpro_dummy_label_to_drop

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: pushgateway
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__

        - job_name: 'kubernetes-pods-insecure'
          kubernetes_sd_configs:
            - role: pod
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          metric_relabel_configs:
            - action: labeldrop
              regex: __cpro_dummy_label_to_drop

          relabel_configs: # If first two labels are present, pod should be scraped  by the istio-secure job.
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
              action: keep
              regex: true
            - source_labels: [__meta_kubernetes_pod_annotation_sidecar_istio_io_status, __meta_kubernetes_pod_annotation_istio_mtls]
              action: drop
              regex: (([^;]+);([^;]*))|(([^;]*);(true))
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              action: drop
              regex: https
            - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              action: replace
              target_label: __metrics_path__
              regex: (.+)
            - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              action: replace
              regex: (.+)(?::\d+);(\d+)
              replacement: $1:$2
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_pod_name]
              action: replace
              target_label: kubernetes_pod_name
            - source_labels: [__meta_kubernetes_pod_node_name]
              action: replace
              target_label: kubernetes_io_hostname
            - source_labels: [ __meta_kubernetes_pod_container_init ]
              action: drop
              regex: true

        # Example scrape config for probing services via the Blackbox Exporter.
        #
        # The relabeling allows the actual service scrape endpoint to be configured
        # via the following annotations:
        #
        # * `prometheus.io/probe`: Only probe services that have a value of `true`
        #
        #
        - job_name: 'prometheus-nodeexporter'
          honor_labels: true

          kubernetes_sd_configs:
            - role: endpoints
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          ## Following parameters needs to set if prometheus has to scrape metrices in tls mode
          scheme: https
          basic_auth:
            username: prometheus
            password: inuitsdemo #$2y$10$EYxs8IOG46m9CtpB/XlPxO1ei7E4BjAen0SUv6di7mD4keR/8JO6m
          tls_config:
            ca_file: /etc/ssl/certs/ca.crt
            key_file: /etc/ssl/certs/server.key
            cert_file: /etc/ssl/certs/server.crt
            insecure_skip_verify: true

          metric_relabel_configs:
            - action: labeldrop
              regex: __cpro_dummy_label_to_drop

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: node-exporter
            - source_labels: [__meta_kubernetes_namespace]
              action: replace
              target_label: namespace
            - source_labels: [__meta_kubernetes_pod_node_name]
              target_label: kubernetes_io_hostname
              action: replace

        - job_name: 'grafana'
          honor_labels: true
          kubernetes_sd_configs:
            - role: endpoints
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          ## Following parameters needs to set if prometheus has to scrape metrices in tls mode
          #  scheme: https
          #  basic auth for kubestatemetrics is experimental
          #  basic_auth:
          #    username:
          #    password:
          tls_config:
            #    ca_file: /etc/grafana/tls-grafana/ca.crt
            #    key_file: /etc/grafana/tls-grafana/tls.key
            #    cert_file: /etc/grafana/tls-grafana/tls.crt
            insecure_skip_verify: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: grafana
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_scheme
              target_label: __scheme__
            - action: replace
              regex: (.+)(?::\d+);(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_service_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_service_name
              target_label: kubernetes_name
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: kubernetes_io_hostname

        - job_name: 'cpro-ksm'
          honor_labels: true
          kubernetes_sd_configs:
            - role: endpoints
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          ## Following parameters needs to set if prometheus has to scrape metrices in tls mode
          #  scheme: https
          #  basic auth for kubestatemetrics is experimental
          #  basic_auth:
          #    username:
          #    password:
          #  tls_config:
          #    ca_file: /etc/ksm/tls-kubestatemetrics/ca.crt
          #    key_file: /etc/ksm/tls-kubestatemetrics/tls.key
          #    cert_file: /etc/ksm/tls-kubestatemetrics/tls.crt
          #    insecure_skip_verify: true
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: kube-state-metrics
            - action: replace
              regex: (.+)
              source_labels:
                - __meta_kubernetes_service_annotation_prometheus_io_path
              target_label: __metrics_path__
            - action: replace
              regex: (.+)(?::\d+);(\d+)
              replacement: $1:$2
              source_labels:
                - __address__
                - __meta_kubernetes_service_annotation_prometheus_io_port
              target_label: __address__
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_service_name
              target_label: kubernetes_name
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: kubernetes_io_hostname

        - job_name: 'kubernetes-services'

          metrics_path: /probe
          params:
            module: [http_2xx]

          kubernetes_sd_configs:
            - role: service
              ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
              ## Example:  names: ['test1','test2']
              namespaces:
                names: [ *altiplanoNamespace ]
          metric_relabel_configs:
            - action: labeldrop
              regex: __cpro_dummy_label_to_drop

          relabel_configs:
            - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
              action: keep
              regex: true
            - source_labels: [__address__]
              target_label: __param_target
            - target_label: __address__
              replacement: blackbox
            - source_labels: [__param_target]
              target_label: instance
            - action: labelmap
              regex: __meta_kubernetes_service_label_(.+)
            - source_labels: [__meta_kubernetes_namespace]
              target_label: namespace
            - source_labels: [__meta_kubernetes_service_name]
              target_label: kubernetes_name

  ## Define custom scrape job here for Prometheus.
  ## These jobs will be appended to prometheus.yml
  customScrapeJobs:
    - job_name: is-prometheus
      tls_config:
        insecure_skip_verify: true
      basic_auth:
        username: altiplanoesadmin
        password_file: /etc/secrets/ALTIPLANO_INTERNAL_IS_PASSWORD
      kubernetes_sd_configs:
        - role: endpoints
          ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
          ## Example:  names: ['test1','test2']
          namespaces:
            names: [ *altiplanoNamespace ]
      relabel_configs:
        - action: keep
          regex: opensearch
          source_labels:
            - __meta_kubernetes_pod_label_source
        - action: keep
          regex: true
          source_labels:
            - __meta_kubernetes_service_annotation_prometheus_io_scrape_is
        - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scheme ]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_path ]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [ __address__, __meta_kubernetes_service_annotation_prometheus_io_port ]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [ __meta_kubernetes_namespace ]
          action: replace
          target_label: namespace
        - source_labels: [ __meta_kubernetes_service_name ]
          action: replace
          target_label: kubernetes_service_name
        - source_labels: [ __meta_kubernetes_pod_node_name ]
          target_label: kubernetes_io_hostname
          action: replace
        - action: replace
          source_labels:
            - __meta_kubernetes_pod_name
          target_label: kubernetes_pod_name

    - job_name: kubernetes-service-endpoints
      tls_config:
        insecure_skip_verify: true
      kubernetes_sd_configs:
        - role: endpoints
          ## Provide the namespaces in 'names' parameter to scrape the metrics within the listed namespaces
          ## Example:  names: ['test1','test2']
          namespaces:
            names: [ *altiplanoNamespace ]
      relabel_configs:
        - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scrape ]
          action: keep
          regex: true
        - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scheme ]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_path ]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [ __address__, __meta_kubernetes_service_annotation_prometheus_io_port ]
          action: replace
          target_label: __address__
          regex: (.+)(?::\d+);(\d+)
          replacement: $1:$2
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [ __meta_kubernetes_namespace ]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [ __meta_kubernetes_service_name ]
          action: replace
          target_label: kubernetes_name
        - source_labels: [ __meta_kubernetes_pod_node_name ]
          target_label: kubernetes_io_hostname
          action: replace
        - source_labels: [ __meta_kubernetes_pod_name ]
          action: replace
          target_label: pod
  rbac:
    psp:
      create: false
altiplano-redis:
  #enabled: true
  _disableImageVersionCheck: true
  custom:
    admin:
      accessRoleLabel: internal-access
      annotations:
        kubectl.kubernetes.io/default-container: lcmdb
        kubectl.kubernetes.io/default-logs-container: lcmdb
    server:
      accessRoleLabel: external-access
      annotations:
        kubectl.kubernetes.io/default-container: server
        kubectl.kubernetes.io/default-logs-container: server
  #Disable IPv6 binding for installations that do not have IPv6 localhost ::1 on the pod interfaces, default is false
  #See https://jiradc2.ext.net.nokia.com/browse/CSFS-38763?focusedCommentId=14867554&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14867554
  disableIPv6: true
  ## CRDB-Redisio Parameters
  groupName: "altiplano-redis"
  acl:
    # custom user wtih password for redis client auth
    default:
      enabled: true
      password: "cGFzc3dvcmQ="
      rules: "on ~* &* +@all"
    # define password for system user as Regeneration password not supported on helm upgrade.
    repl-user:
      password: "c3lzdGVtdXNlcnBhc3N3b3Jk"
      # https://redis.io/topics/acl#acl-rules-for-sentinel-and-replicas
      # NOTE: Do not disable
    probe-user:
      password: "c3lzdGVtdXNlcnBhc3N3b3Jk"
    # NOTE: Do not disable unless deploying in cluster mode
    sentinel-user:
      password: "c3lzdGVtdXNlcnBhc3N3b3Jk"
      # https://redis.io/topics/acl#acl-rules-for-sentinel-and-replicas
      # Required by CRDB-redisio
    metrics-user:
      password: "c3lzdGVtdXNlcnBhc3N3b3Jk"
    # NOTE: Do not disable
    crdb-tools-user:
      password: "c3lzdGVtdXNlcnBhc3N3b3Jk"
    # Not required

  services:
    # Indicates the service Type of the redis database
    redis:
      type: NodePort
      port: 6379
      nodePort: 30079
      nodePortReadOnly: 30179
  init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
  metrics:
    image:
      name: "oliver006/redis_exporter"
      tag: v1.57.0
      pullPolicy: IfNotPresent
  server:
    image:
      name: "crdb/crdb-redisio"
      tag: 6.0-4.4453-rocky8
      flavor: custom
    # Number of Redis server instances to run
    accessRoleLabel: external-access
    count: 1
    logLevel: "verbose"
    numDatabases: 16

    # Persistence
    # IMPORTANT: Regardless if the database is being stored on persistent storage,
    #            a persistent volume is still required for retaining configuration.
    #            The size can be extremely small if the database is not being stored
    persistence:
      enabled: *persistence
      accessMode: ReadWriteOnce
      size: 1Gi
      storageClass: *storageClass
      resourcePolicy: delete
      preservePvc: true

    resources:
      requests:
        memory: 512Mi
        cpu: 50m
      limits:
        memory: 1Gi
        cpu: 100m

    nodeSelector: {}
      #role: "Infra"

    # Redis server configuration parameters to include in the server.conf file
    # IMPORTANT: Any parameters set here are subject to being overwritten by dynamic
    #            configuration changes.  This can cause unexpected behavior as
    #            subsequent changes to the following value can trigger update
    #            configuration lifecycle event handling yet not actually apply to
    #            the real, runtime configuration.
    confInclude: |-
      # "cGFzc3dvcmQ=": This value should be updated same with {{ altiplano-redis.common.password }}
      #                 which will automatically re-create the Redis pod to apply the new password.
      #                 Otherwise, you must manually re-create the Redis pod.
      maxclients 20000

  sentinel:
    enabled: false
    image:
      name: "crdb/crdb-redisio"
      tag: 6.0-4.4453-rocky8
      flavor: custom
      pullPolicy: IfNotPresent
    accessRoleLabel: internal-access
  rolemon:
    image:
      name: "crdb/crdb-rolemon"
      tag: 6.0-4.4453-rocky8
      flavor: custom
      pullPolicy: IfNotPresent
    resources:
      requests:
        memory: 64Mi
        cpu: 250m
      limits:
        memory: 256Mi
        cpu: 250m

  admin:
    image:
      name: "crdb/crdb-admin"
      tag: 6.0-4.4453-rocky8
      flavor: custom
      pullPolicy: IfNotPresent
    accessRoleLabel: internal-access
    # Number of Redis server instances to run
    count: 1
    #need this as a WA for https://jiradc2.ext.net.nokia.com/browse/CSFS-38784
    containerSecurityContext:
      readOnlyRootFilesystem: false
    persistence:
      enabled: false
    nodeAffinity:
      enabled: false
    resources:
      requests:
        memory: 100Mi
        cpu: 100m
      limits:
        memory: 200Mi
        cpu: 250m

    hookDeletePolicy: "hook-succeeded"
    #debug: true

    # Post-install hook job essentially waits for all pods/containers to be ready
    # This behavior can be disabled by setting to false
    postInstall:
      enabled: true
    livenessProbe:
      timeoutSeconds: 20
    readinessProbe:
        timeoutSeconds: 20

  ## Backup/Recovery via CBUR
  cbur:
    enabled: false

  timezone:
    ## set to true to mount the /etc/localtime from kubernetes host in container
    mountHostLocaltime: false

altiplano-opentsdb-cluster:
  altiplano-opentsdb:
    #enabled: true
    image:
      registry: #artifactory.net.nokia.com
      #repository: #fnms-opentsdb
      tag: nokia-4.0.2
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    startupProbe:
      initialDelaySeconds: 10
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 15
    livenessProbe:
      initialDelaySeconds:
        k8sversiongt118: 20
        k8sversionlt118: 120
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
    readinessProbe:
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
    fluentd_sidecar:
      image:
        name: "fnms-fluent"
        tag: "nokia-4.1.8"
        pullPolicy: IfNotPresent
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 256Mi
      secrets:
        certSecret: altiplano-opentsdb-certificate-secrets
        passwordSecret: altiplano-secrets
      IS_SCHEME: https
      IS_HOST: altiplano-indexsearch
      IS_PORT: 9200
      LOG_INDEX_PATTERN: logstash
      fluent_conf: |-
        <system>
          log_level error
        </system>
        <source>
          @type tail
          path /logs/opentsdb.log
          read_from_head true
          pos_file /tmp/fluentd/opentsdb-log.pos
          keep_time_key true
          tag otsdb.log
          <parse>
            @type multiline
            format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
            format1 /^(?<date>[^ ]* [^ ]*) (?<level>[^ ]*.) \[(?<thread>[^ ]*)\] - (?<message>[^ ].*)/
          </parse>
        </source>
        <filter otsdb.log>
          @type record_transformer
          enable_ruby true
          <record>
            container_name "#{ENV['CONTAINER_NAME']}"
            container_ip "#{ENV['MY_POD_IP']}"
            date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
          </record>
          renew_record true
          keep_keys container_name,container_ip,date,level,thread,message
        </filter>
        <match otsdb.log>
          @type opensearch
          suppress_type_name true
          reload_on_failure true
          reconnect_on_error true
          logstash_format true
          type_name fluentd
          ssl_verify false
          ssl_version TLSv1_2
          scheme {{ .Values.fluentd_sidecar.IS_SCHEME }}
          host {{ .Values.fluentd_sidecar.IS_HOST }}
          port {{ .Values.fluentd_sidecar.IS_PORT }}
          ca_file "/etc/.certificates/trustchain-cert.pem"
          user "#{ENV['IS_USERNAME']}"
          password "#{ENV['IS_PASSWORD']}"
          <buffer>
            @type file
            path /tmp/fluentd/buffer_otsdb/
            overflow_action drop_oldest_chunk
            chunk_limit_size 16MB
            queued_chunks_limit_size 4096
            flush_thread_count 5
            flush_interval 5s
            retry_wait 0s
            retry_forever true
            total_limit_size 50MB
          </buffer>
          time_key date
          time_key_exclude_timestamp true
          logstash_prefix {{ .Values.fluentd_sidecar.LOG_INDEX_PATTERN }}
          request_timeout 45s
        </match>
        <match **>
          @type null
        </match>
    accessRoleLabel: internal-access
    env:
      zookeeperquorumPort: 2181
      timeZoneEnv: *timeZoneEnv
      secrets:
        IS_USERNAME: fluentd_is_username
        IS_PASSWORD: fluentd_is_password
      init:
        #TTL default value is 604800 seconds => 7Days
        TSDB_TTL: '604800'
        #META_TABLE: 'tsdb-meta'
        #TREE_TABLE: 'tsdb-tree'
        #TSDB_TABLE: 'tsdb'
        #UID_TABLE: 'tsdb-uid'
        #HBASE_IMAGE: fnms-gradiant-hbase
        #HBASE_IMAGE_TAG: nokia-4.0.2
        #ENCODER_PATTERN: "%date{ISO8601} [%logger.%M] %msg%n"
        #LOGGER: "WARN"
      open: {} # remove the braces for the below values to take effect
        #LOG_LEVEL: DEBUG #valid values are ERROR, WARN, INFO, DEBUG, TRACE
        #JVMARGS: "-XX:+UseG1GC --add-opens java.base/java.nio=ALL-UNNAMED --add-opens java.security.jgss/sun.security.krb5=ALL-UNNAMED"
        # Example of job definition:
        # .---------------- minute (0 - 59)
        # |  .------------- hour (0 - 23)
        # |  |  .---------- day of month (1 - 31)
        # |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
        # |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
        # |  |  |  |  |
        # *  *  *  *  *
     #cronjob:
     #  schedule: "0 1 * * *"
    service: {} # remove the braces for the below values to take effect
      #type: NodePort
      #ports:
      #opentsdbNodePort: 30044
    persistence:
      enabled: *persistence
      storageClass: *storageClass
      accessMode: ReadWriteOnce
      # Size of the persistent storage of opentsdb pod
      size: 1Gi
    ingress:
      <<: *ingressconfig
      #path: /altiplano-opentsdb
      #authUrl: http://altiplano-oauth2-proxy.{{ $.Release.Namespace }}.svc.cluster.local:4180/oauth2/auth?allowed_groups=%2Fopentsdbadmin
    nginxIncIngress:
      <<: *nginxIncIngressConfig
    resources:
      requests:
        cpu: 100m
        memory: 2Gi
      limits:
        cpu: 2
        memory: 4Gi
    cbur:
      enabled: *cburEnable
      apiVersion: *cburApiVersion
      #an integer. This value only applies to statefulset. The value can be 0,1 or 2.
      #Recommended value of brOption for BELK is 0.
      brOption: 0
      #the maximum copy you want to saved.
      maxiCopy: *backupRetain
      #Modes supported now: "local","NETBKUP","AVAMAR","CEPHS3","AWSS3", case insensitive
      backendMode: *globalBackend
      #specifies if the backup scheduling cron job should be auto-scheduled
      autoEnableCron: false
      #specifies cron update will be triggered automatically by BrPolicy update
      autoUpdateCron: false
      # allows user to schedule backups
      cronSpec: "0 1 * * *"
      ## should backups be encrypted?
      dataEncryption: false
      persistence:
        cburtmp:
          enabled: *cburEnable
          storageClass: *storageClass
          accessMode: ReadWriteOnce
          dir: /tmp
      cbura:
        imageRepo: cbur/cbur-agent
        imageTag: 1.2.0-alpine-580
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: "1"
            memory: "2Gi"
          requests:
            cpu: "500m"
            memory: "1Gi"
  
  altiplano-opentsdb-av:
    enabled: false
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    startupProbe:
      initialDelaySeconds: 10
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 15
    livenessProbe:
      initialDelaySeconds:
        k8sversiongt118: 20
        k8sversionlt118: 120
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
    readinessProbe:
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
    env:
      zookeeperquorumPort: 2181
      init:
        TSD_UID_LRU_ID_SIZE: "6000000"
        TSD_UID_LRU_NAME_SIZE: "36000000"
        TSDB_TTL: "86400"
      open:
        JVMARGS: "-XX:+UseG1GC -Xms15360m -Xmx15360m -XX:MaxGCPauseMillis=1000 -XX:ActiveProcessorCount=6 -XX:ConcGCThreads=4 --add-opens java.base/java.nio=ALL-UNNAMED --add-opens java.security.jgss/sun.security.krb5=ALL-UNNAMED"
      secrets:
        IS_USERNAME: fluentd_is_username
        IS_PASSWORD: fluentd_is_password
    resources:
      requests:
        cpu: 2
        memory: 10Gi
      limits:
        cpu: 5
        memory: 19Gi
    fluentd_sidecar:
      image:
        name: "fnms-fluent"
        tag: "nokia-4.1.8"
        pullPolicy: IfNotPresent
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 256Mi
      secrets:
        certSecret: altiplano-secrets-trustchain-certs
        passwordSecret: altiplano-secrets
      IS_SCHEME: https
      IS_HOST: altiplano-indexsearch
      IS_PORT: 9200
      LOG_INDEX_PATTERN: logstash
      fluent_conf: |-
        <system>
          log_level error
        </system>
        <source>
          @type tail
          path /logs/opentsdb.log
          read_from_head true
          pos_file /tmp/fluentd/opentsdb-log.pos
          keep_time_key true
          tag otsdb.log
          <parse>
            @type multiline
            format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
            format1 /^(?<date>[^ ]* [^ ]*) (?<level>[^ ]*.) \[(?<thread>[^ ]*)\] - (?<message>[^ ].*)/
          </parse>
        </source>
        <filter otsdb.log>
          @type record_transformer
          enable_ruby true
          <record>
            container_name "#{ENV['CONTAINER_NAME']}"
            container_ip "#{ENV['MY_POD_IP']}"
            date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
          </record>
          renew_record true
          keep_keys container_name,container_ip,date,level,thread,message
        </filter>
        <match otsdb.log>
          @type opensearch
          suppress_type_name true
          reload_on_failure true
          reconnect_on_error true
          logstash_format true
          type_name fluentd
          ssl_verify false
          ssl_version TLSv1_2
          scheme {{ .Values.fluentd_sidecar.IS_SCHEME }}
          host {{ .Values.fluentd_sidecar.IS_HOST }}
          port {{ .Values.fluentd_sidecar.IS_PORT }}
          ca_file "/etc/.certificates/trustchain-cert.pem"
          user "#{ENV['IS_USERNAME']}"
          password "#{ENV['IS_PASSWORD']}"
          <buffer>
            @type file
            path /tmp/fluentd/buffer_otsdb/
            overflow_action drop_oldest_chunk
            chunk_limit_size 16MB
            queued_chunks_limit_size 4096
            flush_thread_count 5
            flush_interval 5s
            retry_wait 0s
            retry_forever true
            total_limit_size 50MB
          </buffer>
          time_key date
          time_key_exclude_timestamp true
          logstash_prefix {{ .Values.fluentd_sidecar.LOG_INDEX_PATTERN }}
          request_timeout 45s
        </match>
        <match **>
          @type null
        </match>
  
  altiplano-hbase:
    #enabled: true
    timeZoneEnv: *timeZoneEnv
    persistence:
      enabled: *persistence
      storageClassName: *storageClass
      accessMode: ReadWriteOnce
      # Size of persistent storage of data pod to store the elasticsearch data.
      size: 1Gi
    image:
      registry: #artifactory.net.nokia.com
      #repository: fnms-gradiant-hbase
      tag: nokia-4.0.2
      pullPolicy: IfNotPresent
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    conf:
      hbaseSite:
        zookeeperquorumPort: "2181"
        hbase_regionserver_thread_compaction_small: "3"
        hbase_zookeeper_session_timeout: "60000"
        hbase_status_published: "true"
        hbase_region_replica_replication_enabled: "true"
        hbase_regionserver_handler_count: "30"
        hbase_ipc_server_callqueue_read_ratio: "0.5"
        hbase_meta_replica_count: "1"
        hbase_regionserver_hostname_disable_master_reversedns: "true"
        hbase_zookeeper_dns_interface: "default"
        hbase_regionserver_dns_interface: "default"
        hbase_master_dns_interface: "default"
        hbase_zookeeper_property_tickTime: "6000"
        hbase_rpc_timeout: "120000"
        hbase_wal_async_wait_on_shutdown_seconds: "60"
      hbasePolicy:
        security.client.protocol.acl: "hdfs hdfs"
        security.admin.protocol.acl: "hdfs hdfs"
        security.masterregion.protocol.acl: "hdfs hdfs"
    prometheus:
      enabled: true
    cbur:
      enabled: *cburEnable
      apiVersion: *cburApiVersion
      #an integer. This value only applies to statefulset. The value can be 0,1 or 2.
      #Recommended value of brOption for BELK is 0.
      brOption: 0
      #the maximum copy you want to saved.
      maxiCopy: *backupRetain
      #Modes supported now: "local","NETBKUP","AVAMAR","CEPHS3","AWSS3", case insensitive
      backendMode: *globalBackend
      #specifies if the backup scheduling cron job should be auto-scheduled
      autoEnableCron: false
      #specifies cron update will be triggered automatically by BrPolicy update
      autoUpdateCron: false
      # allows user to schedule backups
      cronSpec: "0 1 * * *"
      ## should backups be encrypted?
      dataEncryption: false
      persistence:
        cburtmp:
          enabled: *cburEnable
          storageClass: *storageClass
          accessMode: ReadWriteOnce
          dir: /tmp
      cbura:
        imageRepo: cbur/cbur-agent
        imageTag: 1.2.0-alpine-580
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: "1"
            memory: "2Gi"
          requests:
            cpu: "500m"
            memory: "1Gi"
    hbase:
      master:
        accessRoleLabel: internal-access
        env:
          HBASE_HEAPSIZE: 1000M
          HBASE_LOG_LEVEL: INFO
          HBASE_ROOT_LOGGER: "INFO,console"
          HBASE_SECURITY_LOGGER: "INFO,console"
          secrets:
            IS_USERNAME: fluentd_is_username
            IS_PASSWORD: fluentd_is_password
        livenessProbe:
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 3
        readinessProbe:
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 2
          failureThreshold: 3
        fluentd_sidecar:
          image:
            name: "fnms-fluent"
            tag: "nokia-4.1.8"
            pullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 10m
              memory: 256Mi
          secrets:
            certSecret: altiplano-hbase-certificate-secrets
            passwordSecret: altiplano-secrets
          LOG_INDEX_PATTERN: logstash
          IS_HOST: altiplano-indexsearch
          IS_PORT: "9200"
          IS_SCHEME: https

          fluent_conf: |-

            <system>
              log_level error
            </system>

            <source>
              @type tail
              path /logs/*.log
              read_from_head true
              pos_file /tmp/fluentd/masterlog.pos
              keep_time_key true
              tag master.log
              <parse>
                @type multiline
                format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
                format1 /^(?<date>([^ ]* [^ ]*))\s+(?<level>[^ ]*)\s+\[(?<thread>[^ ]*)\]\s(?<category>[^ :]*)(.*?:)\s(?<message>[^ ].*)/
              </parse>
            </source>
            <filter master.log>
              @type record_transformer
              enable_ruby true
              <record>
                container_name "#{ENV['CONTAINER_NAME']}"
                container_ip "#{ENV['POD_IP']}"
                date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
                level ${record["level"]}
                message ${record["message"]}
              </record>
              renew_record true
              keep_keys container_name,container_ip,date,level,thread,category,message
            </filter>
            <match master.log>
              @type opensearch
              reload_on_failure true
              reconnect_on_error true
              logstash_format true
              type_name fluentd
              ssl_verify false
              ssl_version TLSv1_2
              scheme {{ .Values.hbase.master.fluentd_sidecar.IS_SCHEME }}
              host {{ .Values.hbase.master.fluentd_sidecar.IS_HOST }}
              port {{ .Values.hbase.master.fluentd_sidecar.IS_PORT }}
              ca_file "/etc/.certificates/trustchain-cert.pem"
              user "#{ENV['IS_USERNAME']}"
              password "#{ENV['IS_PASSWORD']}"
              <buffer>
                @type file
                path /tmp/fluentd/buffer_master/
                overflow_action drop_oldest_chunk
                chunk_limit_size 16MB
                queued_chunks_limit_size  4096
                flush_thread_count 5
                flush_interval 5s
                retry_wait 0s
                retry_forever true
                total_limit_size 50MB
              </buffer>
              time_key date
              time_key_exclude_timestamp true
              suppress_type_name true
              logstash_prefix {{ .Values.hbase.master.fluentd_sidecar.LOG_INDEX_PATTERN }}
              request_timeout 45s
            </match>
            <match **>
              @type null
            </match>
        nodeSelector: {}
      regionServer:
        accessRoleLabel: internal-access
        env:
          HBASE_HEAPSIZE: 1000M
          #HBASE_LOG_LEVEL: WARN
          #HBASE_ROOT_LOGGER: "WARN,console"
          #HBASE_SECURITY_LOGGER: "WARN,console"
          secrets:
            IS_USERNAME: fluentd_is_username
            IS_PASSWORD: fluentd_is_password
        fluentd_sidecar:
          image:
            name: "fnms-fluent"
            tag: "nokia-4.1.8"
            pullPolicy: IfNotPresent
          resources:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 10m
              memory: 256Mi
          secrets:
            certSecret: altiplano-hbase-certificate-secrets
            passwordSecret: altiplano-secrets
          LOG_INDEX_PATTERN: logstash
          IS_HOST: altiplano-indexsearch
          IS_PORT: "9200"
          IS_SCHEME: https

          fluent_conf: |-

            <system>
              log_level error
            </system>

            <source>
              @type tail
              path /logs/*.log
              read_from_head true
              pos_file /tmp/fluentd/regionserverlog.pos
              keep_time_key true
              tag regsrvr.log
              <parse>
                @type multiline
                format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
                format1 /^(?<date>([^ ]* [^ ]*))\s+(?<level>[^ ]*)\s+\[(?<thread>[^ ]*)\]\s(?<category>[^ :]*)(.*?:)\s(?<message>[^ ].*)/
              </parse>
            </source>
            <filter regsrvr.log>
              @type record_transformer
              enable_ruby true
              <record>
                container_name "#{ENV['CONTAINER_NAME']}"
                container_ip "#{ENV['POD_IP']}"
                date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
                level ${record["level"]}
                message ${record["message"]}
              </record>
              renew_record true
              keep_keys container_name,container_ip,date,level,thread,category,message
            </filter>
            <match regsrvr.log>
              @type opensearch
              reload_on_failure true
              reconnect_on_error true
              logstash_format true
              type_name fluentd
              ssl_verify false
              ssl_version TLSv1_2
              scheme {{ .Values.hbase.regionServer.fluentd_sidecar.IS_SCHEME }}
              host {{ .Values.hbase.regionServer.fluentd_sidecar.IS_HOST }}
              port {{ .Values.hbase.regionServer.fluentd_sidecar.IS_PORT }}
              ca_file "/etc/.certificates/trustchain-cert.pem"
              user "#{ENV['IS_USERNAME']}"
              password "#{ENV['IS_PASSWORD']}"
              <buffer>
                @type file
                path /tmp/fluentd/buffer_regsrvr/
                overflow_action drop_oldest_chunk
                chunk_limit_size 16MB
                queued_chunks_limit_size  4096
                flush_thread_count 5
                flush_interval 5s
                retry_wait 0s
                retry_forever true
                total_limit_size 50MB
              </buffer>
              time_key date
              time_key_exclude_timestamp true
              suppress_type_name true
              logstash_prefix {{ .Values.hbase.regionServer.fluentd_sidecar.LOG_INDEX_PATTERN }}
              request_timeout 45s
            </match>
            <match **>
              @type null
            </match>
        nodeSelector: {}
  altiplano-hdfs:
    image:
      registry: #artifactory.net.nokia.com
      #repository: fnms-gradiant-hdfs
      tag: nokia-4.0.0
      pullPolicy: IfNotPresent
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    zookeeperquorumPort: 2181
    conf:
      coreSite:
        ipc.client.connect.timeout: 90000
        #fs.trash.interval: "10080"  # trash auto purge in minutes
      hdfsSite:
        dfs.replication: 3
        dfs.datanode.max.xcievers: 1024
        dfs.namenode.replication.min: 1
        dfs.namenode.datanode.registration.ip-hostname-check: "false"
        dfs.namenode.avoid.read.stale.datanode: "true"
        dfs.namenode.avoid.write.stale_datanode: "true"
        dfs.namenode.write.stale.datanode.ratio: "1.0f"
        dfs.namenode.check.stale.datanode: "true"
        #dfs.datanode.du.reserved: "4294967296"  # number of bytes to reserve on disk to block hitting disk full, must be quoted for large numbers, because of gotemplate converting large numbers to float with scientific notation
        dfs.datanode.max.transfer.threads: "16000"
        dfs.qjournal.start-segment.timeout.ms: 90000
        dfs.qjournal.select-input-streams.timeout.ms: 90000
        dfs.qjournal.write-txns.timeout.ms: 90000
      hadoopPolicy:
        security.client.protocol.acl: "hdfs hdfs"
        security.client.datanode.protocol.acl: "hdfs hdfs"
        security.datanode.protocol.acl: "hdfs hdfs"
        security.inter.datanode.protocol.acl: "hdfs hdfs"
        security.namenode.protocol.acl: "hdfs hdfs"
        security.admin.operations.protocol.acl: "hdfs hdfs"
        security.refresh.user.mappings.protocol.acl: "hdfs hdfs"
        security.refresh.policy.protocol.acl: "hdfs hdfs"
        security.ha.service.protocol.acl: "hdfs hdfs"
        security.zkfc.protocol.acl: "hdfs hdfs"
        security.qjournal.service.protocol.acl: "hdfs hdfs"
        security.mrhs.client.protocol.acl: "hdfs hdfs"
        security.resourcetracker.protocol.acl: "hdfs hdfs"
        security.resourcemanager-administration.protocol.acl: "hdfs hdfs"
        security.applicationclient.protocol.acl: "hdfs hdfs"
        security.applicationmaster.protocol.acl: "hdfs hdfs"
        security.resourcelocalizer.protocol.acl: "hdfs hdfs"
        security.job.task.protocol.acl: "hdfs hdfs"
        security.job.client.protocol.acl: "hdfs hdfs"
        security.applicationhistory.protocol.acl: "hdfs hdfs"
        security.containermanagement.protocol.acl: "hdfs hdfs"
      kmsAcls:
        hadoop.kms.acl.CREATE: "hdfs hdfs"
        hadoop.kms.acl.DELETE: "hdfs hdfs"
        hadoop.kms.acl.ROLLOVER: "hdfs hdfs"
        hadoop.kms.acl.GET: "hdfs hdfs"
        hadoop.kms.acl.GET_KEYS: "hdfs hdfs"
        hadoop.kms.acl.GET_METADATA: "hdfs hdfs"
        hadoop.kms.acl.SET_KEY_MATERIAL: "hdfs hdfs"
        hadoop.kms.acl.GENERATE_EEK: "hdfs hdfs"
        hadoop.kms.acl.DECRYPT_EEK: "hdfs hdfs"
        default.key.acl.MANAGEMENT: "hdfs hdfs"
        default.key.acl.GENERATE_EEK: "hdfs hdfs"
        default.key.acl.DECRYPT_EEK: "hdfs hdfs"
        default.key.acl.READ: "hdfs hdfs"
    timeZoneEnv: *timeZoneEnv
    journalNode:
      accessRoleLabel: internal-access
      journalnodeQuorumSize: 1
      env:
        secrets:
          IS_USERNAME: fluentd_is_username
          IS_PASSWORD: fluentd_is_password
      fluentd_sidecar:
        image:
          name: "fnms-fluent"
          tag: "nokia-4.1.8"
          pullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 100m
            memory: 512Mi
          requests:
            cpu: 10m
            memory: 256Mi
        secrets:
          certSecret: altiplano-hdfs-certificate-secrets
          passwordSecret: altiplano-secrets
        IS_SCHEME: https
        IS_HOST: altiplano-indexsearch
        IS_PORT: 9200
        LOG_INDEX_PATTERN: logstash
        fluent_conf: |-
          <system>
            log_level error
          </system>
          <source>
            @type tail
            path /logs/*.log
            read_from_head true
            pos_file /tmp/fluentd/journalnode-log.pos
            keep_time_key true
            tag journalnode.log
            <parse>
              @type multiline
              format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
              format1 /^(?<date>([^ ]* [^ ]*))\s+(?<level>[^ ]*)\s(?<category>[^ :]*)(.*?:)\s(?<message>[^ ].*)/
            </parse>
          </source>
          <filter journalnode.log>
            @type record_transformer
            enable_ruby true
            <record>
              container_name "#{ENV['CONTAINER_NAME']}"
              container_ip "#{ENV['POD_IP']}"
              date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
              level ${record["level"]}
              category ${record["category"]}
              message ${record["message"]}
            </record>
            renew_record true
            keep_keys container_name,container_ip,date,level,category,message
          </filter>
          <match journalnode.log>
            @type opensearch
            suppress_type_name true
            reload_on_failure true
            reconnect_on_error true
            logstash_format true
            type_name fluentd
            ssl_verify false
            ssl_version TLSv1_2
            scheme {{ .Values.dataNode.fluentd_sidecar.IS_SCHEME }}
            host {{ .Values.dataNode.fluentd_sidecar.IS_HOST }}
            port {{ .Values.dataNode.fluentd_sidecar.IS_PORT }}
            ca_file "/etc/.certificates/trustchain-cert.pem"
            user "#{ENV['IS_USERNAME']}"
            password "#{ENV['IS_PASSWORD']}"
            <buffer>
              @type file
              path /tmp/fluentd/buffer_regsrvr/
              overflow_action drop_oldest_chunk
              chunk_limit_size 16MB
              queued_chunks_limit_size  4096
              flush_thread_count 5
              flush_interval 5s
              retry_wait 0s
              retry_forever true
              total_limit_size 50MB
            </buffer>
            time_key date
            time_key_exclude_timestamp true
            logstash_prefix {{ .Values.dataNode.fluentd_sidecar.LOG_INDEX_PATTERN }}
            request_timeout 45s
          </match>
      #pdbMinAvailable: 1
      nodeSelector: {}
    nameNode:
      accessRoleLabel: internal-access
      replicas: 1
      env:
        secrets:
          IS_USERNAME: fluentd_is_username
          IS_PASSWORD: fluentd_is_password
      fluentd_sidecar:
        image:
          name: "fnms-fluent"
          tag: "nokia-4.1.8"
          pullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 100m
            memory: 512Mi
          requests:
            cpu: 10m
            memory: 256Mi
        secrets:
          certSecret: altiplano-hdfs-certificate-secrets
          passwordSecret: altiplano-secrets
        IS_SCHEME: https
        IS_HOST: altiplano-indexsearch
        IS_PORT: 9200
        LOG_INDEX_PATTERN: logstash
        fluent_conf: |-
          <system>
            log_level error
          </system>
          <source>
            @type tail
            path /logs/*.log
            read_from_head true
            pos_file /tmp/fluentd/namenode-log.pos
            keep_time_key true
            tag namenode.log
            <parse>
              @type multiline
              format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
              format1 /^(?<date>([^ ]* [^ ]*))\s+(?<level>[^ ]*)\s(?<category>[^ :]*)(.*?:)\s(?<message>[^ ].*)/
            </parse>
          </source>
          <filter namenode.log>
            @type record_transformer
            enable_ruby true
            <record>
              container_name "#{ENV['CONTAINER_NAME']}"
              container_ip "#{ENV['POD_IP']}"
              date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
              level ${record["level"]}
              category ${record["category"]}
              message ${record["message"]}
            </record>
            renew_record true
            keep_keys container_name,container_ip,date,level,category,message
          </filter>
          <match namenode.log>
            @type opensearch
            suppress_type_name true
            reload_on_failure true
            reconnect_on_error true
            logstash_format true
            type_name fluentd
            ssl_verify false
            ssl_version TLSv1_2
            scheme {{ .Values.dataNode.fluentd_sidecar.IS_SCHEME }}
            host {{ .Values.dataNode.fluentd_sidecar.IS_HOST }}
            port {{ .Values.dataNode.fluentd_sidecar.IS_PORT }}
            ca_file "/etc/.certificates/trustchain-cert.pem"
            user "#{ENV['IS_USERNAME']}"
            password "#{ENV['IS_PASSWORD']}"
            <buffer>
              @type file
              path /tmp/fluentd/buffer_regsrvr/
              overflow_action drop_oldest_chunk
              chunk_limit_size 16MB
              queued_chunks_limit_size  4096
              flush_thread_count 5
              flush_interval 5s
              retry_wait 0s
              retry_forever true
              total_limit_size 50MB
            </buffer>
            time_key date
            time_key_exclude_timestamp true
            logstash_prefix {{ .Values.dataNode.fluentd_sidecar.LOG_INDEX_PATTERN }}
            request_timeout 45s
          </match>
      nodeSelector: {}
    dataNode:
      accessRoleLabel: internal-access
      env:
        secrets:
          IS_USERNAME: fluentd_is_username
          IS_PASSWORD: fluentd_is_password
      fluentd_sidecar:
        image:
          name: "fnms-fluent"
          tag: "nokia-4.1.8"
          pullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 100m
            memory: 512Mi
          requests:
            cpu: 10m
            memory: 256Mi
        secrets:
          certSecret: altiplano-hdfs-certificate-secrets
          passwordSecret: altiplano-secrets
        IS_SCHEME: https
        IS_HOST: altiplano-indexsearch
        IS_PORT: 9200
        LOG_INDEX_PATTERN: logstash
        fluent_conf: |-
          <system>
            log_level error
          </system>
          <source>
            @type tail
            path /logs/*.log
            read_from_head true
            pos_file /tmp/fluentd/datanode-log.pos
            keep_time_key true
            tag datanode.log
            <parse>
              @type multiline
              format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
              format1 /^(?<date>([^ ]* [^ ]*))\s+(?<level>[^ ]*)\s(?<category>[^ :]*)(.*?:)\s(?<message>[^ ].*)/
            </parse>
          </source>
          <filter datanode.log>
            @type record_transformer
            enable_ruby true
            <record>
              container_name "#{ENV['CONTAINER_NAME']}"
              container_ip "#{ENV['POD_IP']}"
              date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
              level ${record["level"]}
              category ${record["category"]}
              message ${record["message"]}
            </record>
            renew_record true
            keep_keys container_name,container_ip,date,level,category,message
          </filter>
          <match datanode.log>
            @type opensearch
            suppress_type_name true
            reload_on_failure true
            reconnect_on_error true
            logstash_format true
            type_name fluentd
            ssl_verify false
            ssl_version TLSv1_2
            scheme {{ .Values.dataNode.fluentd_sidecar.IS_SCHEME }}
            host {{ .Values.dataNode.fluentd_sidecar.IS_HOST }}
            port {{ .Values.dataNode.fluentd_sidecar.IS_PORT }}
            ca_file "/etc/.certificates/trustchain-cert.pem"
            user "#{ENV['IS_USERNAME']}"
            password "#{ENV['IS_PASSWORD']}"
            <buffer>
              @type file
              path /tmp/fluentd/buffer_regsrvr/
              overflow_action drop_oldest_chunk
              chunk_limit_size 16MB
              queued_chunks_limit_size  4096
              flush_thread_count 5
              flush_interval 5s
              retry_wait 0s
              retry_forever true
              total_limit_size 50MB
            </buffer>
            time_key date
            time_key_exclude_timestamp true
            logstash_prefix {{ .Values.dataNode.fluentd_sidecar.LOG_INDEX_PATTERN }}
            request_timeout 45s
          </match>
      replicas: 1  # ensure this value is higher or equal to 'conf.hdfsSite.dfs.replication'
      #pdbMinAvailable: 1
      nodeSelector: {}
    persistence:
      dataNode:
        enabled: true
        storageClass: *storageClass
        size: 1Gi
      nameNode:
        enabled: true
        storageClass: *storageClass
        size: 1Gi
      journalNode:
        enabled: true
        storageClass: *storageClass
        accessMode: ReadWriteOnce
        size: 1Gi
    prometheus:
      exporter:
        enabled: true

altiplano-ingress:
  #enabled: true
  custom:
    deployment:
      annotations:
        kubectl.kubernetes.io/default-container:  altiplano-ingress-controller
        kubectl.kubernetes.io/default-logs-container:  altiplano-ingress-controller
  #metrics: false
  accessRoleLabel: external-access
  controller:
    imageRepo: citm/citm-nginx-ingress
    imageTag: 1.24.0-1.4.3-1.1.1
    scope:
      enabled: true
    httpSnippet: |
      more_set_headers "X-Frame-Options: SAMEORIGIN";
      more_set_headers "Cross-Origin-Resource-Policy: same-origin";
      more_set_headers "Strict-Transport-Security: max-age=31536000; includeSubDomains; preload";
      more_set_headers "X-Content-Type-Options: nosniff";
      map $uri $cache_control {
        ~*\/(ap-index\..*|ap-vendor\..*)\.js$                             "public, max-age=2592000, immutable";
        ~*\/(ap-main\..*|ap-vendor\..*)\.css$                             "public, max-age=2592000, immutable";
        ~*\/([^\/]+\.[^\/]+\.(?i)(svg|png|jp(e?)g|gif|ico|woff|woff2))$   "public, max-age=2592000, immutable";
        default                                                           "no-cache, no-store, must-revalidate";
      }
      more_set_headers "Cache-Control: $cache_control";
    podAnnotations:
      nginx.ingress.kubernetes.io/cors-allow-methods: "PUT, GET, POST, DELETE, PATCH"
    podLabels:
      accessRoleLabel: external-access
    #disableIpv4: false
    #disableIpv6: false
    hostNetwork: true
    # When hostNetwork is true, ingress controller will listen on these host ports
    # for HTTP access, defaults to 80
    #httpPort: 32080
    # for HTTPS access, defaults to 443
    #httpsPort: *ingressHttpsPort
    # Name of the ingress class to route through this controller
    ingressClass: nginx
    # DaemonSet or Deployment
    kind: DaemonSet
    # Node tolerations for server scheduling to nodes with taints
    # Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    tolerations:
    - operator: Exists
    # Set this to false if you do not want to run only on edge nodes
    runOnEdge: false
    nodeSelector: {}
    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 1Gi
        cpu: 500m

    service:
      # Service type can be NodePort or ClusterIP
      type: ClusterIP
      #below targetPorts sets the targetPort that maps to the Ingress' port, suggested to be used for Customized Ingress Port scenario along with INGRESS_HTTPSPORT_ENABLED & INGRESS_CONTROLLER_HTTPSPORT
      #targetPorts:
        #http: 32080
        #https: 32443
      # If the type is NodePort, uncomment the next three lines
      #nodePorts:
      #  http: 32080
      #  https: 32443
    config:
      # These settings are migrated from fnms-nginx
      proxy-body-size: 5000m
      proxy-buffer-size: 128K
      proxy-read-timeout: 1200
      proxy-stream-timeout: 3600
      # If the worker connections is not handling all the client connections, tune the below two parameters.
      worker-processes: 2
      #max-worker-connections: 16384
      server-name-hash-bucket-size: 128
      include: /etc/nginx/mime.types
      default-type: application/octet-stream
      sendfile: on
      use-gzip: false
      upstream-keepalive-timeout: 65
      server-tokens: false
      ssl-protocols: 'TLSv1.2 TLSv1.3'
      ssl-ciphers: 'DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM-SHA256:TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256:TLS_AES_128_CCM_SHA256:TLS_AES_128_CCM_8_SHA256'
      ssl-ecdh-curve: "secp521r1:secp384r1"
      use-port-in-redirects: "true"
      disable-access-log: true

      # To disable setting Strict-Transport-Security header, keep hsts=false
      hsts: true
      hsts-preload: true
      # If true, NGINX passes the incoming X-Forwarded-* headers to upstreams
      use-forwarded-headers: true
      use-luabackend-for-ingress: true
      # Set this to false, to allow both http and https access to the ingress resources
      #ssl-redirect: "false"
    # In the following line, replace the namespace and  release name used for deploying altiplano-secrets helm chart to configure external certificates. For example: default/nokiasecrets-altiplano-secrets-server-certs
    defaultSSLCertificate: "{{ .Release.Namespace }}/altiplano-ingress-certificate-secrets"
    defaultSSLCertificateTLS:
      keyNames:
        caCrt: "ingress-server-trustchain-cert.pem"
        tlsKey: "ingress-server-key.pem"
        tlsCrt: "ingress-server-cert.pem"
    # Different value needs to be set when multiple ingress controllers
    #statusPort: *ingressStatusPort
  defaultBackend:
    #If false, controller.defaultBackendService must be provided
    enable: true
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
  rbac:
    podSecurityPolicy:
      enabled: false
  default404:
    backend:
      #activate debug log of default http backend
      debug: false
      page:
        #page title of default http backend
        title: "404 - Not found"
        #page body of default http backend
        body: "The requested page was not found"
        #copyright of default http backend
        copyright: "Nokia. All rights reserved"
        #Product Family Name of default http backend
        productFamilyName: " "
        #Product name of default http backend
        productName: " "
        #Product release of default http backend,
        productRelease: " "
        #toolbar title of default http backend
        toolbarTitle: "View more ..."
        #Image logo of default http backend
        imageBanner: "Nokia_logo_white.svg"
    securityContext:
      runAsUser: 1000
      runAsGroup: 1000
    podLabels:
      accessRoleLabel: internal-access
    test:
      skipTest: false
      imageRepo: "tools"
      # NOTE: when imageRepo is provided as "repo/imageName", test.imageName shouldn't be used
      imageName: "kubectl"
      imageTag: 1.28.7-rocky8-nano-20240301

altiplano-oauth2-proxy:
  #enabled: true
  resources:
    limits:
      cpu: 200m
      memory: 250Mi
    requests:
      cpu: 100m
      memory: 128Mi
  image:
    registry: #artifactory.net.nokia.com
    #repository: #fnms-oauth2-proxy
    tag: nokia-3.0.0
    pullPolicy: IfNotPresent
  init:
    repository: fnms-init-container
    tag: nokia-2.0.1
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      runAsGroup: 1000
  passwordSecret: "altiplano-secrets"
  securityContext:
    runAsNonRoot: true
    runAsUser: 201
    runAsGroup: 114
  #args:
    #oidcGroupsClaim: "group_membership"
  keycloakCheck: "https://altiplano-sso-headless:8443"
  accessRoleLabel: internal-access
  timeoutSeconds: "10"
  livenessProbe:
    initialDelaySeconds: "360"
    periodSeconds: "60"
    failureThreshold: "5"
  readinessProbe:
    initialDelaySeconds: "30"
    periodSeconds: "20"
    failureThreshold: "5"
  service:
    type: ClusterIP
    #ports:
      #oauthproxyNodePort: 32590
      #oauthproxyPort: 4180
  ingress:
    enabled: true
    #path: /oauth2
  nginxIncIngress:
    <<: *nginxIncIngressConfig
  env:
    #OAUTH2_PROXY_PROVIDER: oidc
    #OAUTH2_PROXY_OIDC_ISSUER_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}
    #OAUTH2_PROXY_OIDC_JWKS_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/certs
    #OAUTH2_PROXY_LOGIN_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/auth
    #OAUTH2_PROXY_REDEEM_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/token
    #OAUTH2_PROXY_PROFILE_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/userinfo
    #OAUTH2_PROXY_VALIDATE_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/userinfo
    #OAUTH2_PROXY_ALLOWED_GROUPS: /opentsdbadmin,/ksqldbadmin

    #OAUTH2_PROXY_OIDC_ISSUER_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}
    #OAUTH2_PROXY_OIDC_JWKS_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/certs
    #OAUTH2_PROXY_LOGIN_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/auth
    #OAUTH2_PROXY_REDEEM_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/token
    #OAUTH2_PROXY_PROFILE_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/userinfo
    #OAUTH2_PROXY_VALIDATE_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/userinfo

    #OAUTH2_PROXY_REDIRECT_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/oauth2/callback
    #OAUTH2_PROXY_UPSTREAMS: http://altiplano-opentsdb:4242/api,http://altiplano-ksql-server:8088
    timeZoneEnv: *timeZoneEnv
  hostAliases: []
  #hostAliases:
  #- hostnames:
  #  - "altiplano-ipv6"
  #  ip: "2001:db8:0:3:f816:3eff:fe5c:a179"

altiplano-oauth2-proxy-nbi:
  #enabled: true
  resources:
    limits:
      cpu: 200m
      memory: 250Mi
    requests:
      cpu: 100m
      memory: 128Mi
  image:
    registry: #artifactory.net.nokia.com
    #repository: #fnms-oauth2-proxy
    tag: nokia-3.0.0
    pullPolicy: IfNotPresent
  init:
    repository: fnms-init-container
    tag: nokia-2.0.1
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      runAsGroup: 1000
  passwordSecret: "altiplano-secrets"
  securityContext:
    runAsNonRoot: true
    runAsUser: 201
    runAsGroup: 114
  #args:
    #oidcGroupsClaim: "group_membership"
  keycloakCheck: "https://altiplano-sso-headless:8443"
  timeoutSeconds: "10"
  livenessProbe:
    initialDelaySeconds: "360"
    periodSeconds: "60"
    failureThreshold: "5"
  readinessProbe:
    initialDelaySeconds: "30"
    periodSeconds: "20"
    failureThreshold: "5"
  service:
    type: ClusterIP
    #ports:
      #oauthproxyNodePort: 32590
      #oauthproxyPort: 4180
  ingress:
    enabled: true
    path: /oauth2-nbi
  nginxIncIngress:
    <<: *nginxIncIngressConfig
  env:
    #OAUTH2_PROXY_PROVIDER: oidc
    OAUTH2_PROXY_OIDC_ISSUER_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_NBI_REALM }}
    OAUTH2_PROXY_OIDC_JWKS_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_NBI_REALM }}/protocol/openid-connect/certs
    OAUTH2_PROXY_LOGIN_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_NBI_REALM }}/protocol/openid-connect/auth
    OAUTH2_PROXY_REDEEM_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_NBI_REALM }}/protocol/openid-connect/token
    OAUTH2_PROXY_PROFILE_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_NBI_REALM }}/protocol/openid-connect/userinfo
    OAUTH2_PROXY_VALIDATE_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_NBI_REALM }}/protocol/openid-connect/userinfo

    #OAUTH2_PROXY_OIDC_ISSUER_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}
    #OAUTH2_PROXY_OIDC_JWKS_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/certs
    #OAUTH2_PROXY_LOGIN_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/auth
    #OAUTH2_PROXY_REDEEM_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/token
    #OAUTH2_PROXY_PROFILE_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/userinfo
    #OAUTH2_PROXY_VALIDATE_URL: https://{{ ternary "[" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}{{ .Values.global.EXTERNAL_SERVICE_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.EXTERNAL_SERVICE_IP) }}/altiplano-sso/realms/{{ .Values.global.ALTIPLANO_KEYCLOAK_UI_REALM }}/protocol/openid-connect/userinfo

    #OAUTH2_PROXY_ALLOWED_GROUPS: /opentsdbadmin,/ksqldbadmin
    OAUTH2_PROXY_REDIRECT_URL: https://{{ ternary "[" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}{{ .Values.global.K8S_PUBLIC_IP }}{{ ternary ":" "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary .Values.global.INGRESS_CONTROLLER_HTTPSPORT "" (.Values.global.INGRESS_HTTPSPORT_ENABLED) }}{{ ternary "]" "" (contains ":" .Values.global.K8S_PUBLIC_IP) }}/oauth2/callback
    #OAUTH2_PROXY_UPSTREAMS: http://altiplano-opentsdb:4242/api,http://altiplano-ksql-server:8088
  hostAliases: []
  #hostAliases:
  #- hostnames:
  #  - "altiplano-ipv6"
  #  ip: "2001:db8:0:3:f816:3eff:fe5c:a179"

altiplano-kafka-mirrormaker:
  enabled: false
  security:
    runAsNonRoot: true
    runAsUser: 999
    fsGroup: 998
    runAsGroup: 998
    readOnlyRootFilesystem: true
  kubectlImage: "tools/kubectl"
  kubectlTag: 1.28.7-rocky8-nano-20240301
  accessRoleLabel: external-access
  fullnameOverride: "altiplano-kafka-mirrormaker"
  strategy:
    type: Recreate
#       rollingUpdate:
#         maxSurge: 1
#         maxUnavailable: 0
#       type: RollingUpdate
  image:
    repository: "fnms-kafka-mirrormaker-2"
    tag: nokia-3.0.3
  JmxExporter:
    imageRepo: "cpro/cpro-jmx-exporter"
    imageTag: 4.1.0-rocky8-0.20.0-3612
  init:
    imageRepo: "ckaf/ckaf-kafka-init"
    imageTag: 9.1.0-rocky8-jre17-4.1.0-8675
    pullPolicy: IfNotPresent
    vault:
      vaultInitRepo: fnms-init-container
      vaultInitTag: nokia-2.0.1
      pullPolicy: IfNotPresent
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
  ALTIPLANO_GEO_STANDBY_SITE_IP: ""
  initContainer:
    image:
      name: fnms-init-container
      tag: nokia-2.0.1
      pullPolicy: IfNotPresent
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      runAsGroup: 1000
  certificates:
    secrets:
      altiplano_kafka_certificate_secrets: altiplano-kafka-mirrormaker-certificate-secrets
      altiplano_keystore_secrets: altiplano-keystore-secrets
    fileNames:
      kafka_server_key_pem: kafka-mirrormaker-kafka-client-key.pem
      kafka_server_key_pass: kafka-mirrormaker-kafka-client-key.pass
      kafka_server_cert_pem: kafka-mirrormaker-kafka-client-cert.pem
      altiplano_keystore_password: keystore-password
      kafka_server_keystore_jks: server.jks
      kafka_server_trustchain_cert_pem: kafka-mirrormaker-kafka-client-trustchain-cert.pem
      kafka_server_truststore_jks: trustchain.jks
  env:
    secrets:
      IS_USERNAME: fluentd_is_username
      IS_PASSWORD: fluentd_is_password
  fluentd_sidecar:
    image:
      name: "fnms-fluent"
      tag: "nokia-4.1.8"
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 512Mi
      requests:
        cpu: 10m
        memory: 256Mi
    secrets:
      certSecret: altiplano-kafka-certificate-secrets
      passwordSecret: altiplano-secrets
    securityContext:
      readOnlyRootFilesystem: true
      runAsUser: 1000
      runAsGroup: 996
    IS_SCHEME: https
    IS_HOST: altiplano-indexsearch
    IS_PORT: 9200
    LOG_INDEX_PATTERN: logstash  
    fluent_conf: |-
    
      <system>
        log_level error
      </system>
      
      <source>
        @type tail
        path /logs/connect.log
        read_from_head true
        pos_file /tmp/fluentd/connect-log.pos
        keep_time_key true
        tag mirrormaker.log
        #format json
        format /^(.*?,){2}[^:]*:"(?<level>[^,]*)",(.*?,){2}[^:]*:"(?<date>[^,]*)",[^:]*:"(?<timezone>[^,]*)",[^:]*:[^:]*:"(?<thread>.*)\s-\s(?<category>[^ ]*)\s-\s(?<message>[^}]*)"}}/
      </source>
      <filter mirrormaker.log>
        @type record_transformer
        enable_ruby true
        <record>
          container_name "#{ENV['CONTAINER_NAME']}"
          container_ip "#{ENV['MY_POD_IP']}"
        </record>
        renew_record true
        keep_keys container_name,container_ip,date,level,thread,category,message
        remove_keys timezone
      </filter>
      <match mirrormaker.log>
        @type opensearch
        reload_on_failure true
        reconnect_on_error true
        logstash_format true
        type_name fluentd
        ssl_verify false
        ssl_version TLSv1_2
        scheme {{ .Values.fluentd_sidecar.IS_SCHEME }}
        host {{ .Values.fluentd_sidecar.IS_HOST }}
        port {{ .Values.fluentd_sidecar.IS_PORT }}
        ca_file "/etc/.certificates/kafka-server-trustchain-cert.pem"
        user "#{ENV['IS_USERNAME']}"
        password "#{ENV['IS_PASSWORD']}"
        <buffer>
          @type file
          path /tmp/fluentd/buffer_mirrormaker
          overflow_action drop_oldest_chunk
          chunk_limit_size 16MB
          queued_chunks_limit_size  4096
          flush_thread_count 5
          flush_interval 5s
          retry_wait 0s
          retry_forever true
          total_limit_size 50MB
        </buffer>
        time_key date
        time_key_exclude_timestamp true
        suppress_type_name true
        logstash_prefix {{ .Values.fluentd_sidecar.LOG_INDEX_PATTERN }}
        request_timeout 45s
      </match>
      <match **>
        @type null
      </match>
  mirrorMakerConfig:
    certManager:
      enabled: false
    mmClusters: "edge,central"
    maxTasks: 1
#    krbConfigmapName: des-infra-krb5-config #please enable krbConfigmapName if SASL is enabled
    KrbConfKeyName: krb5config
    clusters:
      - alias: "edge"
        bootstrap.servers: "altiplano-kafka-headless:9092"
        security.protocol: SSL
        config:
          config.storage.replication.factor: 1
          offset.storage.replication.factor: 1
          status.storage.replication.factor: 1
          ssl.endpoint.identification.algorithm: ""
        ssl:
          enabled: *useTls
          enabledProtocols: TLSv1.2,TLSv1.3
          protocol: TLSv1.2
          secret_name: "altiplano-secrets-all-certs"
          keystore_key: server.jks
          truststore_key: trustchain.jks
          truststore_passwd_key: trustchain_jks_pass
          keystore_passwd_key: server_jks_pass
          keystore_key_passwd_key: server_jks_pass
          certificates:
            altiplano_keystore_secrets: altiplano-keystore-secrets
            altiplano_keystore_password: keystore-password
            secrets:
              altiplano_kafka_certificate_secrets: altiplano-kafka-mirrormaker-certificate-secrets
              altiplano_keystore_secrets: altiplano-keystore-secrets
            fileNames:
              kafka_server_key_pem: kafka-mirrormaker-kafka-client-key.pem
              kafka_server_key_pass: kafka-mirrormaker-kafka-client-key.pass
              kafka_server_cert_pem: kafka-mirrormaker-kafka-client-cert.pem
              altiplano_keystore_password: keystore-password
              kafka_server_keystore_jks: server.jks
              kafka_server_trustchain_cert_pem: kafka-mirrormaker-kafka-client-trustchain-cert.pem
              kafka_server_truststore_jks: trustchain.jks
      - alias: "central"
        bootstrap.servers: "" #Use central kafka instance
        security.protocol: SSL #SASL_SSL
        config:
          config.storage.replication.factor: 1
          offset.storage.replication.factor: 1
          status.storage.replication.factor: 1
          ssl.endpoint.identification.algorithm: ""
        ssl:
          enabled: *useTls
          enabledProtocols: TLSv1.2,TLSv1.3
          protocol: TLSv1.2
          secret_name: "altiplano-secrets-all-certs"
          keystore_key: server.jks
          truststore_key: trustchain.jks
          truststore_passwd_key: trustchain_jks_pass
          keystore_passwd_key: server_jks_pass
          keystore_key_passwd_key: server_jks_pass
          certificates:
            altiplano_keystore_secrets: altiplano-keystore-secrets
            altiplano_keystore_password: keystore-password
            secrets:
              altiplano_kafka_certificate_secrets: altiplano-kafka-mirrormaker-certificate-secrets
              altiplano_keystore_secrets: altiplano-keystore-secrets
            fileNames:
              kafka_server_key_pem: kafka-mirrormaker-kafka-client-key.pem
              kafka_server_key_pass: kafka-mirrormaker-kafka-client-key.pass
              kafka_server_cert_pem: kafka-mirrormaker-kafka-client-cert.pem
              altiplano_keystore_password: keystore-password
              kafka_server_keystore_jks: server.jks
              kafka_server_trustchain_cert_pem: kafka-mirrormaker-kafka-client-trustchain-cert.pem
              kafka_server_truststore_jks: trustchain.jks
        saslKrb:
          enabled: false #please correct to your secret if enable SASL
          krbSecretName: *saslSecret
          krbPrincipalKey: *krbPrincipalKey
          krbKeytabKey: *krbKeytabKey
    mirrors:
      - sourceCluster: "edge"
        targetCluster: "central"
        sourceConnector:
          config:
            replication.factor: 1
            offset-syncs.topic.replication.factor: 1
            sync.topic.acls.enabled: "false"
            sync.topic.configs.enabled: "true"
            replication.policy.class: "com.nokia.csf.kafka.mm2.RetainTopicNameReplicationPolicy"
#            replication.policy.class: "org.apache.kafka.connect.mirror.DefaultReplicationPolicy"
#            replication.policy.class: "com.nokia.csf.kafka.mm2.IdentityMapperTopicReplicationPolicy" #This class is used for topic rename. If enable this line, please comment out above one
#            topic.replication.maps: "" #e.g: source_topic_1:target_topic_1,source_topic_2:target_topic_2
        heartbeatConnector:
          config:
            emit.hearbeats.enabled: "true"
            heartbeats.topic.replication.factor: 1
        checkpointConnector:
          config:
            emit.checkpoints.enabled: "true"
            checkpoints.topic.replication.factor: 1
        refresh.groups.interval.seconds: "300"
        refresh.topics.interval.seconds: "300"
        sync.topic.configs.interval.seconds: "300"
        topics: ".*_ALARM$,.*_INTERNAL_Intent_Changes$,ALTIPLANO_INTERNAL_HAM_UPDATE$"
        groups: ".*"
        groups.blacklist: "console-consumer-.*, connect-.*, __.*"
    nodeSelector: {}
  resources:
    limits:
      cpu: 1
      ephemeral-storage: 1G
      memory: 4Gi
    requests:
      cpu: 1
      ephemeral-storage: 1G
      memory: 1Gi

altiplano-cbur:
  #enabled: *cburEnable
  global:
    # If flatRegistry is set to true, the repository path in all container images will be skipped
    flatRegistry: true
    timeZoneEnv: *timeZoneEnv
  image:
    master:
      name: "fnms-cburm"
      tag: "nokia-1.1.4"
      pullPolicy: IfNotPresent
    agent:
      name: cbur/cbur-agent
      tag: "1.2.0-alpine-580"
    croncli:
      name: cbur/cbur-cli
      tag: "1.2.0-alpine-580"
    redis:
      name: crdb/crdb-redisio
      tag: "6.1-1.4604-rocky8"
    init:
      name: fnms-init-container
      tag: nokia-2.0.1
      pullPolicy: IfNotPresent
  fullnameOverride:
  nameOverride: altiplano-cbur
  accessRoleLabel: internal-access
  #FNMS-39240 - TD / Gaps - 3rd party containers' logging in kubernetes
  env:
    secrets:
      IS_USERNAME: fluentd_is_username
      IS_PASSWORD: fluentd_is_password
  cbur_fluentd_sidecar:
    image:
      name: "fnms-fluent"
      tag: "nokia-4.1.8"
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 512Mi
      requests:
        cpu: 10m
        memory: 256Mi
    secrets:
      certSecret: altiplano-cbur-certificate-secrets
      passwordSecret: altiplano-secrets
    LOG_INDEX_PATTERN: logstash
    IS_PORT: 9200
    IS_HOST: altiplano-indexsearch
    IS_SCHEME: https
    CERT: /etc/altiplano/certificates/secrets/cbur-indexsearch-client-trustchain-cert.pem
    fluent_conf: |-

      <system>
        log_level error
      </system>
      <source>
        @type tail
        path /logs/br.log
        read_from_head true
        pos_file /tmp/fluentd/br-log.pos
        keep_time_key true
        tag cbur.log
        <parse>
          @type multiline
          format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
          format1 /^(?<date>([^ ]* [^ ]*)) (?<thread>[^ ]*)(:) (?<category>[^ ]*) (?<level>[^ ]*) ([^ ] *) (?<message>[^ ].*)/
        </parse>
      </source>
      <filter cbur.log>
        @type grep
        <exclude>
          key level 
          pattern /DEBUG/
        </exclude>
      </filter>
      <filter cbur.log>
        @type record_transformer
        enable_ruby true
        <record>
          content_type "log"
        </record>   
      </filter>
      <filter *.log>
        @type record_transformer
        enable_ruby true
        <record>
          date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
          container_name "#{ENV['CONTAINER_NAME']}"
          container_ip "#{ENV['MY_POD_IP']}"
          level ${record['level'] == "WARNING" || record['level'] == "Warning" || record['level'] == "ALERT" ? "WARN" : record['level'] == "CRITICAL" ? "ERROR" : record['level']}
        </record>
        renew_record true
        keep_keys date,level,thread,category,message,content_type
        remove_keys chunk_id,next_retry_seconds,retry_time,chunk,error
      </filter>   
      <match *.log>
        @type opensearch
        reload_on_failure true
        reconnect_on_error true
        logstash_format true
        type_name fluentd
        ssl_verify false
        ssl_version TLSv1_2
        scheme {{ .Values.cbur_fluentd_sidecar.IS_SCHEME }}
        host {{ .Values.cbur_fluentd_sidecar.IS_HOST }}
        port {{ .Values.cbur_fluentd_sidecar.IS_PORT }}
        ca_file {{ .Values.cbur_fluentd_sidecar.CERT }}
        user "#{ENV['IS_USERNAME']}"
        password "#{ENV['IS_PASSWORD']}"
        <buffer>
          @type file
          path /tmp/fluentd/buffer_es/
          overflow_action drop_oldest_chunk
          chunk_limit_size 16MB
          queued_chunks_limit_size  4096
          flush_thread_count 5
          flush_interval 5s
          retry_wait 0s
          retry_forever true
          total_limit_size 50MB
        </buffer>
        time_key date
        time_key_exclude_timestamp true
        logstash_prefix {{ .Values.cbur_fluentd_sidecar.LOG_INDEX_PATTERN }}
        request_timeout 45s
        suppress_type_name true
      </match>
      <match **>
        @type null
      </match>
  
  # cburScope valid values are "Cluster|Namespaced", default is "Cluster"
  # "Cluster" means CBUR's functions are open to all namespaces in a cluster
  # "Namespaced" means CBUR will be installed per namespace and its
  # functions will be limited in its namespace, such as k8swatcher/role/backup/restore, etc
  cburScope: "Namespaced"

  auth:
    enabled: true
    # useNodePort is removed in cbur 1.13.0 and replaced by service.type
    # useNodePort: false

  ingress:
    enabled: true

  rbac:
    enabled: true
    # If rbac.enabled=false, CBUR will use the configured "serviceAccountName".
    serviceAccountName: ""
    # If rbac.enabled=true, CBUR will create ServiceAccount, (Cluster)Role, (Cluster)RoleBinding as the following:
    # 1) CBUR always binds to the (Cluster)Role defined in basic_role.yaml.
    # 2) When "accessAllResources: true", CBUR will also add access ("get", "list", "create", "update", "patch") to all Kubenetes resources.
    accessAllResources: false
    # 3) When "accessAllResources: false" and "adminRole: true", CBUR will also bind to ClusterRole "admin".
    adminRole: false

  securityContext:
    enabled: true
    readOnlyRootFilesystem: true
    # If k8swatcher.clusterBrEnabled or volumeType.glusterfs set to true, then securityContext.runAsUser and securityContext.runAsGroup should be set to "auto" or 0
    runAsUser: "1000"
    fsGroup: "1000"
    runAsGroup: "1000"
    runAsNonRoot: true

  nginx:
    # CSFLCM-8653: CSFID-3946 Add configurable item in the helm chart for nginx listen IP protocol
    listenIPv4Only: false
    listenIPv6Only: false

  nodeSelectorOverride:
    "kubernetes.io/os": "linux"
  nodeSelector:
    is_control: 'true'

  volumeType:
    glusterfs: false

  storageBackup: "2Gi"
  storageRepo: "8Gi"
  storageClass: *storageClass
  keepPvc: true
  # in case cbur-celery pod fails in "CreateContainerConfigError" state, with Message "Error: stat /cluster01/cbur-repo: no such file or directory" in Rancher environment set "useSubPath" parameter to false
  # Default value of useSubPath: true
  useSubPath: true

  storageMonitor:
    enabled: true
    # Check disk usage before application level backup/restore to SFTP
    # Fire warning alarm 'VOLUME_EXCEED_THRESHOLD' if the disk usage of '/BACKUP' reaches this threshold, but does not reach the storageMonitor.backup_high_threshold
    backup_low_threshold: "75%"
    # Check disk usage before application level backup/restore to local.
    # Fire warning alarm 'VOLUME_EXCEED_THRESHOLD' if the disk usage of '/CBUR_REPO' reaches this threshold, but does not reach the storageMonitor.repo_high_threshold
    repo_low_threshold: "75%"
    # Check disk usage before application level backup/restore to SFTP.
    # Fail backup/restore and fire critical alarm 'VOLUME_EXCEED_THRESHOLD' if the disk usage of '/BACKUP' reaches this threshold.
    backup_high_threshold: "85%"
    # Check disk usage before application level backup/restore to local.
    # Fail backup/restore and fire critical alarm 'VOLUME_EXCEED_THRESHOLD' if the disk usage of '/CBUR_REPO' reaches this threshold.
    repo_high_threshold: "85%"

  dataCompression:
    #The name of program to be used for data compression ("gzip" or "pigz").
    method: pigz
    #The maximum number of compression threads; this option is valid only for "pigz" method (default 6)
    pigzThreads:

  cburaVolume:
    #If true, create a PVC with the default StorageClass and mount it to "/tmp" of cbura auto-injected sidecar. It will be used to save temporary data during application backup/restore.
    #If false, the temporary data will consume the node storage on which the pod runs.
    #Please note, the "cburaVolume" setting in BrPolicy has the priority. i.e.
    #cburaVolume in BrPolicy > cburaVolume in CBUR helm values > use node storage
    enabled: true
    #"/tmp" volume size when cburaVolume.enabled is true
    #size should be double the size of component data (i.e. altiplano-mariadb.mariadb.persistence.size)
    storage: "10Gi"

  SSH:
    mode: "sftp"
    #the username to connect to the remote server
    username: "xxxxxx"
    # the IP address or FQDN of the remote server
    host: "xxx.xxx.xxx.xxx"
    port: 22
    # the target directory on the remote server (i.e. the directory containing the backup data)
    path: "xxxx"
    strictHostKeyChecking: !!str "no"   # only supports "yes", "no", "autoadd"
    hostKey: ""

  # clusterBrEnabled can only be set to true when cburScope is "Cluster"
  # For openshift, please set clusterBrEnabled to false
  # since cluster BR is not supported in openshift
  k8swatcher:
    enabled: true
    clusterBrEnabled: false
    resources:
      limits:
        # Per HBP 3.4.0, the default value for cpu limit should not be set
        # Please set it accordingly it if you need it.
        # cpu: 500m
        ephemeral-storage: 1Gi
        memory: 500Mi
      requests:
        cpu: 200m
        ephemeral-storage: 512Mi
        memory: 80Mi
        
  #FNMS-39240 - TD / Gaps - 3rd party containers' logging in kubernetes
  k8swatcher_fluentd_sidecar:
    image:
      name: "fnms-fluent"
      tag: "nokia-4.1.8"
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 512Mi
      requests:
        cpu: 10m
        memory: 256Mi
    secrets:
      certSecret: altiplano-cbur-certificate-secrets
      passwordSecret: altiplano-secrets
    LOG_INDEX_PATTERN: logstash
    IS_PORT: 9200
    IS_HOST: altiplano-indexsearch
    IS_SCHEME: https
    CERT: /etc/altiplano/certificates/secrets/cbur-indexsearch-client-trustchain-cert.pem
    fluent_conf: |-

      <system>
        log_level error
      </system>
      <source>
        @type tail
        path /logs/br.log
        read_from_head true
        pos_file /tmp/fluentd/br-log.pos
        keep_time_key true
        tag cbur.log
        <parse>
          @type multiline
          format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
          format1 /^(?<date>([^ ]* [^ ]*)) (?<thread>[^ ]*)(:) (?<category>[^ ]*) (?<level>[^ ]*) ([^ ] *) (?<message>[^ ].*)/
        </parse>
      </source>
      <filter cbur.log>
        @type grep
        <exclude>
          key level 
          pattern /DEBUG/
        </exclude>
      </filter>
      <filter cbur.log>
        @type record_transformer
        enable_ruby true
        <record>
          content_type "log"
        </record>   
      </filter>
      <filter *.log>
        @type record_transformer
        enable_ruby true
        <record>
          date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
          container_name "#{ENV['CONTAINER_NAME']}"
          container_ip "#{ENV['MY_POD_IP']}"
          level ${record['level'] == "WARNING" || record['level'] == "Warning" || record['level'] == "ALERT" ? "WARN" : record['level'] == "CRITICAL" ? "ERROR" : record['level']}
        </record>
        renew_record true
        keep_keys date,level,thread,category,message,content_type
        remove_keys chunk_id,next_retry_seconds,retry_time,chunk,error
      </filter>
     
      <match *.log>
        @type opensearch
        reload_on_failure true
        reconnect_on_error true
        logstash_format true
        type_name fluentd
        ssl_verify false
        ssl_version TLSv1_2
        scheme {{ .Values.k8swatcher_fluentd_sidecar.IS_SCHEME }}
        host {{ .Values.k8swatcher_fluentd_sidecar.IS_HOST }}
        port {{ .Values.k8swatcher_fluentd_sidecar.IS_PORT }}
        ca_file {{ .Values.k8swatcher_fluentd_sidecar.CERT }}
        user "#{ENV['IS_USERNAME']}"
        password "#{ENV['IS_PASSWORD']}"
        <buffer>
          @type file
          path /tmp/fluentd/buffer_es/
          overflow_action drop_oldest_chunk
          chunk_limit_size 16MB
          queued_chunks_limit_size  4096
          flush_thread_count 5
          flush_interval 5s
          retry_wait 0s
          retry_forever true
          total_limit_size 50MB
        </buffer>
        time_key date
        time_key_exclude_timestamp true
        logstash_prefix {{ .Values.k8swatcher_fluentd_sidecar.LOG_INDEX_PATTERN }}
        request_timeout 45s
        suppress_type_name true
      </match>
      <match **>
        @type null
      </match>

  logging:
    file_logging_level: "DEBUG"
    console_logging_level: "WARNING"

  integrityCheck:
    local_enable: true

  direct_redis:
    enabled: true
    # If enabled, celery will use localhost to connect to redis instead of k8s service endpoint
    useLocalhost: true
    redis_port: 6379
    resources:
      limits:
        # Per HBP 3.4.0, the default value for cpu limit should not be set
        # Please set it accordingly it if you need it.
        # cpu: 500m
        memory: 500Mi
        ephemeral-storage: 512Mi
      requests:
        cpu: 50m
        memory: 50Mi
        ephemeral-storage: 512Mi
  celery:
    workerConcurrency: 10
    resources:
      limits:
        # Per HBP 3.4.0, the default value for cpu limit should not be set
        # Please set it accordingly it if you need it.
        # cpu: 10
        memory: 3Gi
        ephemeral-storage: 512Mi
      requests:
        cpu: 1
        memory: 2Gi
        ephemeral-storage: 512Mi

  #FNMS-39240 - TD / Gaps - 3rd party containers' logging in kubernetes
  mycelery_fluentd_sidecar:
    image:
      name: "fnms-fluent"
      tag: "nokia-4.1.8"
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 512Mi
      requests:
        cpu: 10m
        memory: 256Mi
    secrets:
      certSecret: altiplano-cbur-certificate-secrets
      passwordSecret: altiplano-secrets
    KAFKA_BOOTSTRAP_SERVERS: altiplano-kafka-headless:9092
    KAFKA_TOPIC: ALTIPLANO_INTERNAL_cbur-alarm
    LOG_INDEX_PATTERN: logstash
    IS_PORT: 9200
    IS_HOST: altiplano-indexsearch
    IS_SCHEME: https
    CERT: /etc/altiplano/certificates/secrets/cbur-indexsearch-client-trustchain-cert.pem
    fluent_conf: |-

      <system>
        log_level error
      </system>
      <source>
        @type tail
        path /CBUR_REPO/log/BR.log
        read_from_head true
        pos_file /tmp/fluentd/BR-log.pos
        keep_time_key true
        tag cbur.log
        <parse>
          @type multiline
          format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
          format1 /^(?<date>([^ ]* [^ ]*)) (?<thread>[^ ]*)(:) (?<category>[^ ]*) (?<level>[^ ]*) ([^ ] *) (?<cburalarm>[^ ].*)/
        </parse>
      </source>
      <filter cbur.log>
        @type grep
        <exclude>
          key level 
          pattern /DEBUG/
        </exclude>
      </filter>
      <match cbur.log>
        @type copy
        <store>
          @type relabel
          @label @alarm.log
        </store>
        <store>
          @type relabel
          @label @all.br.log
        </store>
      </match>
      
      <label @alarm.log>
        <match cbur.log>
          @type rewrite_tag_filter
          <rule>
            key "category"
            pattern /alarm_unified_logging/
            tag "alarm.unified.log"
          </rule> 
        </match>
        <filter alarm.unified.log>
          @type record_transformer
          renew_record true
          keep_keys cburalarm
        </filter>

        <match alarm.unified.log>
          @type copy
          <store>
            @type stdout
          </store>
          <store>
            @type kafka2
            <format>
              @type json
            </format>
            <buffer topic>
              @type file
              path /tmp/fluentd/kafka-buffer/alarm.unified.log.alarm
              overflow_action drop_oldest_chunk
              chunk_limit_size 16MB
              queued_chunks_limit_size  4096
              flush_thread_count 1
              flush_interval 5s
              retry_wait 0s
              retry_forever true
              total_limit_size 50MB
            </buffer>
            ssl_ca_cert "/etc/altiplano/certificates/secrets/cbur-kafka-client-trustchain-cert.pem"
            ssl_client_cert "/etc/altiplano/certificates/secrets/cbur-kafka-client-cert.pem"
            ssl_client_cert_key "/etc/altiplano/certificates/secrets/cbur-kafka-client-key.pem"
            ssl_client_cert_key_password "/etc/.certificates/client_key_pass"
            ssl_verify_hostname false
            get_kafka_client_log true
            max_send_retries 10
            brokers {{ .Values.mycelery_fluentd_sidecar.KAFKA_BOOTSTRAP_SERVERS }}
            default_topic {{ .Values.mycelery_fluentd_sidecar.KAFKA_TOPIC }}
          </store>
        </match>
      </label>
    
      <label @all.br.log>
        
        <filter cbur.log>
          @type record_transformer
          enable_ruby true
          <record>
            message ${record['cburalarm']}
            content_type "log"
          </record>   
        </filter>
        <filter *.log>
          @type record_transformer
          enable_ruby true
          <record>
            date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
            container_name "#{ENV['CONTAINER_NAME']}"
            container_ip "#{ENV['MY_POD_IP']}"
            level ${record['level'] == "WARNING" || record['level'] == "Warning" || record['level'] == "ALERT" ? "WARN" : record['level'] == "CRITICAL" ? "ERROR" : record['level']}
          </record>
          renew_record true
          keep_keys date,level,thread,category,message,content_type
          remove_keys chunk_id,next_retry_seconds,retry_time,chunk,error,cburalarm
        </filter>
        <match *.log>
          @type opensearch
          reload_on_failure true
          reconnect_on_error true
          logstash_format true
          type_name fluentd
          ssl_verify false
          ssl_version TLSv1_2
          scheme {{ .Values.mycelery_fluentd_sidecar.IS_SCHEME }}
          host {{ .Values.mycelery_fluentd_sidecar.IS_HOST }}
          port {{ .Values.mycelery_fluentd_sidecar.IS_PORT }}
          ca_file {{ .Values.mycelery_fluentd_sidecar.CERT }}
          user "#{ENV['IS_USERNAME']}"
          password "#{ENV['IS_PASSWORD']}"
          <buffer>
            @type file
            path /tmp/fluentd/buffer_es/
            overflow_action drop_oldest_chunk
            chunk_limit_size 16MB
            queued_chunks_limit_size  4096
            flush_thread_count 5
            flush_interval 5s
            retry_wait 0s
            retry_forever true
            total_limit_size 50MB
          </buffer>
          time_key date
          time_key_exclude_timestamp true
          logstash_prefix {{ .Values.mycelery_fluentd_sidecar.LOG_INDEX_PATTERN }}
          request_timeout 45s
          suppress_type_name true
        </match>
      </label>
      <match **>
        @type null
      </match>

  # cluster name and cluster id
  # Make sure they are strings by --set-string or -f if pure numbers
  # Or else they will be parsed into integers
  # For openshift, clusterName and clusterId settings are mandatory
  clusterName: "nokia-infra"
  clusterId: "1"
  clusterDomain: ""
  onlyRestore: false
  enableBrSchemaValidation: true
  startupCheck:
    enabled: false
  service:
    # Supported: ClusterIP, NodePort, LoadBalancer
    type: ClusterIP

  # The method used to copy (backup) data from cbura-sidecar to cburm.
  #   cp: "kubectl cp" will be used
  #   cat: "kubectl exec <pod> -c <container> -n <namespace> -- sh -c 'cat <src_tarball>' > <dest_tarball>" will be used
  # This doesnt apply when copying (restore) data from cburm to cbura-sidecar, which will always use "kubectl cp".
  copyFromCburaMethod: cat

  # The "--retries" in "kubectl cp". It's only available when the Kubernetes version >= v1.23.
  # If not defined, will not add "--retries" in "kubectl cp" command.
  # This applies for both copy to / from cbura-sidecar.
  cpRetries:

altiplano-backup-restore-helper:
  #enabled: true
  global:
    registry: artifactory.net.nokia.com
    persistence: false

    updateStrategy:
      type: Recreate

  accessRoleLabel: internal-access
  replicaCount: 1

  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 1000

  image:
    #This value will override the value set by global.registry
    #Set this to "-" if we need to just remove the value set by the global.registry
    #registry: #artifactory.net.nokia.com
    repository: fnms-backup-restore-helper
    tag: nokia-1.2.17
    pullPolicy: IfNotPresent

  rbac:
    enabled: true
    serviceAccountName:

  tls:
    overrideGlobal: false
    enabled: false

  persistence:
    ## If true, use a Persistent Volume Claim, If false, use emptyDir
    ## NOTE if enabled, pvc named "cliadapteribn-data-pvclaim" is required
    enabled: false
    ## Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: *storageClass
    ## Persistent Volume Claim annotations
    ##
    annotations:
    ## Persistent Volume Access Mode
    ##
    accessModes:
      - ReadWriteOnce
    ## Persistent Volume size
    ##
    size: 1Gi
    ##

  use_tls:
    enabled: false

  stdin: true
  tty: true

  env:

  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits:
      cpu: 800m
      memory: 300Mi
    requests:
      cpu: 100m
      memory: 80Mi

  nodeSelector: { }

  tolerations: [ ]

  affinity: { }

altiplano-nspos-rabbitmq:
  #enabled: true
  replicas: 1
  tmpForceRecreateResources: true
  serviceType: NodePort
  rabbitmq:
    nodePort: 30010
    plugins: |-
      [rabbitmq_jms_topic_exchange,rabbitmq_management,rabbitmq_peer_discovery_k8s].
    tls:
      enabled: true
      nodePort: 30011
    clustering:
      ## The address_type can only be set to default or hostname.
      address_type: default
      k8s_domain: cluster.local
  persistence:
    enabled: *persistence
    reservePvc: *persistence
    resourcePolicy: keep
    data:
      storageClass: *storageClass
      size: 8Gi
    log:
      storageClass: *storageClass
      size: 8Gi
  ingress:
    <<: *ingressconfig
    hostName: ""
  ipv6Enabled: false
  image:
    registry: artifactory.net.nokia.com
    repository: "nokia-nsp-docker-virtual/crmq/rabbitmq/rocky8"
    tag: 2.3.2-3.11.18-21
    pullPolicy: IfNotPresent

  init:
    repository: fnms-init-container
    tag: nokia-2.0.1

  kubectlImage:
    repository: "fnms-kubectl"
    tag: nokia-1.2.0

  env:
    LOG_LEVEL: "INFO"

  resources:
    requests:
      cpu: 0.5
      memory: 230Mi
    limits:
      cpu: 1
      memory: 1Gi

altiplano-sso:
  #enabled: true
  fullnameOverride: "altiplano-sso"
  enableDefaultCpuLimits: true
  replicaCount: 1
  podManagementPolicy: "OrderedReady"
  isuUpgrade:
    enabled: false
  nodeAffinity:
    enabled: false
    key: is_worker
    value: true
 
  rollme:
    enabled: false
  #hostAliases:
  #  - hostnames:
  #      - "altiplano-ipv6"
  #    ip: "2001:db8:0:3:f816:3eff:fe5c:a179"
  accessRoleLabel: external-access
  automountServiceAccountToken: true
  # Node Antiaffinity rules
  # If enabled set preferred antiaffinity to deploy pods across zones and hostnames
  nodeAntiAffinity:
    enabled: false
  podAntiAffinity:
  zone:
  #Possible options: soft/hard/none
    type: none
    topologyKey: "topology.kubernetes.io/zone"
  node:
  #Possible options: soft/hard/none
    type: none
    topologyKey: "kubernetes.io/hostname"

  # Toleration rules
  # tolerations:
  #   - key: 'is_control'
  #     operator: 'Equal'
  #     value: 'true'
  #     effect: 'NoExecute'
  #   - key: 'is_edge'
  #     operator: 'Equal'
  #     value: 'true'
  #     effect: 'NoExecute'
  #   - key: 'is_storage'
  #     operator: 'Equal'
  #     value: 'true'
  #     effect: 'NoExecute'

  images:
    pullPolicy: IfNotPresent
    keycloak:
      imageName: fnms-keycloak
      imageTag: nokia-6.1.18
    kubectl:
      imageName: tools/kubectl
      imageTag: 1.28.7-rocky8-nano-20240301
  init:
    image:
      name: fnms-init-container
      tag: nokia-2.0.1
      pullPolicy: IfNotPresent
  initBusyBoxContainer:
    resources:
      limits:
        memory: 256Mi
        cpu: 500m
        ephemeral-storage: 1Gi
  cmdb:
    enabled: false
  cbur:
    enabled: false
  #hostAliases:
  #  - hostnames:
  #      - "altiplano-ipv6"
  #    ip: "2001:db8:0:3:f816:3eff:fe5c:a179"
  nodeSelector: {}
  #ipFamilyPolicy:  SingleStack | PreferDualStack | RequireDualStack
  ipFamilyPolicy:
  # ipFamilies: ["IPv4"] | ["IPv6"] | ["IPv4","IPv6"] | ["IPv6","IPv4"]
  ipFamilies: []
  unifiedLogging:
    kcLog: "file"
    logLevel: "DEBUG"
  secretCredentials:
    ckeySecret:
    dbSecret: altiplano-sso
    keystoreSecret:
    truststoreSecret: altiplano-keystore-secrets
    dbKeystoreSecret: altiplano-keystore-secrets
    zeroTrustMetricSecret:
  keycloakUser: admin
  keycloakPassword: admin
  dbName: keycloak
  dbUser: altiplano
  dbVendor: mariadb
  jksPassword: keystore-password
  base64DBPassword: bm9raWFmbm1zQDEyMw==
  jdbcParams: ?autoReconnect=true&sslMode=verify-full&trustServerCertificate=true&enabledSslProtocolSuites=TLSv1.3,TLSv1.2&trustStore=/tmp/opt/keycloak/certs/client_keystore.jks&trustStorePassword=${JKS_PASSWORD}
  dbAddress: altiplano-mariadb
  dbPort: 3306
  masterRealmConfigurationJob:
    enabled: false
  httpRelativePath: /altiplano-sso
  customPreStartScript: |
    source /opt/keycloak/app/altiplanoConfiguration.sh &
    echo "altiplanoConfiguration.sh script called successfully"
# REDIRECT_URIS: "https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-kibana/auth/openid/login,https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-kibana,https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-alarms-indexsearch-dashboards/auth/openid/login,https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-alarms-indexsearch-dashboards,https://{{ .Values.global.K8S_PUBLIC_IP }}:30030,https://{{ .Values.global.K8S_PUBLIC_IP }}/altiplano-grafana,https://{{ .Values.global.K8S_PUBLIC_IP }}:30647,https://{{ .Values.global.K8S_PUBLIC_IP }}/oauth2/callback"
  probeDelays:
    readinessProbeInitialDelay: 30
    readinessProbeTimeoutSeconds: 5
    readinessProbePeriodSeconds: 15
    livenessProbeInitialDelay: 600
    livenessProbeTimeoutSeconds: 5
    livenessProbePeriodSeconds: 15
    livenessProbeFailureThreshold: 5
  resources:
    requests:
      cpu: 100m
      memory: 1Gi
    limits:
      cpu: 2
      memory: 1500Mi
  customJavaOpts: "-Dkeycloak.profile.feature.upload_scripts=enabled -Xmx512m -Xms256m -Djdk.tls.ephemeralDHKeySize=2048 -Dlog4j2.formatMsgNoLookups=true -Dcom.sun.jndi.ldap.object.disableEndpointIdentification=true"
  radiusImage:
    registry: #artifactory.net.nokia.com
    repository: fnms-keycloak-radius-plugin
    tag: nokia-7.0.2
    pullPolicy: IfNotPresent
  env:
    secrets:
      ALTIPLANO_SYSTEM_USER: sysuser
      ALTIPLANO_SYSTEM_PASSWORD: password
      OPENTSDB_USER: opentsdbuser
      OPENTSDB_PASSWORD: password
      KSQLDB_USER: ksqldbuser
      KSQLDB_PASSWORD: password
      ALTIPLANO_USER: adminuser
      ALTIPLANO_PASSWORD: password
      IS_USERNAME: fluentd_is_username
      IS_PASSWORD: fluentd_is_password
  certificates:
    secrets:
      altiplano_keystore_secrets: altiplano-keystore-secrets
  extraEnv: |
    - name: ADD_ALTIPLANO_ENTERPRISE_PERMISSIONS
      value: "false"
  fluentd_sidecar:
    image:
      name: "fnms-fluent"
      tag: "nokia-4.1.8"
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 10m
        memory: 256Mi
    secrets:
      certSecret: altiplano-sso-certificate-secrets
      passwordSecret: altiplano-secrets
    LOG_INDEX_PATTERN: logstash
    UAL_INDEX_PATTERN: user-activity-log
    IS_HOST: altiplano-indexsearch
    IS_PORT: "9200"
    IS_SCHEME: https
    IS_DEBUG: "DEBUG|FINE "

    fluent_conf: |-

      <system>
        log_level error
      </system>

      <source>
        @type tail
        path /logs/keycloak.log
        read_from_head true
        pos_file /tmp/fluentd/serverlog.pos
        keep_time_key true
        tag server.log
        <parse>
          @type multiline
          format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
          format1 /^(?<date>[^ ]* [^ ]*) (?<level>[^ ]*.) \[(?<category>[^ ]*)\] \((?<thread>[^)]*)\) (?<message>[^ ].*)/
        </parse>
      </source>
      <source>
        @type tail
        path /logs/keycloak.log
        read_from_head true
        pos_file /tmp/fluentd/eventlog.pos
        keep_time_key true
        tag event.log
        <parse>
          @type multi_format
          <pattern>
            format regexp
            expression /^(?<date>[^ ]* [^ ]*) (?<level>[^ ]*.) \[(?<category>[^ ]*)\] \((?<thread>[^)]*)\) (.*?=)(?<operation>[^ ]*.), (realmId=)(?<id>[^ ]*), (?<message1>[^ ].*) (ipAddress=)(?<ip>[^ ]*), (?<message2>[^ ].*) (username=)(?<user>[^ ]*), (?<message3>[^ ].*), (authSessionTabId=)(?<session>[^ ].*)/
          </pattern>
          <pattern>
            format regexp
            expression /^(?<date>[^ ]* [^ ]*) (?<level>[^ ]*.) \[(?<category>[^ ]*)\] \((?<thread>[^)]*)\) (.*?=)(?<operation>[^ ]*.), (realmId=)(?<id>[^ ]*), (?<message1>[^ ].*) (ipAddress=)(?<ip>[^ ]*), (?<message2>[^ ].*) (?<message3>[^ ].*) (username=)(?<user>[^ ]*)/
          </pattern>
          <pattern>
            format regexp
            expression /^(?<date>[^ ]* [^ ]*) (?<level>[^ ]*.) \[(?<category>[^ ]*)\] \((?<thread>[^)]*)\) (.*?=)(?<operation>[^ ]*.), (realmId=)(?<id>[^ ]*), (?<message1>[^ ].*) (userId=)(?<user>[^ ]*), (ipAddress=)(?<ip>[^ ]*), (?<message2>[^ ].*) (?<message3>[^ ].*)/
          </pattern>
          <pattern>
            format regexp
            expression /^(?<date>[^ ]* [^ ]*) (?<level>[^ ]*.) \[(?<category>[^ ]*)\] \((?<thread>[^)]*)\) (.*?=)(?<operation>[^ ]*.), (realmId=)(?<id>[^ ]*), (?<message1>[^ ].*) (ipAddress=)(?<ip>[^ ]*), (?<message2>[^ ].*) (?<message3>[^ ].*)/
          </pattern>
        </parse>
      </source>
      <filter server.log>
        @type record_transformer
        enable_ruby true
        <record>
          container_name "#{ENV['CONTAINER_NAME']}"
          container_ip "#{ENV['POD_IP']}"
          date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
        </record>
        renew_record true
        keep_keys container_name,container_ip,date,level,thread,category,message
      </filter>
      <filter server.log>
        @type grep
        <exclude>
          key level
          pattern /^(?:{{ .Values.fluentd_sidecar.IS_DEBUG }})$/
        </exclude>
      </filter>
      <filter event.log>
        @type record_transformer
        enable_ruby true
        <record>
          application_name "User Management"
          container_name "#{ENV['CONTAINER_NAME']}"
          container_ip "#{ENV['POD_IP']}"
          date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
          invocation_time ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
          arguments ${"IP Address - " + record['ip'] + "###" + "Realm ID - " + record['id']}
          message ${record['message1'] + " " +record['message2'] + " " + record['message3']}
          operation ${record['operation'] == "\"LOGIN_ERROR\"" || record['operation'] == "\"LOGIN\"" ? "Login" : record['operation'] == "\"LOGOUT\"" ? "Logout" : record['operation'] }
          result ${record['operation'] == "\"LOGIN_ERROR\"" ? "failure" : record['operation'] == "\"LOGIN\"" || record['operation'] == "\"LOGOUT\"" ? "success" : "-" }
        </record>
        renew_record true
        keep_keys container_name,container_ip,date,level,thread,category,operation,arguments,message,session,user,result,invocation_time
      </filter>
      <filter event.log>
        @type grep
        <regexp>
          key operation
          pattern ^Login|^Logout|^UPDATE_PASSWORD|^TOKEN_EXCHANGE|^TOKEN_EXCHANGE_ERROR
        </regexp>
      </filter>
      <match server.log>
        @type opensearch
        reload_on_failure true
        reconnect_on_error true
        logstash_format true
        type_name fluentd
        ssl_verify false
        ssl_version TLSv1_2
        scheme {{ .Values.fluentd_sidecar.IS_SCHEME }}
        host {{ .Values.fluentd_sidecar.IS_HOST }}
        port {{ .Values.fluentd_sidecar.IS_PORT }}
        ca_file "/etc/.certificates/sso-server-trustchain-cert.pem"
        user "#{ENV['IS_USERNAME']}"
        password "#{ENV['IS_PASSWORD']}"
        <buffer>
          @type file
          path /tmp/fluentd/buffer_master/
          overflow_action drop_oldest_chunk
          chunk_limit_size 16MB
          queued_chunks_limit_size  4096
          flush_thread_count 5
          flush_interval 5s
          retry_wait 0s
          retry_forever true
          total_limit_size 50MB
        </buffer>
        time_key date
        time_key_exclude_timestamp true
        suppress_type_name true
        logstash_prefix {{ .Values.fluentd_sidecar.LOG_INDEX_PATTERN }}
        request_timeout 45s
      </match>
      <match event.log>
        @type opensearch
        reload_on_failure true
        reconnect_on_error true
        logstash_format true
        type_name fluentd
        ssl_verify false
        ssl_version TLSv1_2
        scheme {{ .Values.fluentd_sidecar.IS_SCHEME }}
        host {{ .Values.fluentd_sidecar.IS_HOST }}
        port {{ .Values.fluentd_sidecar.IS_PORT }}
        ca_file "/etc/.certificates/trustchain-cert.pem"
        user "#{ENV['IS_USERNAME']}"
        password "#{ENV['IS_PASSWORD']}"
        <buffer>
          @type file
          path /tmp/fluentd/buffer_ual/
          overflow_action drop_oldest_chunk
          chunk_limit_size 16MB
          queued_chunks_limit_size  4096
          flush_thread_count 5
          flush_interval 5s
          retry_wait 0s
          retry_forever true
          total_limit_size 50MB
        </buffer>
        time_key date
        time_key_exclude_timestamp true
        suppress_type_name true
        logstash_prefix {{ .Values.fluentd_sidecar.UAL_INDEX_PATTERN }}
        request_timeout 45s
      </match>
      <match **>
        @type null
      </match>

altiplano-diag:
  enabled: false
  image:
    registry: #artifactory.net.nokia.com
    #repository: #fnms-diag-server
    tag: nokia-1.0.8
  service:
    type: ClusterIP
    #ports:
      #diagNodePort: 30082
      #httpsNodePort: 30083
      #udpNodePort: 30084
  accessRoleLabel: external-access
  persistence:
    enabled: false
    storageClass: *storageClass
    accessMode: ReadWriteOnce
    # Size of the persistent storage of diag pod
    size: 10Gi
  env:
    secrets:
      USERNAME: admin
      PASSWORD: nokiafnms@123
    open:
      LOG_LEVEL: "INFO"
      FLUENTD_LOG_DIRECT_URL: "altiplano-fluentd:24224"
      #HTTPS_ENABLED: false #All other values are considered true
  ingress:
    <<: *ingressconfig
    #path: /altiplano-diag
  nginxIncIngress:
    <<: *nginxIncIngressConfig
  resources:
    requests:
      cpu: 0.05
      memory: 10Mi
    limits:
      cpu: 0.1
      memory: 100Mi
  cbur:
    enabled: false

altiplano-ksql-server:
  initContainer:
    image:
      name: fnms-init-container
      tag: nokia-2.0.1
      pullPolicy: IfNotPresent
  certificates:
    secrets:
      altiplano_keystore_secrets: altiplano-keystore-secrets
    fileNames:
      kafka_server_key_pem: kafka-server-key.pem
      kafka_server_key_pass: kafka-server-key.pass
      kafka_server_cert_pem: kafka-server-cert.pem
      altiplano_keystore_password: keystore-password
      kafka_server_keystore_jks: server.jks
      kafka_server_trustchain_cert_pem: kafka-server-trustchain-cert.pem
      kafka_server_truststore_jks: trustchain.jks
  enabled: false
  # To ensure that logs and other data use a common timezone configure timeZoneEnv.
  # This has precedence over the global level timeZoneEnv.
  #timeZone:
  #  timeZoneEnv: ""

  fullnameOverride: altiplano-ksql-server
  accessRoleLabel: internal-access

  #clusterDomain: "cluster.local"

  ## Image Info
  init:
    imageRepo: "ckaf/ckaf-kafka-init"
    imageTag: 9.1.0-rocky8-jre17-4.1.0-8675

  # KSQL server image
  image: "ckaf/ckaf-ksql"
  imageTag: 9.1.0-rocky8-jre17-7.5.3-8675

  # Kubectl image
  kubectlImageRepo: "tools/kubectl"
  kubectlTag: 1.28.7-rocky8-nano-20240301
  imagePullPolicy: IfNotPresent

  # Jmx Exporter image
  JmxExporter:
    imageRepo: "cpro/cpro-jmx-exporter"
    imageTag: 4.1.0-rocky8-0.20.0-3612
    imagePullPolicy: "IfNotPresent"

  # KSQL pod security policy
  security:
    enabled: true
    runAsUser: 999
    runAsGroup: 998
    fsGroup: 998
    readOnlyRootFilesystem: true

  # Kafka service to bootstrap ksql-server
  # Example bootstrapServers: "PLAINTEXT://kf-my-kaf-headless.namespace.svc.cluster.local:9092"
  # or "SSL://kf-my-kaf-headless.namespace.svc.cluster.local:9092"
  kafka:
    bootstrapServers: "SSL://altiplano-kafka-headless:9092"

  KafkaKsql:
    security:
      kafka:
        ssl:
          enabled: *useTls
      schema:
        ssl:
          enabled: false
      rest:
        ssl:
          enabled: false
    ssl:
      # To connect kafka(TLS enabled) and start ksql with HTTPS protocol, KSQL uses common keystore and truststore mounted using
      # below secret. Make sure to copy public cert of Kafka in truststore or use common CA.
      enabledProtocols: "TLSv1.2,TLSv1.3"
      protocol: "TLSv1.2"
      kafkarestssl:
        sslSecretName: "altiplano-secrets-all-certs"
        sslKeyStoreLocationKey: server.jks
        sslTrustStoreLocationKey: trustchain.jks
        sslKeyStorePassKey: server_jks_pass
        sslTrustStorePassKey: trustchain_jks_pass
        sslKeyPassKey: trustchain_jks_pass
        # Configures ksql to request client authentication. To enable set it to true
        clientAuth: true

  # enable external access using ingress.
  ingress:
    enabled: true
    tls:
      enabled: false
    path: /altiplano-ksql-server/?(.*)
    #authUrl: http://altiplano-oauth2-proxy.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}:4180/oauth2/auth?allowed_groups=%2Fksqldbadmin
    annotations:
      nginx.ingress.kubernetes.io/disable-annotation-validation: "true"
      nginx.ingress.kubernetes.io/proxy-pass-suffix: $uri
      nginx.ingress.kubernetes.io/rewrite-target: /$1
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers "X-Frame-Options: DENY";
        rewrite ^ $request_uri;
        rewrite "(?i)/altiplano-ksql-server(/|$)(.*)" /$2 break;
        return 400;

  ingressNbi:
    enabled: true
    tls:
      enabled: false
    path: /altiplano-ksql-server-nbi/?(.*)
    #authUrl: http://altiplano-oauth2-proxy-nbi.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}:4180/oauth2/auth?allowed_groups=%2Fksqldbadmin
    annotations:
      nginx.ingress.kubernetes.io/disable-annotation-validation: "true"
      nginx.ingress.kubernetes.io/proxy-pass-suffix: $uri
      nginx.ingress.kubernetes.io/rewrite-target: /$1
      nginx.ingress.kubernetes.io/configuration-snippet: |
        more_set_headers "X-Frame-Options: DENY";
        rewrite ^ $request_uri;
        rewrite "(?i)/altiplano-ksql-server-nbi(/|$)(.*)" /$2 break;
        return 400;

  livenessProbe:
    initialDelaySeconds: 120
    periodSeconds: 20
  readinessProbe:
    initialDelaySeconds: 30
    periodSeconds: 20

  # KSQL server resources
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1
      memory: 1Gi
    # ephemeral-storage: 1G
    #limits:
    #  cpu: 1
    #  memory: 4Gi
    #  ephemeral-storage: 1G

  # KSQL job resources for LCM events
  jobResources:
    requests:
      cpu: 200m
      memory: 256Mi
    # ephemeral-storage: 200M
    #limits:
    #  cpu: 1
    #  memory: 4Gi
    #  ephemeral-storage: 200M

  #KSQL init container resources
  initContainerResources:
    limits:
      cpu: 200m
      memory: 256Mi
    # ephemeral-storage: 500M
    #requests:
    #  cpu: 100m
    #  memory: 128Mi
    #  ephemeral-storage: 500M

altiplano-long-retention-opentsdb:
    #enabled: true
    init:
      image:
        name: fnms-init-container
        tag: nokia-2.0.1
        pullPolicy: IfNotPresent
    image:
      registry: #artifactory.net.nokia.com
      #repository: #fnms-opentsdb
      tag: nokia-4.0.2
    startupProbe:
      initialDelaySeconds: 10
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 15
    livenessProbe:
      initialDelaySeconds:
        k8sversiongt118: 20
        k8sversionlt118: 120
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
    readinessProbe:
      initialDelaySeconds: 20
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 6
    fluentd_sidecar:
      image:
        name: "fnms-fluent"
        tag: "nokia-4.1.8"
        pullPolicy: IfNotPresent
      resources:
        limits:
          cpu: 100m
          memory: 512Mi
        requests:
          cpu: 10m
          memory: 256Mi
      secrets:
        certSecret: altiplano-sso-certificate-secrets
        passwordSecret: altiplano-secrets
      IS_SCHEME: https
      IS_HOST: altiplano-indexsearch
      IS_PORT: 9200
      LOG_INDEX_PATTERN: logstash
      fluent_conf: |-
        <system>
          log_level error
        </system>
        <source>
          @type tail
          path /logs/opentsdb.log
          read_from_head true
          pos_file /tmp/fluentd/opentsdb-log.pos
          keep_time_key true
          tag otsdb.log
          <parse>
            @type multiline
            format_firstline /^\d{4}-\d{1,2}-\d{1,2} \d{1,2}:\d{1,2}:\d{1,2},\d{1,3}/
            format1 /^(?<date>[^ ]* [^ ]*) (?<level>[^ ]*.) \[(?<thread>[^ ]*)\] - (?<message>[^ ].*)/
          </parse>
        </source>
        <filter otsdb.log>
          @type record_transformer
          enable_ruby true
          <record>
            container_name "#{ENV['CONTAINER_NAME']}"
            container_ip "#{ENV['MY_POD_IP']}"
            date ${Time.strptime(record['date'], '%Y-%m-%d %H:%M:%S,%L').strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
          </record>
          renew_record true
          keep_keys container_name,container_ip,date,level,thread,message
        </filter>
        <match otsdb.log>
          @type opensearch
          suppress_type_name true
          reload_on_failure true
          reconnect_on_error true
          logstash_format true
          type_name fluentd
          ssl_verify false
          ssl_version TLSv1_2
          scheme {{ .Values.fluentd_sidecar.IS_SCHEME }}
          host {{ .Values.fluentd_sidecar.IS_HOST }}
          port {{ .Values.fluentd_sidecar.IS_PORT }}
          ca_file "/etc/.certificates/trustchain-cert.pem"
          user "#{ENV['IS_USERNAME']}"
          password "#{ENV['IS_PASSWORD']}"
          <buffer>
            @type file
            path /tmp/fluentd/buffer_otsdb/
            overflow_action drop_oldest_chunk
            chunk_limit_size 16MB
            queued_chunks_limit_size 4096
            flush_thread_count 5
            flush_interval 5s
            retry_wait 0s
            retry_forever true
            total_limit_size 50MB
          </buffer>
          time_key date
          time_key_exclude_timestamp true
          logstash_prefix {{ .Values.fluentd_sidecar.LOG_INDEX_PATTERN }}
          request_timeout 45s
        </match>
        <match **>
          @type null
        </match>
    accessRoleLabel: internal-access
    env:
      #HBASE_CB_TIMEOUT_SECOND: "1"
      timeZoneEnv: *timeZoneEnv
      secrets:
        IS_USERNAME: fluentd_is_username
        IS_PASSWORD: fluentd_is_password
      init:
        #TTL default value is 604800 seconds => 7Days
        TSDB_TTL: '2592000'
        #META_TABLE: 'tsdb-meta'
        #TREE_TABLE: 'tsdb-tree'
        #TSDB_TABLE: 'tsdb'
        #UID_TABLE: 'tsdb-uid'
        #HBASE_IMAGE: fnms-gradiant-hbase
        #HBASE_IMAGE_TAG: nokia-4.0.2
        #ENCODER_PATTERN: "%date{ISO8601} [%logger.%M] %msg%n"
        #LOGGER: "WARN"
      open: {} # remove the braces for the below values to take effect
        #LOG_LEVEL: DEBUG #valid values are ERROR, WARN, INFO, DEBUG, TRACE
        #JVMARGS: "-XX:+UseG1GC --add-opens java.base/java.nio=ALL-UNNAMED --add-opens java.security.jgss/sun.security.krb5=ALL-UNNAMED"
        # Example of job definition:
        # .---------------- minute (0 - 59)
        # |  .------------- hour (0 - 23)
        # |  |  .---------- day of month (1 - 31)
        # |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
        # |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
        # |  |  |  |  |
      # *  *  *  *  *
      #cronjob:
      #  schedule: "0 1 * * *"
    service: {} # remove the braces for the below values to take effect
      #type: NodePort
      #ports:
    #opentsdbNodePort: 30044
    persistence:
      enabled: *persistence
      storageClass: *storageClass
      accessMode: ReadWriteOnce
      # Size of the persistent storage of opentsdb pod
      size: 1Gi
    ingress:
      <<: *ingressconfig
      #path: /altiplano-long-retention-opentsdb
      #authUrl: http://altiplano-opentsdb-oauthproxy.{{ $.Release.Namespace }}.svc.cluster.local:4180/oauth2/auth
    nginxIncIngress:
      <<: *nginxIncIngressConfig
    resources:
      requests:
        cpu: 200m
        memory: 1Gi
      limits:
        cpu: 1000m
        memory: 2Gi
        
altiplano-installation-info:
  description: "Info about valuesModel list used during the installation."
  installationStatus:
    values: installed
